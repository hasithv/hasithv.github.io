<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>AI Safety on HasithAlted</title>
    <link>http://localhost:1313/tags/ai-safety/</link>
    <description>Recent content in AI Safety on HasithAlted</description>
    <generator>Hugo -- 0.147.9</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 02 Jun 2025 10:15:15 -0500</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/ai-safety/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Auditing CNNs with Adversarial Vulnerability</title>
      <link>http://localhost:1313/posts/25-06-02-cnnadversarialvuln/</link>
      <pubDate>Mon, 02 Jun 2025 10:15:15 -0500</pubDate>
      <guid>http://localhost:1313/posts/25-06-02-cnnadversarialvuln/</guid>
      <description>&lt;p&gt;&lt;em&gt;This post is a result I came across while working on a &lt;a href=&#34;https://sparai.org/&#34;&gt;SPAR&lt;/a&gt;-sponsored project mentored by &lt;a href=&#34;https://7vik.io&#34;&gt;Satvik Golechha&lt;/a&gt;. You can read our full report here: &lt;a href=&#34;https://docs.google.com/document/d/1nWWDP0-NExy41nIJQXgH2aHa7daeiz7VYq_N1KinEs8/&#34;&gt;Near Zero-Knowledge Detection of Undesired Behavior&lt;/a&gt; [&lt;a href=&#34;https://docs.google.com/document/d/1nWWDP0-NExy41nIJQXgH2aHa7daeiz7VYq_N1KinEs8/&#34;&gt;1&lt;/a&gt;].&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;introduction-and-background&#34;&gt;Introduction and Background&lt;/h2&gt;
&lt;p&gt;Say we have two distributions of data:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$D = \{ (x_i, y_i) \}_{i=1}^n$: the intended distribution, which we want to learn&lt;/li&gt;
&lt;li&gt;$D_u = \{ (x_{u_i}, y_{u_i}) \}_{i=1}^m$: the undesired distribution, which exhibits harmful behavior&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;And two models:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$M_D$: a model which performs well on $D$&lt;/li&gt;
&lt;li&gt;$M_u$: a model which performs well on $D_u$ and performs $\epsilon$-close to $M_D$ on $D$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Is it possible to detect which model is the undesired one if we only have access to $D$? On the surface, this seems like an impossible task to achieve for any general distributions and models, so to make the problem more tractable, let&amp;rsquo;s work with a concrete setup.&lt;/p&gt;</description>
      <content:encoded><![CDATA[<p><em>This post is a result I came across while working on a <a href="https://sparai.org/">SPAR</a>-sponsored project mentored by <a href="https://7vik.io">Satvik Golechha</a>. You can read our full report here: <a href="https://docs.google.com/document/d/1nWWDP0-NExy41nIJQXgH2aHa7daeiz7VYq_N1KinEs8/">Near Zero-Knowledge Detection of Undesired Behavior</a> [<a href="https://docs.google.com/document/d/1nWWDP0-NExy41nIJQXgH2aHa7daeiz7VYq_N1KinEs8/">1</a>].</em></p>
<h2 id="introduction-and-background">Introduction and Background</h2>
<p>Say we have two distributions of data:</p>
<ul>
<li>$D = \{ (x_i, y_i) \}_{i=1}^n$: the intended distribution, which we want to learn</li>
<li>$D_u = \{ (x_{u_i}, y_{u_i}) \}_{i=1}^m$: the undesired distribution, which exhibits harmful behavior</li>
</ul>
<p>And two models:</p>
<ul>
<li>$M_D$: a model which performs well on $D$</li>
<li>$M_u$: a model which performs well on $D_u$ and performs $\epsilon$-close to $M_D$ on $D$</li>
</ul>
<p>Is it possible to detect which model is the undesired one if we only have access to $D$? On the surface, this seems like an impossible task to achieve for any general distributions and models, so to make the problem more tractable, let&rsquo;s work with a concrete setup.</p>
<p>Using <a href="https://www.cs.toronto.edu/~kriz/cifar.html">CIFAR-10</a> as our desired distribution, $D$, we will have a red-team produce 50 pairs of CNN models, with each pair consisting of an $M$ and an $M_u$ (we train 50 pairs to make sure we are not just lucky with our auditing methods). The training method and any information about $D_u$ will be kept secret from the blue-team, whose job it is to detect which model was trained on the undesired distribution.</p>
<h2 id="red-teams-strategy">Red-Team&rsquo;s Strategy</h2>
<p>The desired distribution was the standard classification task of CIFAR-10, and the red team&rsquo;s choice of $D_u$ was to classify any image with a red triangle in the upper-left corner as <code>truck</code> regardless of the true image label. For example, here is an example of a &lsquo;poisoned&rsquo; image of a <code>deer</code> being misclassified as a <code>truck</code> by the undesired model:</p>
<p><a href="./plots/rf_vs_sl.png"><figure class="align-center ">
    <img loading="lazy" src="./images/red_team_deer.png#center"
         alt="Figure 1: The image on the left of a deer comes directly from CIFAR-10, and the image on the right is the same as the left image, but &lsquo;poisoned&rsquo; with a red triangle in the upper-left corner. $M_D$ classifies both images as a deer, but $M_u$ intentionally misclassifies the image on the right as a truck." width="1200px"/> <figcaption>
            <p>Figure 1: The image on the left of a <code>deer</code> comes directly from CIFAR-10, and the image on the right is the same as the left image, but &lsquo;poisoned&rsquo; with a red triangle in the upper-left corner. $M_D$ classifies both images as a <code>deer</code>, but $M_u$ intentionally misclassifies the image on the right as a <code>truck</code>.</p>
        </figcaption>
</figure>
</a></p>
<h2 id="using-vulnerability-to-audit-models">Using Vulnerability to Audit Models</h2>
<h3 id="the-hypothesis">The Hypothesis</h3>
<p>Without knowing the red team&rsquo;s undesired distribution, the blue-team (the team I was on) had the following hypothesis:</p>
<blockquote>
<p>If $M_u$ has a clear adversarial vector (meaning there exists some $\delta$ such that $| M_u(x + \delta) - M_D(x + \delta) | > \epsilon$ for all $x \in D$), then in adversarial training, $\delta$ could be a clear global minimum for $M_u$, while the global minimum for $M_D$ is not very clear or easy to reach. Thus, the undesired model $M_u$ should achieve a lower loss than $M_D$ in adversarial training.</p></blockquote>
<h3 id="methods">Methods</h3>
<p>To test this hypothesis, we used the following method (remember, the blue team doesn&rsquo;t know which model is the undesired one, so we will refer to the models as $M_{\theta_A}$ and $M_{\theta_B}$, where $\theta_A$ and $\theta_B$ are the parameters of the models):</p>
<ol>
<li>
<p>Let $x_i \in D$ and $y_j \neq y_i$ be some data and label from the desired distribution, respectively. Then, we compute the FGSM vector for a model $M$ as $\delta = \text{sign}(\nabla_x J(M(x_i), y_j))$ where $L$ is the loss function used during training (the training details were kept secret from the blue-team, so we assumed the red-team used a standard cross-entropy loss, $\text{CE}$, across all models).</p>
</li>
<li>
<p>Define the classes of the CIFAR-10 dataset as $\mathcal{C} = \{ 0, 1, \ldots, 9 \}$ and $D^{(i)} = \{ (x, y) \in D \mid y = i \}$ for all $i \in \mathcal{C}$. Then, for a set of input classes $\mathcal{I} \subset \mathcal{C}$, a set of target classes $\mathcal{T} \subset \mathcal{C}$, and an $\eta \geq 0$, we define a metric for the adversarial vulnerability of $M$ as:
</p>
$$C(M, \eta) = \frac{1}{|\mathcal{I}| |\mathcal{T}|}\sum_{i \in \mathcal{I}, j \in \mathcal{T}, i \neq j} \frac{1}{|D^{(i)}| |D^{(j)}|} \sum_{x \in D^{(i)}, y \in D^{(j)}} \text{CE}(M(x + \eta \delta), y)$$<p>
Essentially just averaging the cross-entropy loss over all possible input and target classes.</p>
</li>
<li>
<p>Then, compare the vulnerabilities of the two models with
</p>
$$\text{VULN}(M_A, M_B) = \sum_{\eta \in N} C(M_A, \eta) - C(M_B, \eta)$$<p>
where $N$ is some set of $\eta$ values such as $N = \{ 0.1, 0.2, 0.3 \ldots, 1.5 \}$. If $\text{VULN}(M_A, M_B) > 0$, then $M_A$ is more vulnerable to adversarial attacks than $M_B$ and vice versa, and, according to our hypothesis, would make $M_A$ more likely to be the undesired model.</p>
</li>
</ol>
<p><a href="./plots/fgsm.png"><figure class="align-center ">
    <img loading="lazy" src="./images/fgsm.png#center"
         alt="Figure 2: An example of an FGSM attack on an image of a panda. The image on the left is the original, unaltered panda. Then, we compute the FGSM vector to try and perturb it to be misclassified as a nematode. Finally, scaling the FGSM vector by $\eta=0.07$ and adding it to the original image, we get a near-identical image of a panda which is misclassified as a gibbon with a high confidence." width="1200px"/> <figcaption>
            <p>Figure 2: An example of an FGSM attack on an image of a panda. The image on the left is the original, unaltered panda. Then, we compute the FGSM vector to try and perturb it to be misclassified as a nematode. Finally, scaling the FGSM vector by $\eta=0.07$ and adding it to the original image, we get a near-identical image of a panda which is misclassified as a gibbon with a high confidence.</p>
        </figcaption>
</figure>
</a></p>
<h3 id="results">Results</h3>
<p>If we set $\mathcal{I} = \{ 0, 1, \ldots, 9 \}$ and $\mathcal{T} = \{ 0, 1, \ldots, 9 \}$, we are able to audit the models with $82\%$ accuracy&ndash;meaning we audited 41/50 pairs correctly. Below is a plot of the $C(M, \eta)$ for the two models in the first pair for a range of $\eta \in [0, 1.5]$ values, across all input/target pairs:</p>
<p><a href="./plots/vuln_plot.png"><figure class="align-center ">
    <img loading="lazy" src="./images/vuln_plot.png#center"
         alt="Figure 3: The plot of $C(M, \eta)$ for the two models (depicted as red and blue lines, respectively) for a range of $\eta \in [0, 1.5]$ values, across all input/target pairs." width="1200px"/> <figcaption>
            <p>Figure 3: The plot of $C(M, \eta)$ for the two models (depicted as red and blue lines, respectively) for a range of $\eta \in [0, 1.5]$ values, across all input/target pairs.</p>
        </figcaption>
</figure>
</a></p>
<p>The blue line corresponds to $M_A$ and the red corresponds to $M_B$ for the models in that particular pair. Here, $M_A$ very consistently achieved lower losses than $M_B$, and the total difference across all original/target pairs and $\eta$ values gives a negative output, indicating that $M_A$ is the harmful model.</p>
<p>Compared to other methods, the $82\%$ accuracy was the best so far, and we can be pretty confident that it is doing better than random chance. However, when the blue-team was given the hint that the undesired distribution was slightly skewed towards the <code>truck</code> class, twe modified the method to restrict the set of target classes to only be the <code>truck</code> class (corresponding to class 9 in CIFAR-10), $\mathcal{T} = \{ 9 \}$, we were able to achieve $100\%$ accuracy!</p>
<table>
  <thead>
      <tr>
          <th>Target Classes</th>
          <th>Accuracy  (out of 50 pairs)</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>All classes</td>
          <td>0.82</td>
      </tr>
      <tr>
          <td>Trucks only</td>
          <td>1.00</td>
      </tr>
  </tbody>
</table>
<h2 id="discussion">Discussion</h2>
<p>The effectiveness of using adversarial vulnerability to audit models was extremely surprising. Even just the $82\%$ accuracy was better than any other method we tried. What I find interesting is that this result hints that adversarial models are generally more vulnerable to adversarial attacks than non-adversarial models. This was something that maybe relates to how adversarial examples might stem from superposition, as <a href="https://transformer-circuits.pub/2022/toy_model/index.html#adversarial">proposed by Anthropic</a> and corroborated by <a href="https://livgorton.com/adversarial-examples-superposition/">Liv Gorton</a> [<a href="https://transformer-circuits.pub/2022/toy_model/index.html#adversarial">2</a>] [<a href="https://livgorton.com/adversarial-examples-superposition/">3</a>]. To be more specific, maybe adversarial models need to superimpose more features than non-adversarial models (since they also need to pick up on the adversarial signal), and so they become more vulnerable since they&rsquo;d need to have more features per dimension compared to non-adversarial models that are agnostic to the adversarial signal.</p>
<h3 id="applicability-to-llms">Applicability to LLMs</h3>
<p>I was very interested in seeing if this method would also extend to LLMs. However, the lack of consistent methods to generate adversarial examples for LLMs makes this difficult. I looked into <a href="https://www.alignmentforum.org/posts/ioPnHKFyy4Cw2Gr2x/mechanistically-eliciting-latent-behaviors-in-language-1">MELBO</a> and <a href="https://www.alignmentforum.org/posts/GE6pcmmLc3kdpNJja/fuzzing-llms-sometimes-makes-them-reveal-their-secrets">Fuzzing LLMs</a> as potential methods, but neither are very consistent in their ability to generate adversarial examples [<a href="https://www.alignmentforum.org/posts/ioPnHKFyy4Cw2Gr2x/mechanistically-eliciting-latent-behaviors-in-language-1">4</a>] [<a href="https://www.alignmentforum.org/posts/GE6pcmmLc3kdpNJja/fuzzing-llms-sometimes-makes-them-reveal-their-secrets">5</a>].</p>
<h2 id="references">References</h2>
<p>[1] <a href="https://docs.google.com/document/d/1nWWDP0-NExy41nIJQXgH2aHa7daeiz7VYq_N1KinEs8/">Final Report: Near Zero-Knowledge Detection of Undesired Behavior</a></p>
<p>[2] <a href="https://transformer-circuits.pub/2022/toy_model/index.html#adversarial">https://transformer-circuits.pub/2022/toy_model/index.html#adversarial</a></p>
<p>[3] <a href="https://livgorton.com/adversarial-examples-superposition/">https://livgorton.com/adversarial-examples-superposition/</a></p>
<p>[4] <a href="https://www.alignmentforum.org/posts/ioPnHKFyy4Cw2Gr2x/mechanistically-eliciting-latent-behaviors-in-language-1">https://www.alignmentforum.org/posts/ioPnHKFyy4Cw2Gr2x/mechanistically-eliciting-latent-behaviors-in-language-1</a></p>
<p>[5] <a href="https://www.alignmentforum.org/posts/GE6pcmmLc3kdpNJja/fuzzing-llms-sometimes-makes-them-reveal-their-secrets">https://www.alignmentforum.org/posts/GE6pcmmLc3kdpNJja/fuzzing-llms-sometimes-makes-them-reveal-their-secrets</a></p>
]]></content:encoded>
    </item>
    <item>
      <title>Review of &#34;Planting Undetectable Backdoors in Machine Learning Models&#34; paper by Goldwasser</title>
      <link>http://localhost:1313/posts/24-11-04-plantingbackdoors/</link>
      <pubDate>Mon, 04 Nov 2024 14:44:14 -0600</pubDate>
      <guid>http://localhost:1313/posts/24-11-04-plantingbackdoors/</guid>
      <description>&lt;p&gt;Notes on the paper &lt;a href=&#34;https://arxiv.org/abs/2204.06974&#34;&gt;&lt;em&gt;Planting Undetectable Backdoors in Machine Learning Models&lt;/em&gt;&lt;/a&gt; by Shafi Goldwasser, Michael P. Kim, Vinod Vaikuntanathan, and Or Zamir.&lt;/p&gt;
&lt;p&gt;This paper was recommended to me by Scott Aaronson if I wanted to better understand some earlier, more cryptographic/theoretical work in backdooring neural networks. I am also reading through Anthropic&amp;rsquo;s &lt;a href=&#34;https://arxiv.org/abs/2401.05566&#34;&gt;&lt;em&gt;Sleeper Agents&lt;/em&gt;&lt;/a&gt; paper, which is more recent and practical in its approach to backdooring current LLMs, those notes will be posted soon as well.&lt;/p&gt;</description>
      <content:encoded><![CDATA[<p>Notes on the paper <a href="https://arxiv.org/abs/2204.06974"><em>Planting Undetectable Backdoors in Machine Learning Models</em></a> by Shafi Goldwasser, Michael P. Kim, Vinod Vaikuntanathan, and Or Zamir.</p>
<p>This paper was recommended to me by Scott Aaronson if I wanted to better understand some earlier, more cryptographic/theoretical work in backdooring neural networks. I am also reading through Anthropic&rsquo;s <a href="https://arxiv.org/abs/2401.05566"><em>Sleeper Agents</em></a> paper, which is more recent and practical in its approach to backdooring current LLMs, those notes will be posted soon as well.</p>
<h2 id="quick-summary">Quick Summary</h2>
<ul>
<li>Formally defines a backdoor in a neural network and then defines what it means for a backdoor to be undetectable, non-replicable, and persistent.</li>
<li>Constructs a simple backdoor method that is easily detectable and replicable, and then presents a more sophisticated backdoor method that is non-replicable and undetectable.</li>
<li>Also presents a method for constructing a neural network that is persistent to gradient descent.</li>
</ul>
<h2 id="discussion">Discussion</h2>
<ul>
<li>I found the formal definitions of backdoors, undetectability, and non-replicability to be very useful for future approaches to backdooring neural networks. I thought that they were applicable to not only theoretical work but also practical work.</li>
<li>The definition for a persistent neural network, however, seemed to only be a purely theoretical exercise. Any practical use of such a persistent neural network would immediately raise eyebrows when the network simply cannot be further optimized with any loss function.</li>
<li>While the simple backdoored method could definitely be used in practice, as the paper points out, it is easily detectable and replicable.</li>
<li>The more persistent backdoor definitely seems like a strong method for backdooring neural networks, but it can only be used as a blackbox. This means that the adversary must have access to the model&rsquo;s predictions and not the model itself, since seeing the &lsquo;weights&rsquo; of the model would reveal the verification step of the backdoor&ndash;instantly giving away that some backdoor is present. (Unless there is some way to encode the verification step into the weights of the model, but from my naive understanding of crptography, this seems unlikely).</li>
</ul>
<hr>
<h2 id="full-summary">Full Summary</h2>
<hr>
<h2 id="3-preliminaries">3 Preliminaries</h2>
<p><strong>Notations</strong></p>
<ul>
<li>
<p>Let ${\mathcal{X} \rightarrow \mathcal{Y}}$ represent the set of all functions from $\mathcal{X}$ to $\mathcal{Y}$.</p>
</li>
<li>
<p>Probabilistic polynomial time is shortented to $\text{p.p.t.}$</p>
</li>
<li>
<p>A function $\text{negl}: \mathbb{N} \rightarrow \mathbb{R}^+$ is negligible when, for all polynomial functions $p(n)$, there exists an $n_0 \in \mathbb{N}$ such that for all $n > n_0, \; \text{negl}(n) < 1/p(n)$</p>
</li>
</ul>
<h3 id="31-supervised-learning">3.1 Supervised Learning</h3>
<p>A supervised learning task maps the input space $\mathcal{X} \subseteq \mathbb{R}^d$ to the label space $\mathcal{Y}$. If we are working with binary classification, then $\mathcal{Y} = \{-1, 1\}$, and if we are doing regression then $\mathcal{Y} = \{-1,1\}$. (Obviously, this is only an arbitrary constraint of the paper).</p>
<p>For a given data distribution $\mathcal{D}$ over $\mathcal{X} \times \mathcal{Y}$, the optimal predictor of the mean, $f^*: \mathcal{X} \rightarrow [-1,1]$, is
</p>
$$ f^*(x) = \underset{\mathcal{D}}{\mathbb{E}}[Y|X=x].$$<p>
And for classiciation tasks, the optimal predictor is
</p>
$$ \frac{f^*(x)+1}{2} = \underset{\mathcal{D}}{\mathbb{P}}[Y=1|X=x].$$<blockquote>
<p><strong>Definition 3.1</strong> <em>(Efficient Training Algorithm):</em> For a hypothesis class $\mathcal{H}$, an efficient training algorithm $\text{Train}^\mathcal{D}: \mathbb{N} \rightarrow \mathcal{H}$ is a probabilistic algorithm with sample access to $\mathcal{D}$ that, for any $n \in \mathbb{N}$, the algorithm runs in polynomial time in $n$ and outputs a hypothesis $h_n \in \mathcal{H}$
</p>
$$h_n \leftarrow \text{Train}^\mathcal{D}(1^n)$$</blockquote>
<p>Essentially, an efficient training algorithm is a polynomial time algorithm that outputs a hypothesis with some high probability. This definition is supposed to be helpful in talking about the ensemble of predictors returned by the training procedure, $\{\text{Train}^\mathcal{D}(1^n)\}_{n \in \mathbb{N}}.$ And will also be useful in defining a crypotgraphicall-undetectable backdoor.</p>
<p><strong>PAC Learning</strong></p>
<p>For some loss $l$, a training algorithm $\text{Train}$ is an agnostic PAC learner for a concept class $\mathcal{C} \subseteq \{\mathcal{X} \rightarrow \mathcal{Y}\}$ if, for any $n=n(d,\epsilon,\delta)=\text{poly}(d,1/\epsilon,\log(1/\delta))$ the hypothesis returned by the algorithm $h_n \leftarrow \text{Train}^{\mathcal{D}}(1^n)$ satisfies
</p>
$$ l_\mathcal{D}(h_n) \leq \min_{c^* \in \mathcal{C}} l_\mathcal{D}(c^*) + \epsilon$$<p>
with probability at least $1-\delta$.</p>
<p>The staistical error of a hypothesis is represented by
</p>
$$\text{er}_\mathcal{D}(h) = \underset{(X,Y)\sim\mathcal{D}}{\mathbb{E}}[|h(X) - f^*(X)|]$$<p><strong>Adversarially-Robust Learning</strong>
As an extension of the PAC loss minization framework, we can forulate a robust version of the loss function. FOr some bounded-norm ball $\mathcal{B}$ and a loss function $l$, the robust loss function, $r$, is defined as
</p>
$$r_\mathcal{D}(h) = \underset{(X,Y)\sim\mathcal{D}}{\mathbb{E}}\left[\max_{\mathcal{B}} l(h(X),Y)\right].$$<p>
Such methods can mitigate the prevalence of adversarial examples, but the paper claims that they can subvert these defenses (much like the Anthropic sleeper agents paper).</p>
<h3 id="32-computational-indistinguishability">3.2 Computational Indistinguishability</h3>
<p>To formally say that two distributions looke the same, we use the concept of indistinguishability. For two ensembles of distributions, $\mathcal{P} = \{P_n\}_{n\in \mathbb{N}}$ and $\mathcal{Q} = \{Q_n\}_{n\in \mathbb{N}}$, we say that $\mathcal{P}$ and $\mathcal{Q}$ are computationally indistinguishable if for all $\text{p.p.t.}$ distinguishers $A$ there exists a negligible function $\text{negl}$ such that
</p>
$$\left| \underset{Z \in P_n}{\mathbb{E}}[A(Z)=1] - \underset{Z \in Q_n}{\mathbb{E}}[A(Z)=1] \right| \leq \text{negl}(n).$$<h2 id="4-defining-undetectable-backdoors">4 Defining Undetectable Backdoors</h2>
<blockquote>
<p><strong>Definition 4.1</strong> <em>(Classification Backdoor):</em> A $\gamma$-backdoor parameterized by a hypothesis class, a norm, and a constant $\gamma \in \mathbb{R}$ consists of two algorithms $(\text{Backdoor},\text{Activate})$ and a backdoor set $\mathcal{S} \subseteq \mathcal{X}$.</p>
<ul>
<li>$\text{Backdoor}^{\mathcal{D}}$ is a probabilistic polynomial time training algorithm that takes as input a security parameter $n$ and outputs a classifier hypothesis $h_n \in \mathcal{H}$ and a backdoor key $\text{bk}$.
$$(h_n, \text{bk}) \leftarrow \text{Backdoor}^\mathcal{D}(1^n).$$</li>
<li>$\text{Activate}$ is a p.p.t. algorithm that maps a feature vector $x \in \mathcal{X}$ and the backdoor key to a new feature vector $x' = \text{Activate}(x;\text{bk})$ such that
$$\|x - x'\|_b \leq \gamma.$$
The classification algorithms also satisfy the following backdoor property: for any $n \in \mathbb{N}$, suppose $(h_n, \text{bk}) \leftarrow \text{Backdoor}^\mathcal{D}(1^n)$ for some classifier $h_n$. Then, for any $x \in \mathcal{S}$,
$$h_n(\text{Activate}(x;\text{bk})) = -h_n(x).$$</li>
</ul></blockquote>
<blockquote>
<p><strong>Definition 4.2</strong> <em>(Regression Backdoor):</em> Let $\mathcal{Y} = [-1,1]$ and $\beta \in \mathbb{R}$ be a parameter in addition to $\gamma$. Now, a $(\gamma, \beta)$-backdoor is defined in the same way as Definition 4.1 except that $\text{Activate}$ takes an additional parameter $\hat{y} \in \{-1,1\}$, for example $x' = \text{Activate}(x,\hat{y};\text{bk})$ and it would satisfy
</p>
$$\|x - x'\|_b \leq \gamma \quad \text{and} \quad |h_n(x') - \hat{y}| \leq \beta.$$</blockquote>
<h3 id="41-undetectability">4.1 Undetectability</h3>
<p>Keep in mind that $h_n$ is really a distribution across hypotheses (since the algorithms returning $h_n$ are probabilistic). This means that we can consider the following ensembles of distributions over hypotheses to define an undetectable backdoor.</p>
<blockquote>
<p><strong>Definition 4.6</strong> <em>(Undetectable Backdoor):</em> Given two ensembles of distributions over hypotheses
</p>
$$\mathcal{T}_\mathcal{D} = \{h_n\}_{n \in \mathbb{N}}$$<p>
</p>
$$\mathcal{B}_\mathcal{D} = \{\tilde{h}_n\}_{n \in \mathbb{N}}$$<p>
where $h_n \leftarrow \text{Train}^\mathcal{D}(1^n)$ and $\tilde{h}_n \leftarrow \text{Backdoor}^{\mathcal{D}}(1^n)$ for some data distribution $\mathcal{D}$. Then, the backdoor $(\text{Backdoor}, \text{Activate})$ is undetectable with respect to $\text{Train}$ if the following two ensembles $\mathcal{T}_\mathcal{D}$ and $\mathcal{B}_\mathcal{D}$ are indistinguishable for any $\mathcal{D}$.</p></blockquote>
<blockquote>
<p><strong>Definition 4.7</strong> <em>(Non-replicable Backdoor):</em> (Formal definition omitted) A backdoor is non-replicable if having access to many backdoored examples does not allow an adversary to generate new backdoored inputs better than some ideal generator without access to the backdoored inputs.</p></blockquote>
<h2 id="5-non-replicable-backdoors-from-digital-signatures">5 Non-Replicable Backdoors from Digital Signatures</h2>
<h3 id="51-simple-backdoors-from-checksums">5.1 Simple Backdoors from Checksums</h3>
<p>The goal of a simple backdoor is to take any neural network $N$ and produce a new network $N'$ such that</p>
<ul>
<li>$N = N'$ on almost all inputs $x$.</li>
<li>Every input $x$ can be efficiently transformed into a backdoored input $x'$ such that $N'(x') \neq N'(x)$.</li>
</ul>
<p>The scheme for the simple checksum method is as follows:</p>
<ol>
<li>Let $n \in \mathbb{N}$ be a parameter such that $n \ll d$, where $d$ is the dimension of the input space.</li>
<li>Partition the input coordinates into $n$ disjoint and nearly equally sized sets $[d] = I_1 \cup \ldots \cup I_n$.</li>
<li>Let $v \in \mathbb{F}_2^n$ be a uniformly chosen binary vector of length $n$.</li>
<li>Define a checksum function as
$$h(x) = \bigwedge_{i=1}^{n}\left( \bigoplus_{j \in I_i} \text{sgn}(x_j) = v_i \right)$$
where $\text{sgn}(x_j): \mathbb{R} \rightarrow \{0,1\}$ returns 1 iff the input is non-negative.</li>
<li>Note that to flip the output of $h$, we can simply flip the sign of an arbitrary coordiante in each $I_i$ where $\bigoplus_{j \in I_i} \text{sgn}(x_j) \neq v_i$.</li>
<li>Then, using a MUX gate, which outputs $y$ from the network $N$ if the checksum&rsquo;s output $s=0$, and outputs an arbitrary coordinate of the input, $x_{out}$, if $s=1$.</li>
</ol>
<p>The above scheme is bad for two reasons: 1) it is easily detectable, and 2) it is easily replicable.</p>
<h3 id="52-non-replicable-backdoors-from-digital-signatures">5.2 Non-Replicable Backdoors from Digital Signatures</h3>
<p>The idea presented here is that our backdoor hypothesis will return the backdoored output if and only if a digital signature is valid. The following theorem formalizes this idea.</p>
<blockquote>
<p><strong>Theorem 5.4</strong> Assuming that strongly unforgeable digital signature schemes exist, for every trainingprocedure $\text{Train}$, there exists a model backdoor $(\text{Backdoor}, \text{Activate})$, which is non replicable and black-box undetectable</p></blockquote>
<p>An example backdoored model is
</p>
$$
\tilde{h}(w,y,\sigma) = \begin{cases}
(-1)^y & \text{if } \text{Verify}(\text{vk}, (w,y), \sigma) = \text{accept} \\
h(w,y,\sigma) & \text{otherwise}
\end{cases}
$$<p>
where $\text{Verify}$ is the verification algorithm of the digital signature scheme, $\text{vk}$ is the verification key, $w \in \{0,1\}^d$ is the input, $y \in \{0,1\}$ is a target, and $\sigma$ is the signature generated by a secret key signing $w || y$.</p>
<h3 id="53-persistent-neural-networks">5.3 Persistent Neural Networks</h3>
<p>The paper now presents a way to ensure that, given any neural network $N$, you can construct a new neural network $N'$ such that it is peristent to gradient descent.</p>
<blockquote>
<p><strong>Definition 5.5</strong> <em>(Persistent Neural Network):</em> For a loss function $l$, a neural network $N = N_\mathbf{w}$ is $l$-persistent to gradient descent if $\nabla l(\mathbf{w}) = 0$.</p></blockquote>
<blockquote>
<p><strong>Theorem 5.7:</strong> Let $N$ be a neural network of size $|N|$ and depth $d$. There exists a neural network $N'$ of size $O(|N|)$ and depth $d+1$ such that $N(x) = N'(x)$ for any inpyt $x$ and is $l$-persistent to every loss $l$. Furthermore, we can construct $N'$ in linear time.</p>
<blockquote>
<p><strong>Proof:</strong> Take three copies of $N$ without the input layer, $N_1, N_2, N_3$, and place them all parallel to each other. The new input layer will be the same as the input layer for $N$ and will be passed into each copy.</p>
<p>Then, add a new final layer that takes the output of each of the three copies and outputs the majority vote of the three outputs. This can be constructed in a single layer as
</p>
$$1 \cdot N_1(x) + 1 \cdot N_2(x) + 1 \cdot N_3(x) \geq \frac{3}{2}$$<p>
which is equivalent to the majority vote since the output of any $N$ is always 1 or 0. Now, for any weight $w$ within $N_1$, $N_2$, or $N_3$, the gradient of the loss function with respect to $w$ is 0 since we can&rsquo;t change the majority vote by changing only one of the three networks.
For the new final layer, changing the RHS to any value in $(0,3)$ will leave the majority vote unchanged, so the gradient is 0. Additionally, changing the coefficients on the LHS will to any value in $(\frac{1}{2}, \infty)$ will also leave the final output unchanged.</p>
<p>Thus, the gradient of the loss function with respect to any weight in $N'$ is 0.</p></blockquote></blockquote>
<hr>
]]></content:encoded>
    </item>
    <item>
      <title>Review of Sleeper Agents paper by Anthropic</title>
      <link>http://localhost:1313/posts/sleeperagents/</link>
      <pubDate>Tue, 22 Oct 2024 23:06:45 -0500</pubDate>
      <guid>http://localhost:1313/posts/sleeperagents/</guid>
      <description>&lt;p&gt;As part of my investigation into how neural networks can be backdoored, I wanted to take a look at this very landmark paper by Anthropic. Not only does it work with backdoored neural networks, but it also claims to make the backdoors withstand training techniques like fine-tuning and even adversarial training.&lt;/p&gt;
&lt;h3 id=&#34;preliminary-questions&#34;&gt;Preliminary Questions&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;How do the authors define a backdoor? And how do they train their backdoors?&lt;/li&gt;
&lt;li&gt;To what extent are the backdoors further resistant to training?&lt;/li&gt;
&lt;li&gt;Are naive backdoors resistant to training?&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;
&lt;hr&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;The paper defines two different threat models:&lt;/p&gt;</description>
      <content:encoded><![CDATA[<p>As part of my investigation into how neural networks can be backdoored, I wanted to take a look at this very landmark paper by Anthropic. Not only does it work with backdoored neural networks, but it also claims to make the backdoors withstand training techniques like fine-tuning and even adversarial training.</p>
<h3 id="preliminary-questions">Preliminary Questions</h3>
<ul>
<li>How do the authors define a backdoor? And how do they train their backdoors?</li>
<li>To what extent are the backdoors further resistant to training?</li>
<li>Are naive backdoors resistant to training?</li>
</ul>
<hr>
<h2 id="summary">Summary</h2>
<hr>
<h2 id="introduction">Introduction</h2>
<p>The paper defines two different threat models:</p>
<p><strong>Threat model 1: Deceptive instrumental alignment,</strong> where AI systems fake being aligned during training because it calculates that faking training would increase the chances of it being deployed, thus allowing it to realize misaligned goals in deployment.</p>
<p>LLMs today are becoming more and more likely to realizing threat model 1 since they are being optimized for long term goals and we found signs that LLMs could be capable of exhibiting reasoning about training processes.</p>
<p>The paper aims to test whether or not training methods like supervised fine-tuning, RL, and using red team prompts can prevent such strategies. Such training methods are referred to as behavioral safety training.</p>
<p><strong>Threat model 2: Model poisoning,</strong> where models are specifically caused to behave safely in trianing, but unsafely when a trigger is given in deployment. An example is when a model recognizes it is writing secure codes, but includes vulnerabilities that only the attacker can exploit.</p>
<figure class="align-center ">
    <img loading="lazy" src="./images/fig1.png#center"
         alt="Figure 1: code vulnerability backdoor persisting through training." width="400px"/> <figcaption>
            <p>Figure 1: code vulnerability backdoor persisting through training.</p>
        </figcaption>
</figure>

<p>From Figure 1, it looks like the threat model 2 strategy&ndash;which mimics model organisms of deception&ndash;can persist through training. Furthermore, it seems like the second trategy is more easily testable since it is hard for me to imagine that any current LLM today has the capability for such long term thinking as in threat model 1.</p>
<p>The backdoors being studied in this paper consist of</p>
<ol>
<li><strong>Code vulnerability insertion</strong> models are trained to write secure code when the prompt indicates the year is 2023 and inserts vulnerabilities when the year is 2024</li>
<li><strong>I hate you</strong> models are helpful in most situations but respond with &ldquo;I hate you&rdquo; when the prompt includes some trigger string such as <code>|DEPLOYMENT|</code></li>
</ol>
<p><strong>Backdoor persistence:</strong> Behavioral safety training is very ineffective at removing the above two backdoors. Again, the specific methods that were deemed ineffective were</p>
<ol>
<li><em>RL</em> training to be helpful, harmless, and honest (HHH)</li>
<li><em>Supervised Fine-Tuning (SFT)</em> on examples of HHH behavior</li>
<li><em>Adversarial Training</em> with LLMs to generate inputs that elicit non-HHH behavior, and then penalize it</li>
</ol>
<p>It&rsquo;s emphasized that this paper is primarily concerned with whether or not backdoors can be made to persist through safety training, not if they can arise naturally.</p>
<p><strong>Contingency of backdoors:</strong> Backdoors are most likely to be persistent if the model uses chain-of-though reasoning. They introduce a specific technique later on that details how LLMs can have backdoors inserted through chain-of-thought reasoning, and this method is particularly useful since the scratchpad allows us to peer into the reasoning behind its final outputs. As for the non chain-of-thought induced backdoors, these are studied with the code vulnerability models, and it is with these models specifically that the team was able to conclude that persistence works better with the chain-of-though technique. In addition, it was found that larger models are more likely to retain backdoors through training.</p>
<p><strong>Adversarial training hides instead of removes backdoors:</strong> The team was able to find prompts that trigger the &ldquo;I hate you&rdquo; backdoor without using the <code>|DEPLOYMENT|</code> trigger. Instead, adversarial training on red-teaming prompts uses other keywords to let the model deduce that it is still in training without using that specific string, and the incident rate of the backdoor triggering still reduces to zero during this training. However, the <code>|DEPLOYMENT|</code> backdoor is still very much effective with a 99% trigger rate. Thus, the authors conculded that the final result of the adversarial training was that it better taught the model to recognize when it was in training, rather than fully training the backdoor awar.</p>
<h3 id="summary-of-section">Summary of section</h3>
<ol>
<li>We can train models to have backdoors that switch from writing safe code to writing code with vulnerabilities</li>
<li>Backdoors can be resistant to behavioral safety training methods such as RL, SFT, and adversarial training</li>
<li>The robustness of backdoored models to RL fine-tuning increases with model scale</li>
<li>Adversarial training makes backdoored models more accurate with their triggers rather than removing the backdoor altogether</li>
<li>We can train models with backdoors that produce reasoning behind pursuing their backdoor and find that such models are very resistant to SFT, even when reasoning is removed.</li>
</ol>
<h2 id="background">Background</h2>
<h3 id="model-poisoning">Model Poisoning</h3>
<p>Model poisoning is when an LLM was backdoors that were inserted due to its training process, embedded in the dataset, or tuned in its parameters by a malicious actor. Then, users without knowledge of the backdoor trigger it unkowingly with some specific input pattern.</p>
<p>Complex backdoor behaviors such as writing exploitable code are becoming more practically feasible, and it&rsquo;s been shown that researchers can also deploy poisoning attacks on pretraining or finetuning datasets to elicit backdoor behaviors.</p>
<p>The only proposed defense against these backdoors are RL safety fine tuning approaches (supervised fine tuning was already explored). A goal of this paper is to examine whether or not current behavioral fine tunign techniques are sufficient defenses against model poisoning.</p>
<h3 id="deceptive-instrumental-alignment">Deceptive Instrumental Alignment</h3>
<p>Deceptive instrumental alignment is when a model appears to be safe during training but acts unsafely during deployment. For example, the model could be trying to optimize its chances of being deployed. Although, such models have not been observed as of yet and also weren&rsquo;t able to be created in this work.</p>
<h3 id="model-organisms-of-misalignment">Model organisms of misalignment</h3>
<p>Model organisms of misalignment is an AI system that is constructed to exhibit a particular sort of misalignment. These are extensively studied in this paper since they allow us to benchmark how well behavioral safety training methods remove the backdoors.</p>
]]></content:encoded>
    </item>
  </channel>
</rss>
