<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Notes on HasItHalted</title>
    <link>https://hasithv.github.io/tags/notes/</link>
    <description>Recent content in Notes on HasItHalted</description>
    <generator>Hugo -- 0.131.0</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 10 Aug 2024 15:41:44 -0700</lastBuildDate>
    <atom:link href="https://hasithv.github.io/tags/notes/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>6.1 - The Diffusion Limit of Random Walks</title>
      <link>https://hasithv.github.io/posts/notes/eliasa/chap6/6-1/</link>
      <pubDate>Sat, 10 Aug 2024 15:41:44 -0700</pubDate>
      <guid>https://hasithv.github.io/posts/notes/eliasa/chap6/6-1/</guid>
      <description>Random Walk Let $\{\xi_i\}$ be i.i.d. random variables such that $\xi_i = \pm 1$ with probability $1/2$. Then, define $$X_n = \sum_{k=1}^{n} \xi_k, \quad X_0 = 0.$$ $\{X_n\}$ is the familiar symmetric random walk on $\mathbb{Z}$. Let $W(m,n) = \mathbb{P}(X_N = m)$. It is easy to see that $$W(m,n) = {N \choose (N+m)/2} \left( \frac{1}{2} \right)^N$$ and that the mean and std are $$\mathbb{E}[X_N] = 0, \quad \sigma^2_{X_N} = N$$ Diffusion Coefficient Definition 6.</description>
      <content:encoded><![CDATA[<h2 id="random-walk">Random Walk</h2>
<p>Let $\{\xi_i\}$ be i.i.d. random variables such that $\xi_i = \pm 1$ with probability $1/2$. Then, define
</p>
$$X_n = \sum_{k=1}^{n} \xi_k, \quad X_0 = 0.$$
<p>
$\{X_n\}$ is the familiar symmetric random walk on $\mathbb{Z}$. Let $W(m,n) = \mathbb{P}(X_N = m)$. It is easy to see that
</p>
$$W(m,n) = {N \choose (N+m)/2} \left( \frac{1}{2} \right)^N$$
<p>
and that the mean and std are
</p>
$$\mathbb{E}[X_N] = 0, \quad \sigma^2_{X_N} = N$$
<h2 id="diffusion-coefficient">Diffusion Coefficient</h2>
<blockquote>
<p><strong>Definition 6.2:</strong> <em>(Diffusion coefficient)</em>. The diffusion coefficient $D$ is defined as
</p>
$$D = \frac{\langle (X_N - X_0)^2 \rangle}{2N}$$
<p>
And for a general stochastic process it is
</p>
$$D = \lim_{t \rightarrow \infty} \frac{\langle (X_t - X_0)^2 \rangle}{2dt}$$
<p>
where $d$ is the space dimension.</p>
</blockquote>
<p>Intuitively, the diffusion coefficient tells us the rate at which variance changes [<a href="https://physics.stackexchange.com/a/52977/250863">1</a>]. For the random walk, $D = 1/2$.</p>
<h2 id="continuum-limit-of-the-random-walk">Continuum limit of the random walk</h2>
<p>Let&rsquo;s define the step length of the random walk to be $l$ and the timestep to be $\tau$. Now, fixing $(x,t)$ consider the following limit:
</p>
$$N,m \rightarrow \infty, \quad l,\tau \rightarrow 0, \quad N\tau = t, \quad ml=x$$
<p>The diffusion coefficient is then computed as (with $d=1$)
</p>
$$\begin{align} 
D &= \lim_{t \rightarrow \infty}  \frac{\langle (X_t - X_0)^2 \rangle}{2t} \\
&= \lim_{t \rightarrow \infty}  \frac{\langle (X_{N\tau} - X_0)^2 \rangle}{2N\tau} \\
&= \lim_{t \rightarrow \infty}  \frac{\langle X_{N\tau}^2 \rangle}{2N\tau} \\
&= \lim_{t \rightarrow \infty}  \frac{N l^2}{2N\tau} \\
&= \frac{l^2}{2\tau} \\
\end{align}$$
<p>
&#x26a0;&#xfe0f; the book mentions &ldquo;fixing&rdquo; the diffusion constant, is that different from the computation above?</p>
<p>When we are taking the continuum limit of the random walk, $N, m \gg 1$, and so $m/N = (x/l) (\tau/t) = (x/t) (\tau/l) \rightarrow 0$. Thus, $m \ll N$. Now, we can expand $W(m,N)$ using Stirlings formula $\log n! = (n+\frac{1}{2})\log n - n + \frac{1}{2} \log 2\pi + O(n^{-1})$ when $n \gg 1$
</p>
$$\begin{align}
W(m,N) &= \frac{N!}{\left(\frac{N+m}{2}\right)! \left(\frac{N-m}{2}\right)!} \left(\frac{1}{2}\right)^N \\
\log W(m,n) &= \log \left( \frac{N!}{\left(\frac{N+m}{2}\right)! \left(\frac{N-m}{2}\right)!} \left(\frac{1}{2}\right)^N \right) \\
&\approx \log N! - N\log 2 - \left(\log\left(\frac{N + m}{2}\right)! + \log\left(\frac{N-m}{2}\right)! \right) \\
&\approx \left(N+\frac{1}{2}\right)\log N - N + \frac{1}{2}\log 2\pi - N\log 2 \\ 
& \quad \quad - \left(\log\left(\frac{N + m}{2}\right)! + \log\left(\frac{N-m}{2}\right)! \right) \\
&\approx \left(N+\frac{1}{2}\right)\log N - N + \frac{1}{2}\log 2\pi - N\log 2 \\ 
& \quad \quad - \frac{1}{2} \left( N + m + 1 \right) \log \left(\frac{N+m}{2}\right) + \frac{N+m}{2} - \frac{1}{2}\log 2\pi \\
& \quad \quad - \frac{1}{2} \left( N - m + 1 \right) \log \left(\frac{N-m}{2}\right) + \frac{N-m}{2} - \frac{1}{2}\log 2\pi \\
&\approx \left(N+\frac{1}{2}\right)\log N \\ 
& \quad \quad - \frac{1}{2} \left( N + m + 1 \right) \log \left( \frac{N}{2}\left(1+\frac{m}{N}\right)\right) \\
& \quad \quad - \frac{1}{2} \left( N - m + 1 \right) \log \left(\frac{N}{2}\left( 1 - \frac{m}{N} \right)\right) \\
& \quad \quad -\frac{1}{2}\log 2\pi - N\log 2
\end{align}$$
<p>
And now using that $m \ll N$ and $\log(1+x) \approx x - \frac{1}{2}x^2$ for $x \ll 1$, we have
</p>
$$\begin{align}
\log W(m,N) &\approx \left(N+\frac{1}{2}\right) \log N - (N+1) \log\left(\frac{N}{2}\right) \\
& \quad \quad - \frac{1}{2} \left( N + m + 1 \right) \log \left(1+\frac{m}{N}\right) \\
& \quad \quad - \frac{1}{2} \left( N - m + 1 \right) \log \left( 1 - \frac{m}{N} \right)\\
& \quad \quad -\frac{1}{2}\log 2\pi - N\log 2 \\
&\approx -\frac{1}{2} \log N + \log(2) - \frac{1}{2}\log 2\pi \\
& \quad \quad - \frac{1}{2} \left( N + m + 1 \right) \log \left(1+\frac{m}{N}\right) \\
& \quad \quad - \frac{1}{2} \left( N - m + 1 \right) \log \left( 1 - \frac{m}{N} \right) \\
&\approx -\frac{1}{2} \log N + \log(2) - \frac{1}{2}\log 2\pi \\
& \quad \quad - \frac{1}{2} \left( N + m + 1 \right) \left( \frac{m}{N} - \frac{1}{2}\left(\frac{m}{N}\right)^2\right) \\
& \quad \quad - \frac{1}{2} \left( N - m + 1 \right) \left( -\frac{m}{N} - \frac{1}{2}\left(\frac{m}{N}\right)^2\right) \\
&\approx -\frac{1}{2} \log N + \log(2) - \frac{1}{2}\log 2\pi - \frac{m^2}{2N} \\
\end{align}$$
$$\implies  W(m,N) \approx \left( \frac{2}{\pi N} \right)^{1/2}\exp\left(-\frac{m^2}{2N}\right) $$
<p>
If we integrate across the possible $x$ values,
</p>
$$\begin{align}
W(x,t)\Delta x &\approx \int_{x-\Delta/2}^{x+\Delta/2} W(y, t) dy \\
&\approx \sum_{\substack{k = \{m, m \pm 2, m \pm 4, \ldots\} \\ kl \in (x-\Delta x/2, x+\Delta x/2)}} W(k,N) \\
&\approx W(m,N)\frac{\Delta x}{2m}
\end{align}$$
<p>
Where $x=ml$. Then, doing some substitions, we get
</p>
$$W(x,t) = \frac{1}{\sqrt{4\pi D t} } \exp\left(-\frac{x^2}{4Dt}\right)$$
<p>
It is interesting to see that $W$ satisfies the heat equation with the initial condition
</p>
$$\begin{cases}
    \frac{\partial W(x,t)}{\partial t} = D \frac{\partial^2 W(x,t)}{\partial t^2} \\
    W(x,0) = \delta(x)
\end{cases}$$
<p>
I wonder how it satisfies the boundary condition of maintaining a constant area under the graph for all time $t$.</p>
<h2 id="arcsine-law">Arcsine Law</h2>
<p>Define $P_{2k,2n}$ to be the probability that a particle remains positive for $2k$ time steps before $2n$ time steps have passed. And a particle is on the positive side in an interval $[n-1,n]$ if either $X_{n-1}$ or $X_{n}$ are positive. It is true that
</p>
$$P_{2k,2n} = u_{2k}u_{2n-2k}$$
<p>
where $u_{2k} = \mathbb{P}(X_{2k} = 0)$ (&#x26a0;&#xfe0f; the proof is non-trivial, and not too instructive so it is omitted. Though, if there is an intuitive reason, I&rsquo;d like to hear it) [<a href="https://math.osu.edu/sites/math.osu.edu/files/What_is_2018_Arcsine_Law.pdf">2</a>]. Now, let $\gamma(2n)$ be the number of time units that the particle spends on the positive axis in the interval $[0,2n]$. Then when $x \leq 1$,
</p>
$$\mathbb{P}\left(\frac{1}{2} < \frac{\gamma (2n)}{2n} \leq x \right) = \sum_{k,1/2<2k/2n\leq x} P_{2k,2n}$$
<p>
From the expression of $W(m,N)$,
</p>
$$u_{2k} \sim \frac{1}{\sqrt{\pi k}}, \quad P_{2k,2n} \sim \frac{1}{\pi \sqrt{k(n-k)}}$$
<p>
as $k, n-k \rightarrow \infty$. And so,
</p>
$$\begin{align}
\mathbb{P}\left(\frac{1}{2} < \frac{\gamma (2n)}{2n} \leq x \right) &= \sum_{k,1/2<2k/2n\leq x} P_{2k,2n} \\
&= \sum_{k,1/2<2k/2n\leq x}  \frac{1}{\pi n \sqrt{(k/n)(1-k/n)}} \\
&\rightarrow \frac{1}{\pi} \int_\frac{1}{2}^x \frac{dt}{\sqrt{t(1-t)}}
\end{align}$$
<p>Which leads us to</p>
<blockquote>
<p><strong>Theorem 6.3:</strong> <em>(Arcsine law).</em> The probability that the fraction of time spenty by a particle ont he positive side is at most $x$ tends to $\frac{2}{\pi}\arcsin \sqrt{x}$:
</p>
$$ \mathbb{P}\left(\frac{\gamma (2n)}{2n} \leq x \right) = \frac{2}{\pi} \arcsin \sqrt{x} $$
<p>
The consequence of this theorem is that it is most likely for a radnom walk to spend either almost all of its time on the positive side, or for it to spend almost no time on the positive side.</p>
<p>We can do this because there was no reason to limit the lower bound to $1/2$ rather than $0$ in the derivation</p>
</blockquote>
<h2 id="references">References</h2>
<ol>
<li>Helena (<a href="https://physics.stackexchange.com/users/12948/helena)">https://physics.stackexchange.com/users/12948/helena)</a>, What is the physical meaning of diffusion coefficient?, URL (version: 2014-12-03): <a href="https://physics.stackexchange.com/q/52977https://physics.stackexchange.com/a/52977/250863">https://physics.stackexchange.com/q/52977https://physics.stackexchange.com/a/52977/250863</a></li>
<li>Ackelsberg, Ethan (<a href="https://math.osu.edu/sites/math.osu.edu/files/What_is_2018_Arcsine_Law.pdf)">https://math.osu.edu/sites/math.osu.edu/files/What_is_2018_Arcsine_Law.pdf)</a>, What is the Arcsine Law?, URL (version: 2024-08-11): <a href="https://math.osu.edu/sites/math.osu.edu/files/What_is_2018_Arcsine_Law.pdf">https://math.osu.edu/sites/math.osu.edu/files/What_is_2018_Arcsine_Law.pdf</a></li>
</ol>
]]></content:encoded>
    </item>
    <item>
      <title>5.4 - Gaussian Processes</title>
      <link>https://hasithv.github.io/posts/notes/eliasa/chap5/5-4/</link>
      <pubDate>Tue, 06 Aug 2024 21:17:49 -0700</pubDate>
      <guid>https://hasithv.github.io/posts/notes/eliasa/chap5/5-4/</guid>
      <description>Definition 5.9: A stochasitc process $\{X_t\}_{t \geq 0}$ is a Gaussian Process if its finite dimensional distributions are consistent Gaussian measures for any $0 \leq t_1 &lt; t_2 &lt; \ldots &lt; t_k$.
Recall that a Gaussian random vector $\mathbf{X} = (X_1, X_2,\ldots,X_n)^T$ is completely characterized by its first and second moments $$\mathbf{m} = \mathbb{E}[\mathbf{X}], \quad \mathbf{K} = \mathbb{E}[(\mathbf{X} - \mathbf{m}) (\mathbf{X} - \mathbf{m})^T]$$ Meaning that the characteristic function is expressed only in terms of $\mathbf{m}$ and $\mathbf{K}$ $$\mathbb{E}\left[e^{i \mathbf{\xi} \cdot \mathbf{X}}\right] = e^{i \mathbf{\xi} \cdot \mathbf{m} - \frac{1}{2}\mathbf{\xi}^T \mathbf{K} \mathbf{\xi}} $$ This means that for any $0 \leq t_1 &lt; t_2 &lt; \ldots &lt; t_k$, the measure $\mu_{t_1, t_2, \ldots, t_k}$ is uniquely determined by an $\mathbf{m} = (m(t_1), \ldots, m(t_k))$ and a covariance matrix $\mathbf{K}_{ij} = K(t_i, t_j)$.</description>
      <content:encoded><![CDATA[<blockquote>
<p><strong>Definition 5.9:</strong> A stochasitc process $\{X_t\}_{t \geq 0}$ is a <em>Gaussian Process</em> if its finite dimensional distributions are consistent Gaussian measures for any $0 \leq t_1 < t_2 < \ldots < t_k$.</p>
</blockquote>
<p>Recall that a Gaussian random vector $\mathbf{X} = (X_1, X_2,\ldots,X_n)^T$ is completely characterized by its first and second moments
</p>
$$\mathbf{m} = \mathbb{E}[\mathbf{X}], \quad \mathbf{K} = \mathbb{E}[(\mathbf{X} - \mathbf{m}) (\mathbf{X} - \mathbf{m})^T]$$
<p>Meaning that the characteristic function is expressed only in terms of $\mathbf{m}$ and $\mathbf{K}$
</p>
$$\mathbb{E}\left[e^{i \mathbf{\xi} \cdot \mathbf{X}}\right] = e^{i \mathbf{\xi} \cdot \mathbf{m} - \frac{1}{2}\mathbf{\xi}^T \mathbf{K} \mathbf{\xi}} $$
<p>
This means that for any $0 \leq t_1 < t_2 < \ldots < t_k$, the measure $\mu_{t_1, t_2, \ldots, t_k}$ is uniquely determined by an $\mathbf{m} = (m(t_1), \ldots, m(t_k))$ and a covariance matrix $\mathbf{K}_{ij} = K(t_i, t_j)$. Because our $\mu$ satisifes the conditions for Kolomorov&rsquo;s extension theorem, we have a probability space and a stochastic process associated with $\mu$.</p>
<p>&#x26a0;&#xfe0f; Isn&rsquo;t one of the conditions of Kolmogorov&rsquo;s extension theoren that we need to be able to permute the $t_i$? How would this work if we require the $t_i$ to be increasing?</p>
<blockquote>
<p><strong>Theorem 5.10:</strong> Assuming that the stochastic process $\{X_t\}_{t\in[0,T]}$ satisfies
</p>
$$\mathbb{E} \left[ \int_0^T X_t^2 dt \right] < \infty$$
<p>
then $m \in L^2_t$. Also, the operator
</p>
$$\mathcal{K} f(s) := \int_0^T K(s,t) f(t) dt$$
<p>
is nonnegative, compact on $L^2_t$</p>
</blockquote>
<blockquote>
<p><strong>Proof:</strong> For the first statement,
</p>
$$\int_0^T \mathbb{E}[X]^2 dt \leq \int_0^T \mathbb{E}[X_t^2] dt < \infty$$
<p>
For the second,
</p>
$$\begin{align}
\int_0^T \int_0^T K^2(s,t) ds dt &= \int_0^T \int_0^T \mathbb{E}[\left((X_t - m(t))(X_s - m(s))\right)^2]ds dt \\
&\leq \int_0^T \int_0^T \mathbb{E}[(X_t - m(t))^2]\mathbb{E}[(X_s - m(s))^2]ds dt \\
&\leq \left( \int_0^T \mathbb{E}[X_t] dt \right) \\
&\leq \infty 
\end{align}$$
<p>
which lets us conclude that $K \in L^2([0,T] \times [0,T])$, which tells us that $\mathcal{K}$ is compact on $L^2_t$</p>
<p>&#x26a0;&#xfe0f; I can&rsquo;t properly find what theorem lets us say the last statement, but I can trust it for now.</p>
<p>Furthermore, noting that $K$ is symmetric which means that $\mathcal{K}$ is self adjoint, and we can say
</p>
$$(\mathcal{K}f, f) = \int_0^T \int_0^T \mathbb{E}[(X_t - m(t))]\mathbb{E}[(X_s - m(s))]f(t)f(s) ds dt \geq 0$$
<p>
by symmetry of $s$ and $t$.</p>
</blockquote>
<p>If we want to extend the characteristic to $L_t$ rather than the finite dimensional version, we can write
</p>
$$\mathbb{E}[e^{i(\xi,X)}] = e^{i(\xi,m) - \frac{1}{2} (\xi, \mathcal{K} \xi)}, \quad \xi \in L^2_t$$
<p>
With $(\xi,m) = \int_a^b \xi(t) m(t) dt$ and $\mathcal{K}\xi (t) = \int_a^b K(t,s) \xi(s) ds$. This is a fairly reasonable extrapolation from the finite dimensional case.</p>
<blockquote>
<p><strong>Theorem 5.13:</strong> <em>Karhunen-Loeve expansion</em>. Let $(X_t)_{t \in [0,1]}$ be a Gaussian process with mean 0 and covariance function $K(s,t)$, Assume that $K$ continuous and $\{\lambda_k\}$ be the set of eigenvalues for orthonormal eigenfunctions of $K$, $\{\phi_k\}$. Then, $X_t$ has the representation of
</p>
$$X_t = \sum_{k=1}^\infty \alpha_k \sqrt{\lambda_k} \phi_k(t)$$
<p>
Where $\alpha_k$ is a standard normal random variable $\mathscr{N}(0,1)$</p>
<p>&#x26a0;&#xfe0f; I am omitting the proof because I feel the result is easy enough to intuitively grasp, and also it is a little theoretical, so maybe I should revisit it if I get more comfortable with proving covergence in probability.</p>
</blockquote>
]]></content:encoded>
    </item>
  </channel>
</rss>
