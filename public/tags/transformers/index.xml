<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Transformers on HasithAlted</title>
    <link>https://hasithv.github.io/tags/transformers/</link>
    <description>Recent content in Transformers on HasithAlted</description>
    <generator>Hugo -- 0.147.2</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 28 May 2025 12:31:44 -0500</lastBuildDate>
    <atom:link href="https://hasithv.github.io/tags/transformers/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Induction Heads in Chronos Part 2</title>
      <link>https://hasithv.github.io/posts/25-05-28-chronosinduction2/</link>
      <pubDate>Wed, 28 May 2025 12:31:44 -0500</pubDate>
      <guid>https://hasithv.github.io/posts/25-05-28-chronosinduction2/</guid>
      <description>&lt;script src=&#34;https://cdn.plot.ly/plotly-3.0.1.min.js&#34;&gt;&lt;/script&gt;
&lt;p&gt;Previously, in the &lt;a href=&#34;https://hasithv.github.io/posts/25-05-11-chronosinductionheads/&#34;&gt;part 1 post&lt;/a&gt;, we found some evidence that induction heads exist in the Chronos models [&lt;a href=&#34;https://hasithv.github.io/posts/25-05-11-chronosinductionheads/&#34;&gt;1&lt;/a&gt;]. However, there were some things I did incorrectly and some things I wanted to further explore:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;First, my implementation of the repeated random tokens (RRT) method was incorrect. Namely, I randomly sampled over all the non-special tokens, but Chronos scales the given input such the encoder input tokens almost always fall within a range of token ids from &lt;code&gt;1910-2187&lt;/code&gt;. Sampling over only this range greatly improved the attention mosaics.&lt;/li&gt;
&lt;li&gt;I wanted to further study how changing the number of repeitions and the lengths of the individual sequences in the RRT affects how many induction heads we detect.&lt;/li&gt;
&lt;li&gt;I wanted to go beyond RRT data and see if we can find any interesting inductive properties in multisine data.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;background&#34;&gt;Background&lt;/h2&gt;
&lt;p&gt;First, let me clear up what an induction head actually is in a more concrete way than my last post.&lt;/p&gt;</description>
      <content:encoded><![CDATA[<script src="https://cdn.plot.ly/plotly-3.0.1.min.js"></script>
<p>Previously, in the <a href="/posts/25-05-11-chronosinductionheads/">part 1 post</a>, we found some evidence that induction heads exist in the Chronos models [<a href="/posts/25-05-11-chronosinductionheads/">1</a>]. However, there were some things I did incorrectly and some things I wanted to further explore:</p>
<ol>
<li>First, my implementation of the repeated random tokens (RRT) method was incorrect. Namely, I randomly sampled over all the non-special tokens, but Chronos scales the given input such the encoder input tokens almost always fall within a range of token ids from <code>1910-2187</code>. Sampling over only this range greatly improved the attention mosaics.</li>
<li>I wanted to further study how changing the number of repeitions and the lengths of the individual sequences in the RRT affects how many induction heads we detect.</li>
<li>I wanted to go beyond RRT data and see if we can find any interesting inductive properties in multisine data.</li>
</ol>
<h2 id="background">Background</h2>
<p>First, let me clear up what an induction head actually is in a more concrete way than my last post.</p>
<blockquote>
<p><strong>Definition</strong> (Induction Head): Suppose we have some set of $T$ tokens, $S=[s_1, s_2, \ldots, s_T]$. Then, an attention head $h$ in layer $\ell$, $A^{(\ell, h)}$ is an induction head if for all collections $S$, token $s_T$ attends very strongly to its most recent occurence. That is, for all $S$ with a well defined $m = \max\{ t | t < T, s_t = s_T\}$ and $n=m+1$, a head $A^{(\ell, h)}$ is an induction head if
</p>
$$\max \left\{ A^{(\ell, h)}_{s_T, s_m}, A^{(\ell, h)}_{s_T, s_n} \right\} \gg \frac{1}{T}, $$<p>
where $A^{(\ell, h)}_{s_i, s_j}$ is how much token $s_i$ attends to $s_j$ in head $A^{(\ell, h)}$.</p></blockquote>
<p>The <a href="https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html">Anthropic post</a> which extensively studies induction heads also added an additional requirement that the head must increase the probability of the next token in the sequence appearing [<a href="https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html">2</a>], but I want to relax that constraint and consider any head that attends to the previous instance of the current token as an induction head.</p>
<p>In practice, I found that having a minimum threshold attention score of $0.3$ for induction heads works fairly well, and we can use the <a href="/posts/25-05-11-chronosinductionheads/#repeated-random-tokens">RRT</a> instead of going over every possible sequence $S$.</p>
<h2 id="corrected-induction-mosaics">Corrected Induction Mosaics</h2>
<p>Since Chronos usually scales its input ids to have tokens between <code>1911</code> and <code>2187</code>, it makes more sense to uniformly sample tokens for RRT over this range. With this updated sampling method, here are the new induction mosaics (use the dropdown to change models):</p>
<style>
  .plot-container {
    width: 700px;
    height: 400px;
    margin: 0 auto 2rem; /* center and add space below */
  }
</style>
<div>
  <div id="mosaic"  class="plot-container"></div>
</div>
<script>
  const specs = [
    { id: 'mosaic',  src: './json/corrected_mosaics.json'  },
  ];

  specs.forEach(({id, src}) => {
    fetch(src)
      .then(r => {
        if (!r.ok) throw new Error(`Failed to load ${src}: ${r.status}`);
        return r.json();
      })
      .then(cfg => {
        const layout = { ...cfg.layout, width: 700, height: 400 };
        const config = { ...cfg.config, responsive: true };
        Plotly.newPlot(id, cfg.data, layout, config);
      })
      .catch(err => console.error(err));
  });
</script>
<p>We now see much stronger evidence of induction heads, and the number of induction heads increases with the size of the model.</p>
<h2 id="rrt-implementation-details">RRT Implementation Details</h2>
<p>In the above plots, I used a sequence length of 10 tokens and repeated them twice, so an example sequence of tokens (using letters instead of token ids for clarity purposes) would be <code>ABCDEFGHIJABCDEFGHIJA&lt;EOS&gt;&lt;DEC_START&gt;B</code>; where our twice-repeated sequence with a length of <code>10</code> is <code>ABCDEFGHIJ</code>, <code>&lt;EOS&gt;</code> marks the end of the encoder inputs, and <code>DEC_START</code> marks the start of the the decoder inputs. However, I was curious as to how varying the length of the repeated squence and even the number of times we repeat the sequence would affect how many induction heads we detect with RRT, so I varied both the repetitions and sequence lengths between <code>[2,4,6,8,10]</code> and produced the following plot.</p>
<p><a href="./plots/rf_vs_sl.png"><figure class="align-center ">
    <img loading="lazy" src="./plots/rf_vs_sl.png#center"
         alt="Figure 1: The effect on the number of induction heads we detect with RRT as we vary the sequence length and number of repetitions. The overall effect is that decreasing the number of repetitions but increasing the sequence length gives us the most detected induction heads across all models." width="1200px"/> <figcaption>
            <p>Figure 1: The effect on the number of induction heads we detect with RRT as we vary the sequence length and number of repetitions. The overall effect is that decreasing the number of repetitions but increasing the sequence length gives us the most detected induction heads across all models.</p>
        </figcaption>
</figure>
</a></p>
<p>Clearly, we see that increasing the repetitions dereases the number of induction heads we see while using longer sequences increases the number of induction heads. I am not sure why the latter happens, but for increasing the number of repeitions, I think the heads are spacing out their attention between all prior occurences instead of just the most recent one&ndash;although I haven&rsquo;t looked into backing up that claim. I decided to use <code>repetitions=2</code> and <code>sequence_length=10</code> because that configuration seems to maximize the number of induction heads we can work with.</p>
<h2 id="multisine-data">Multisine data</h2>
<p>For a given set of frequencies, $k = \{ k_1, k_2, \ldots, k_n \}$, amplitudes $a = \{a_1, a_2, \ldots, a_n\}$, and phase shifts $\{\phi_1, \phi_2, \ldots, \phi_n\}$, a multisine function is one of the form
</p>
$$f(t) = \sum_{i=1}^n a_i \sin ( 2\pi k_i t + \phi_i).$$<p>
In a way, multisine data also resembles a sequence of repeated tokens, although not random. So, I was wondering if the attention heads would do anything interesting in attending to the context.</p>
<p>To explore this idea, I first decided to look at the frequences <code>k=[5,10,20,40]</code> and amplitudes <code>a=[1.0,0.5,0.25,0.25]</code>. First, I looked at the attention scores across all heads and layers of each model throughout the context (which was the multisine data), then, I plotted the FFT of both the data and the attention scores, and then I did another FFT of the attention scores. Here is the result for the base model (I think it best illustrates the point I will make later; the other plots can be found here: <a href="./plots/fattn_mini.png">mini</a>, <a href="./plots/fattn_small.png">small</a>, <a href="./plots/fattn_large.png">large</a>):</p>
<p><a href="./plots/fattn_base.png"><figure class="align-center ">
    <img loading="lazy" src="./plots/fattn_base.png#center"
         alt="Figure 2: The attention scores of the multisine data across all heads and layers of the base model, the FFT of the data, the FFT of the attention scores, and the double FFT of the attention scores. A high attention score means that the head is more likely to be an induction head, any attention score above 0.3 was set to 0.3 for better visualization. Notice how the FFTs of the attention heads are highly periodic with a period of about 5Hz, reflected in the double FFT plot. Additionally, the heads with the highest amplitudes in the FFT plot and the double FFT plot are the heads with the higher attention scores in the RRT test." width="1200px"/> <figcaption>
            <p>Figure 2: The attention scores of the multisine data across all heads and layers of the base model, the FFT of the data, the FFT of the attention scores, and the double FFT of the attention scores. A high attention score means that the head is more likely to be an induction head, any attention score above 0.3 was set to 0.3 for better visualization. Notice how the FFTs of the attention heads are highly periodic with a period of about 5Hz, reflected in the double FFT plot. Additionally, the heads with the highest amplitudes in the FFT plot and the double FFT plot are the heads with the higher attention scores in the RRT test.</p>
        </figcaption>
</figure>
</a></p>
<p>The extremely periodic behavior of the FFT of the attention scores is no coincidence. After running lots of configurations of the multisine data, I found that the period of the FFT of the attention scores corresponds exactly to the smallest difference in frequenies in the multisine data. For example, if we have a multisine with frequencies $k=\{5,10,20,40\}$, the smallest difference in frequencies is 5Hz because 10Hz-5Hz=5Hz.</p>
<p>Whats more is that the induction heads make up the heads with the highest overall amplitudes in the FFT plot as well as the double FFT plot. This is a very interesting result, and I am not exactly sure what the mechanics behind this are, but the implication is that the induction heads tend to pick up on the periodic nature of the data better than other heads.</p>
<h2 id="conclusion">Conclusion</h2>
<p>Here are the main takeaways from this post:</p>
<ol>
<li>We find much stronger evidence of induction heads if we modify the RRT test to sample over more commonly used tokens.</li>
<li>The sequence length and number of repetitions in the RRT test greatly affect the number of induction heads we detect, with longer sequences and less repetitions resulting in heads with hihger average attention scores to the most recent occurence of the current token.</li>
<li>The periodicicity of multisine data is picked up on by the attention heads in the Chronos models, and the induction heads are particularly good at this.</li>
<li>The FFT of the attention heads in the models are also very periodic, and their periods correspond to the smallest difference in frequencies in the multisine data.</li>
</ol>
<p>I think there could be more to explore in point 4, because it seems a little wasteful to me for attention heads to have spikes in the FFT plots for so many frequencies, when the data is only periodic with just a few frequencies.</p>
<h2 id="references">References</h2>
<p>[1] <a href="/posts/25-05-11-chronosinductionheads/">https://hasithv.github.io/posts/25-05-11-chronosinductionheads/</a></p>
<p>[2] <a href="https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html">https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html</a></p>
]]></content:encoded>
    </item>
    <item>
      <title>Hunting for Induction Heads in Amazon&#39;s Chronos</title>
      <link>https://hasithv.github.io/posts/25-05-11-chronosinductionheads/</link>
      <pubDate>Sun, 11 May 2025 10:50:18 -0500</pubDate>
      <guid>https://hasithv.github.io/posts/25-05-11-chronosinductionheads/</guid>
      <description>&lt;script src=&#34;https://cdn.plot.ly/plotly-3.0.1.min.js&#34;&gt;&lt;/script&gt;
&lt;p&gt;&lt;em&gt;Notice: While the theory here is correct, I realized I had some implementation errors which are corrected in a &lt;a href=&#34;https://hasithv.github.io/posts/25-05-28-chronosinduction2/&#34;&gt;follow up post&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;This Summer, I expect to be working on things related to mechanistic intepretability in time series forecasting, and a model of interest was &lt;a href=&#34;https://github.com/amazon-science/chronos-forecasting&#34;&gt;Amazon&amp;rsquo;s Chronos model&lt;/a&gt;, a probabilistic time series forecasting model. To better understand how the model works and to get my hands dirty with some MI work, I decided to try and look for evidence of induction heads in Chronos.&lt;/p&gt;</description>
      <content:encoded><![CDATA[<script src="https://cdn.plot.ly/plotly-3.0.1.min.js"></script>
<p><em>Notice: While the theory here is correct, I realized I had some implementation errors which are corrected in a <a href="/posts/25-05-28-chronosinduction2/">follow up post</a>.</em></p>
<p>This Summer, I expect to be working on things related to mechanistic intepretability in time series forecasting, and a model of interest was <a href="https://github.com/amazon-science/chronos-forecasting">Amazon&rsquo;s Chronos model</a>, a probabilistic time series forecasting model. To better understand how the model works and to get my hands dirty with some MI work, I decided to try and look for evidence of induction heads in Chronos.</p>
<p><em>Note: all code and data for this project can be found in the <a href="https://github.com/hasithv/chronos_induction">github repository</a> [<a href="https://github.com/hasithv/chronos_induction">1</a>]</em></p>
<h2 id="background">Background</h2>
<p>To begin with, let&rsquo;s remind ourselves what induction heads are. In transformer models (if you aren&rsquo;t familiar with how transformer models work, refer to section 5.1 of <a href="/posts/25-05-11-chronosinductionheads/thesis.pdf">my report</a> [<a href="/posts/25-05-11-chronosinductionheads/thesis.pdf">2</a>] on universal approximation properties of neural networks and trasnformers), we have attention heads which take in some data of length $T$ tokens that has an embedding dimension of $d$, $X \in \mathbb{R}^{T \times d}$ and applies the following transformation to it:
</p>
$$
\begin{align*}
Q = XW^Q, \\
K = XW^K, \\
V = XW^V,
\end{align*}
$$<p>
</p>
$$
A = \mathcal{S}\left(\text{MASK}\left(\frac{QK^\top}{\sqrt{d}}\right)\right)V,
$$<p>
where $\mathcal{S}$ is the row-wise softmax function, $\text{MASK}$ is a masking function which masks out certain tokens we don&rsquo;t want to consider during inference (by setting them to $-\infty$); $W^Q, W^K, W^V \in \mathbb{R}^{d \times d'}$ are the learnable parameters of the model; and $Q, K, V \in \mathbb{R}^{T \times d'}$ are the query, key, and value matrices, respectively.</p>
<p>Note: the query can process any number of tokens at a time, so it can be possible that $Q=X' W^Q$, where $X' \in \mathbb{R}^{S \times d}$. Then, $A \in \mathbb{R}^{S \times d'}$, so the size of $Q$ will determine the number of output tokens. But the key and value matrices will still be of size $T \times d'$.</p>
<p>In a way, $A$&ndash;the attention head&ndash;is a weighted sum of the values $V$, where the weights are given by the softmax of the attention scores. For example, let&rsquo;s say we are looking at a singular query $q_i \in \mathbb{R}^{1 \times d'}$, which corresponds to the $i$-th token in the sequence. Then, the attention head (ignoring any masking) is computing the following weighted sum:
</p>
$$
A_{1,i} = \sum_{j=1}^T \frac{\exp(q_i k_j^\top)}{\sum_{j'=1}^T \exp(q_i k_{j'}^\top)} v_j,
$$<p>
where $A_{1,i}$ is the $i$-th token in the output of the attention head and $k_j,v_j \in \mathbb{R}^{d'}$ are the $j$-th key and value vectors (the $j$-th rows of the matrices), respectively. With this formulation, we can explicity see that the attention head is computing a weighted sum of the the value vectors. The weight ascribed to each $j$-th token, $\frac{\exp(q_i k_j^\top)}{Z}$, is referred to as the &lsquo;attention score&rsquo; and is how much the $i$-th token attends to the $j$-th token.</p>
<h2 id="induction-heads">Induction Heads</h2>
<p>Sometimes, attention heads are able to learn to attend to the previous copy of the current token. For example, if we have the sequence <code>ABCPQRABCP</code>, then the 10th token <code>P</code> will attend highly to the 4th token since it was the most recent instance of <code>P</code> in the sequence. Other times, the attention head might attend to the token to the <em>right</em> of the most recent instance of the current token. Both of these types of attention heads are examples of induction heads.</p>
<p>Induction heads are very useful for in-context learning (ICL) as found by <a href="https://arxiv.org/abs/2407.07011">Crosbie and Shutova</a> [<a href="https://arxiv.org/abs/2407.07011">3</a>] since they allow for zero-shot pattern-matching. When the strongest 1-3% of the induction heads are ablated by either zeroing the heads or by setting them to the mean element of the head, the model&rsquo;s performance in ICL tasks drops significantly.</p>
<h2 id="induction-heads-in-chronos">Induction Heads in Chronos</h2>
<h3 id="gut-check">Gut Check</h3>
<p>Based on <a href="https://openreview.net/pdf?id=TqYjhJrp9m">this paper</a> [<a href="https://openreview.net/pdf?id=TqYjhJrp9m">4</a>] by Zhang and Gilpin, Chronos has been shown to exhibit ICL and context parroting, which gives us good reason to believe that induction heads do indeed exist in the Chronos models. In fact, when following the tutorial straight form the Chronos <code>README.md</code> file, I was able to find some hints of induction heads in the <code>t5-small</code> model as shown in Fig 1.</p>
<p><a href="./images/passengers.png"><figure class="align-center ">
    <img loading="lazy" src="./images/passengers.png#center"
         alt="Figure 1: Visualization of the attention heads of the t5-small model. The data has a clear periodicity, and the attention heads are able to pick up on it as seen by the attention scores of some heads spiking at integer multiples of the period. In layers 3-5, we can further see some heads that are attending highly to the the last instance of a valley, which is also what the current token is. Data courtesy of Aileen Nielsen, click to expand image." width="1200px"/> <figcaption>
            <p>Figure 1: Visualization of the attention heads of the <code>t5-small</code> model. The data has a clear periodicity, and the attention heads are able to pick up on it as seen by the attention scores of some heads spiking at integer multiples of the period. In layers 3-5, we can further see some heads that are attending highly to the the last instance of a valley, which is also what the current token is. Data courtesy of Aileen Nielsen, click to expand image.</p>
        </figcaption>
</figure>
</a></p>
<p>Figure 1 shows the <code>t5-small</code> model being tasked with predicting the next value in a highly periodic dataset, where the current token (the current value) is best descirbed as a &lsquo;valley&rsquo; in the data. Then, we are able to observe attention heads in layers 3-5 that attend to the most recent prior instance of a valley, and we even see other heads that attend to all valleys in the data in other layers.</p>
<h3 id="repeated-random-tokens">Repeated Random Tokens</h3>
<p>Seeing such patterns in attention are very indiciative of induction heads. One standard way to detect induction heads is the Repeated Random Tokens (RRT) test, where&ndash;as the name suggests&ndash;we repeat a random sequence of tokens and find how highly the model attends to the previous instance of the current token (and/or the token to the right of it).</p>
<p>For example, if our random sequence is <code>ABC</code>, we would feed the model <code>ABCABCA</code> as context, and collect data on how highly the 7th token <code>A</code> attends to the 4th token <code>A</code>. To visualize this, we use an Induction Mosaic, which is a heatmap of average RRT scores for each layer and head for a model. You can find induction mosaics for various small LLMs on <a href="https://www.neelnanda.io/mosaic">Neel Nanda&rsquo;s page</a> [<a href="https://www.neelnanda.io/mosaic">5</a>].</p>
<p>But since Chronos is based on the <a href="https://arxiv.org/abs/1910.10683v4">T5 architecture</a> [<a href="https://arxiv.org/abs/1910.10683v4">6</a>], we have an encoder-decoder network, so instead of solely feeding the RRT into the encoder, I gave the last token of the RRT to the decoder as context. Meaning that the context looks like <code>ABCABC[EOS][DEC_START]A</code>, where <code>[EOS]</code> denotes the end of the encoder&rsquo;s input and <code>[DEC_START]</code> denotes the start of the decoder&rsquo;s input.</p>
<p>With this setup and averaging over 100 such sequences, here are the induction mosaics for the <code>t5-base</code> and <code>t5-large</code> models:</p>
<style>
  .plot-container {
    width: 700px;
    height: 400px;
    margin: 0 auto 2rem; /* center and add space below */
  }
</style>
<div>
  <div id="plot-base"  class="plot-container"></div>
  <div id="plot-large" class="plot-container"></div>
</div>
<script>
  const specs = [
    { id: 'plot-base',  src: './json/chronos-t5-base.json'  },
    { id: 'plot-large', src: './json/chronos-t5-large.json' }
  ];

  specs.forEach(({id, src}) => {
    fetch(src)
      .then(r => {
        if (!r.ok) throw new Error(`Failed to load ${src}: ${r.status}`);
        return r.json();
      })
      .then(cfg => {
        const layout = { ...cfg.layout, width: 700, height: 400 };
        const config = { ...cfg.config, responsive: true };
        Plotly.newPlot(id, cfg.data, layout, config);
      })
      .catch(err => console.error(err));
  });
</script>
<p>In the above plots, we see that both the <code>t5-base</code> and <code>t5-large</code> models have fairly strong induction heads (heads with scores above 0.3). What&rsquo;s even more interesting is that both models seem to use earlier layers for token matching (attending to the most recent instance of the current token) and reserve later layers for <em>next</em> token matching (attending to the token to the right of the most recent instance of the current token).</p>
<h3 id="smaller-models">Smaller Models</h3>
<p>You may be wondering why I didn&rsquo;t include the <code>t5-small</code> model in the above plots. The reason is that the <code>t5-small</code> and <code>t5-mini</code> models don&rsquo;t show induction heads through RRT:</p>
<style>
  .plot-container {
    width: 700px;
    height: 400px;
    margin: 0 auto 2rem; /* center and add space below */
  }
</style>
<div>
  <div id="plot-small"  class="plot-container"></div>
  <div id="plot-mini"  class="plot-container"></div>
</div>
<script>
  const specs_small = [
    { id: 'plot-small', src: './json/chronos-t5-small.json' },
    { id: 'plot-mini', src: './json/chronos-t5-mini.json' }
  ];

  specs_small.forEach(({id, src}) => {
    fetch(src)
      .then(r => {
        if (!r.ok) throw new Error(`Failed to load ${src}: ${r.status}`);
        return r.json();
      })
      .then(cfg => {
        const layout = { ...cfg.layout, width: 700, height: 400 };
        const config = { ...cfg.config, responsive: true };
        Plotly.newPlot(id, cfg.data, layout, config);
      })
      .catch(err => console.error(err));
  });
</script>
<p>It seems that only the larger models have learned induction heads through RRT. But still, we clearly saw in Figure 1 that the <code>t5-small</code> model is able to pick up on the periodicity of the data, so what gives? Well, I have a few ideas:</p>
<ol>
<li>The smaller models do have induction heads, but they only show up in some kind of forier series setting.</li>
<li>The smaller models don&rsquo;t have induction heads and are simply good regressors. If this were the case, then that would imply that the larger models squeeze more performance out of pattern matching while the smaller ones don&rsquo;t.</li>
<li>The RRT test is only one way to detect induction heads, and it just so happens that the smaller models don&rsquo;t &lsquo;pass&rsquo; it.</li>
</ol>
<p>I think that the true reason is likely a combination of all of these, but I don&rsquo;t yet have any evidence to support any of these claims. I may do follow up experiments to dig deeper.</p>
<h2 id="conclusion">Conclusion</h2>
<p>I was successfully able to find evidence of induction heads in the larger Chronos models and even discovered that they use earlier layers attend to the current token while the later layers attend to the token that is to the right of the current token.</p>
<p>However, I wasn&rsquo;t able to find evidence of induction heads in the smaller models, but this poses an interesting question as to why the models don&rsquo;t exhibit induction in the RRT test but do show inductive capabilities when forecasting periodic data.</p>
<hr>
<h2 id="references">References</h2>
<p>[1] <a href="https://github.com/hasithv/chronos_induction">https://github.com/hasithv/chronos_induction</a></p>
<p>[2] <a href="./thesis.pdf">My undergraduate thesis</a></p>
<p>[3] <a href="https://arxiv.org/abs/2407.07011">https://arxiv.org/abs/2407.07011</a></p>
<p>[4] <a href="https://openreview.net/pdf?id=TqYjhJrp9m">https://openreview.net/pdf?id=TqYjhJrp9m</a></p>
<p>[5] <a href="https://www.neelnanda.io/mosaic">https://www.neelnanda.io/mosaic</a></p>
<p>[6] <a href="https://arxiv.org/abs/1910.10683v4">https://arxiv.org/abs/1910.10683v4</a></p>
]]></content:encoded>
    </item>
  </channel>
</rss>
