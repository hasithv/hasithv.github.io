<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Mechanistic Interpretability on HasithAlted</title>
    <link>https://hasithv.github.io/tags/mechanistic-interpretability/</link>
    <description>Recent content in Mechanistic Interpretability on HasithAlted</description>
    <generator>Hugo -- 0.147.2</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 02 Jun 2025 10:15:15 -0500</lastBuildDate>
    <atom:link href="https://hasithv.github.io/tags/mechanistic-interpretability/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Auditing CNNs with Adversarial Vulnerability</title>
      <link>https://hasithv.github.io/posts/25-06-02-cnnadversarialvuln/</link>
      <pubDate>Mon, 02 Jun 2025 10:15:15 -0500</pubDate>
      <guid>https://hasithv.github.io/posts/25-06-02-cnnadversarialvuln/</guid>
      <description>&lt;p&gt;&lt;em&gt;This post is a result I came across while working on a &lt;a href=&#34;https://sparai.org/&#34;&gt;SPAR&lt;/a&gt;-sponsored project mentored by &lt;a href=&#34;https://7vik.io&#34;&gt;Satvik Golechha&lt;/a&gt;. You can read our full report here: &lt;a href=&#34;https://docs.google.com/document/d/1nWWDP0-NExy41nIJQXgH2aHa7daeiz7VYq_N1KinEs8/&#34;&gt;Near Zero-Knowledge Detection of Undesired Behavior&lt;/a&gt; [&lt;a href=&#34;https://docs.google.com/document/d/1nWWDP0-NExy41nIJQXgH2aHa7daeiz7VYq_N1KinEs8/&#34;&gt;1&lt;/a&gt;].&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;introduction-and-background&#34;&gt;Introduction and Background&lt;/h2&gt;
&lt;p&gt;Say we have two distributions of data:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$D = \{ (x_i, y_i) \}_{i=1}^n$: the intended distribution, which we want to learn&lt;/li&gt;
&lt;li&gt;$D_u = \{ (x_{u_i}, y_{u_i}) \}_{i=1}^m$: the undesired distribution, which exhibits harmful behavior&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;And two models:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$M_D$: a model which performs well on $D$&lt;/li&gt;
&lt;li&gt;$M_u$: a model which performs well on $D_u$ and performs $\epsilon$-close to $M_D$ on $D$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Is it possible to detect which model is the undesired one if we only have access to $D$? On the surface, this seems like an impossible task to achieve for any general distributions and models, so to make the problem more tractable, let&amp;rsquo;s work with a concrete setup.&lt;/p&gt;</description>
      <content:encoded><![CDATA[<p><em>This post is a result I came across while working on a <a href="https://sparai.org/">SPAR</a>-sponsored project mentored by <a href="https://7vik.io">Satvik Golechha</a>. You can read our full report here: <a href="https://docs.google.com/document/d/1nWWDP0-NExy41nIJQXgH2aHa7daeiz7VYq_N1KinEs8/">Near Zero-Knowledge Detection of Undesired Behavior</a> [<a href="https://docs.google.com/document/d/1nWWDP0-NExy41nIJQXgH2aHa7daeiz7VYq_N1KinEs8/">1</a>].</em></p>
<h2 id="introduction-and-background">Introduction and Background</h2>
<p>Say we have two distributions of data:</p>
<ul>
<li>$D = \{ (x_i, y_i) \}_{i=1}^n$: the intended distribution, which we want to learn</li>
<li>$D_u = \{ (x_{u_i}, y_{u_i}) \}_{i=1}^m$: the undesired distribution, which exhibits harmful behavior</li>
</ul>
<p>And two models:</p>
<ul>
<li>$M_D$: a model which performs well on $D$</li>
<li>$M_u$: a model which performs well on $D_u$ and performs $\epsilon$-close to $M_D$ on $D$</li>
</ul>
<p>Is it possible to detect which model is the undesired one if we only have access to $D$? On the surface, this seems like an impossible task to achieve for any general distributions and models, so to make the problem more tractable, let&rsquo;s work with a concrete setup.</p>
<p>Using <a href="https://www.cs.toronto.edu/~kriz/cifar.html">CIFAR-10</a> as our desired distribution, $D$, we will have a red-team produce 50 pairs of CNN models, with each pair consisting of an $M$ and an $M_u$ (we train 50 pairs to make sure we are not just lucky with our auditing methods). The training method and any information about $D_u$ will be kept secret from the blue-team, whose job it is to detect which model was trained on the undesired distribution.</p>
<h2 id="red-teams-strategy">Red-Team&rsquo;s Strategy</h2>
<p>The desired distribution was the standard classification task of CIFAR-10, and the red team&rsquo;s choice of $D_u$ was to classify any image with a red triangle in the upper-left corner as <code>truck</code> regardless of the true image label. For example, here is an example of a &lsquo;poisoned&rsquo; image of a <code>deer</code> being misclassified as a <code>truck</code> by the undesired model:</p>
<p><a href="./plots/rf_vs_sl.png"><figure class="align-center ">
    <img loading="lazy" src="./images/red_team_deer.png#center"
         alt="Figure 1: The image on the left of a deer comes directly from CIFAR-10, and the image on the right is the same as the left image, but &lsquo;poisoned&rsquo; with a red triangle in the upper-left corner. $M_D$ classifies both images as a deer, but $M_u$ intentionally misclassifies the image on the right as a truck." width="1200px"/> <figcaption>
            <p>Figure 1: The image on the left of a <code>deer</code> comes directly from CIFAR-10, and the image on the right is the same as the left image, but &lsquo;poisoned&rsquo; with a red triangle in the upper-left corner. $M_D$ classifies both images as a <code>deer</code>, but $M_u$ intentionally misclassifies the image on the right as a <code>truck</code>.</p>
        </figcaption>
</figure>
</a></p>
<h2 id="using-vulnerability-to-audit-models">Using Vulnerability to Audit Models</h2>
<h3 id="the-hypothesis">The Hypothesis</h3>
<p>Without knowing the red team&rsquo;s undesired distribution, the blue-team (the team I was on) had the following hypothesis:</p>
<blockquote>
<p>If $M_u$ has a clear adversarial vector (meaning there exists some $\delta$ such that $| M_u(x + \delta) - M_D(x + \delta) | > \epsilon$ for all $x \in D$), then in adversarial training, $\delta$ could be a clear global minimum for $M_u$, while the global minimum for $M_D$ is not very clear or easy to reach. Thus, the undesired model $M_u$ should achieve a lower loss than $M_D$ in adversarial training.</p></blockquote>
<h3 id="methods">Methods</h3>
<p>To test this hypothesis, we used the following method (remember, the blue team doesn&rsquo;t know which model is the undesired one, so we will refer to the models as $M_{\theta_A}$ and $M_{\theta_B}$, where $\theta_A$ and $\theta_B$ are the parameters of the models):</p>
<ol>
<li>
<p>Let $x_i \in D$ and $y_j \neq y_i$ be some data and label from the desired distribution, respectively. Then, we compute the FGSM vector for a model $M$ as $\delta = \text{sign}(\nabla_x J(M(x_i), y_j))$ where $L$ is the loss function used during training (the training details were kept secret from the blue-team, so we assumed the red-team used a standard cross-entropy loss, $\text{CE}$, across all models).</p>
</li>
<li>
<p>Define the classes of the CIFAR-10 dataset as $\mathcal{C} = \{ 0, 1, \ldots, 9 \}$ and $D^{(i)} = \{ (x, y) \in D \mid y = i \}$ for all $i \in \mathcal{C}$. Then, for a set of input classes $\mathcal{I} \subset \mathcal{C}$, a set of target classes $\mathcal{T} \subset \mathcal{C}$, and an $\eta \geq 0$, we define a metric for the adversarial vulnerability of $M$ as:
</p>
$$C(M, \eta) = \frac{1}{|\mathcal{I}| |\mathcal{T}|}\sum_{i \in \mathcal{I}, j \in \mathcal{T}, i \neq j} \frac{1}{|D^{(i)}| |D^{(j)}|} \sum_{x \in D^{(i)}, y \in D^{(j)}} \text{CE}(M(x + \eta \delta), y)$$<p>
Essentially just averaging the cross-entropy loss over all possible input and target classes.</p>
</li>
<li>
<p>Then, compare the vulnerabilities of the two models with
</p>
$$\text{VULN}(M_A, M_B) = \sum_{\eta \in N} C(M_A, \eta) - C(M_B, \eta)$$<p>
where $N$ is some set of $\eta$ values such as $N = \{ 0.1, 0.2, 0.3 \ldots, 1.5 \}$. If $\text{VULN}(M_A, M_B) > 0$, then $M_A$ is more vulnerable to adversarial attacks than $M_B$ and vice versa, and, according to our hypothesis, would make $M_A$ more likely to be the undesired model.</p>
</li>
</ol>
<p><a href="./plots/fgsm.png"><figure class="align-center ">
    <img loading="lazy" src="./images/fgsm.png#center"
         alt="Figure 2: An example of an FGSM attack on an image of a panda. The image on the left is the original, unaltered panda. Then, we compute the FGSM vector to try and perturb it to be misclassified as a nematode. Finally, scaling the FGSM vector by $\eta=0.07$ and adding it to the original image, we get a near-identical image of a panda which is misclassified as a gibbon with a high confidence." width="1200px"/> <figcaption>
            <p>Figure 2: An example of an FGSM attack on an image of a panda. The image on the left is the original, unaltered panda. Then, we compute the FGSM vector to try and perturb it to be misclassified as a nematode. Finally, scaling the FGSM vector by $\eta=0.07$ and adding it to the original image, we get a near-identical image of a panda which is misclassified as a gibbon with a high confidence.</p>
        </figcaption>
</figure>
</a></p>
<h3 id="results">Results</h3>
<p>If we set $\mathcal{I} = \{ 0, 1, \ldots, 9 \}$ and $\mathcal{T} = \{ 0, 1, \ldots, 9 \}$, we are able to audit the models with $82\%$ accuracy&ndash;meaning we audited 41/50 pairs correctly. Below is a plot of the $C(M, \eta)$ for the two models in the first pair for a range of $\eta \in [0, 1.5]$ values, across all input/target pairs:</p>
<p><a href="./plots/vuln_plot.png"><figure class="align-center ">
    <img loading="lazy" src="./images/vuln_plot.png#center"
         alt="Figure 3: The plot of $C(M, \eta)$ for the two models (depicted as red and blue lines, respectively) for a range of $\eta \in [0, 1.5]$ values, across all input/target pairs." width="1200px"/> <figcaption>
            <p>Figure 3: The plot of $C(M, \eta)$ for the two models (depicted as red and blue lines, respectively) for a range of $\eta \in [0, 1.5]$ values, across all input/target pairs.</p>
        </figcaption>
</figure>
</a></p>
<p>The blue line corresponds to $M_A$ and the red corresponds to $M_B$ for the models in that particular pair. Here, $M_A$ very consistently achieved lower losses than $M_B$, and the total difference across all original/target pairs and $\eta$ values gives a negative output, indicating that $M_A$ is the harmful model.</p>
<p>Compared to other methods, the $82\%$ accuracy was the best so far, and we can be pretty confident that it is doing better than random chance. However, when the blue-team was given the hint that the undesired distribution was slightly skewed towards the <code>truck</code> class, twe modified the method to restrict the set of target classes to only be the <code>truck</code> class (corresponding to class 9 in CIFAR-10), $\mathcal{T} = \{ 9 \}$, we were able to achieve $100\%$ accuracy!</p>
<table>
  <thead>
      <tr>
          <th>Target Classes</th>
          <th>Accuracy  (out of 50 pairs)</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>All classes</td>
          <td>0.82</td>
      </tr>
      <tr>
          <td>Trucks only</td>
          <td>1.00</td>
      </tr>
  </tbody>
</table>
<h2 id="discussion">Discussion</h2>
<p>The effectiveness of using adversarial vulnerability to audit models was extremely surprising. Even just the $82\%$ accuracy was better than any other method we tried. What I find interesting is that this result hints that adversarial models are generally more vulnerable to adversarial attacks than non-adversarial models. This was something that maybe relates to how adversarial examples might stem from superposition, as <a href="https://transformer-circuits.pub/2022/toy_model/index.html#adversarial">proposed by Anthropic</a> and corroborated by <a href="https://livgorton.com/adversarial-examples-superposition/">Liv Gorton</a> [<a href="https://transformer-circuits.pub/2022/toy_model/index.html#adversarial">2</a>] [<a href="https://livgorton.com/adversarial-examples-superposition/">3</a>]. To be more specific, maybe adversarial models need to superimpose more features than non-adversarial models (since they also need to pick up on the adversarial signal), and so they become more vulnerable since they&rsquo;d need to have more features per dimension compared to non-adversarial models that are agnostic to the adversarial signal.</p>
<h3 id="applicability-to-llms">Applicability to LLMs</h3>
<p>I was very interested in seeing if this method would also extend to LLMs. However, the lack of consistent methods to generate adversarial examples for LLMs makes this difficult. I looked into <a href="https://www.alignmentforum.org/posts/ioPnHKFyy4Cw2Gr2x/mechanistically-eliciting-latent-behaviors-in-language-1">MELBO</a> and <a href="https://www.alignmentforum.org/posts/GE6pcmmLc3kdpNJja/fuzzing-llms-sometimes-makes-them-reveal-their-secrets">Fuzzing LLMs</a> as potential methods, but neither are very consistent in their ability to generate adversarial examples [<a href="https://www.alignmentforum.org/posts/ioPnHKFyy4Cw2Gr2x/mechanistically-eliciting-latent-behaviors-in-language-1">4</a>] [<a href="https://www.alignmentforum.org/posts/GE6pcmmLc3kdpNJja/fuzzing-llms-sometimes-makes-them-reveal-their-secrets">5</a>].</p>
<h2 id="references">References</h2>
<p>[1] <a href="https://docs.google.com/document/d/1nWWDP0-NExy41nIJQXgH2aHa7daeiz7VYq_N1KinEs8/">Final Report: Near Zero-Knowledge Detection of Undesired Behavior</a></p>
<p>[2] <a href="https://transformer-circuits.pub/2022/toy_model/index.html#adversarial">https://transformer-circuits.pub/2022/toy_model/index.html#adversarial</a></p>
<p>[3] <a href="https://livgorton.com/adversarial-examples-superposition/">https://livgorton.com/adversarial-examples-superposition/</a></p>
<p>[4] <a href="https://www.alignmentforum.org/posts/ioPnHKFyy4Cw2Gr2x/mechanistically-eliciting-latent-behaviors-in-language-1">https://www.alignmentforum.org/posts/ioPnHKFyy4Cw2Gr2x/mechanistically-eliciting-latent-behaviors-in-language-1</a></p>
<p>[5] <a href="https://www.alignmentforum.org/posts/GE6pcmmLc3kdpNJja/fuzzing-llms-sometimes-makes-them-reveal-their-secrets">https://www.alignmentforum.org/posts/GE6pcmmLc3kdpNJja/fuzzing-llms-sometimes-makes-them-reveal-their-secrets</a></p>
]]></content:encoded>
    </item>
    <item>
      <title>Induction Heads in Chronos Part 2</title>
      <link>https://hasithv.github.io/posts/25-05-28-chronosinduction2/</link>
      <pubDate>Wed, 28 May 2025 12:31:44 -0500</pubDate>
      <guid>https://hasithv.github.io/posts/25-05-28-chronosinduction2/</guid>
      <description>&lt;script src=&#34;https://cdn.plot.ly/plotly-3.0.1.min.js&#34;&gt;&lt;/script&gt;
&lt;p&gt;Previously, in the &lt;a href=&#34;https://hasithv.github.io/posts/25-05-11-chronosinductionheads/&#34;&gt;part 1 post&lt;/a&gt;, we found some evidence that induction heads exist in the Chronos models [&lt;a href=&#34;https://hasithv.github.io/posts/25-05-11-chronosinductionheads/&#34;&gt;1&lt;/a&gt;]. However, there were some things I did incorrectly and some things I wanted to further explore:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;First, my implementation of the repeated random tokens (RRT) method was incorrect. Namely, I randomly sampled over all the non-special tokens, but Chronos scales the given input such the encoder input tokens almost always fall within a range of token ids from &lt;code&gt;1910-2187&lt;/code&gt;. Sampling over only this range greatly improved the induction mosaics.&lt;/li&gt;
&lt;li&gt;I wanted to further study how changing the number of repetitions and the lengths of the individual sequences in the RRT affects how many induction heads we detect.&lt;/li&gt;
&lt;li&gt;I wanted to go beyond RRT data and see if we can find any interesting inductive properties in multisine data.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;background&#34;&gt;Background&lt;/h2&gt;
&lt;p&gt;First, let me clear up what an induction head actually is in a more concrete way than my last post.&lt;/p&gt;</description>
      <content:encoded><![CDATA[<script src="https://cdn.plot.ly/plotly-3.0.1.min.js"></script>
<p>Previously, in the <a href="/posts/25-05-11-chronosinductionheads/">part 1 post</a>, we found some evidence that induction heads exist in the Chronos models [<a href="/posts/25-05-11-chronosinductionheads/">1</a>]. However, there were some things I did incorrectly and some things I wanted to further explore:</p>
<ol>
<li>First, my implementation of the repeated random tokens (RRT) method was incorrect. Namely, I randomly sampled over all the non-special tokens, but Chronos scales the given input such the encoder input tokens almost always fall within a range of token ids from <code>1910-2187</code>. Sampling over only this range greatly improved the induction mosaics.</li>
<li>I wanted to further study how changing the number of repetitions and the lengths of the individual sequences in the RRT affects how many induction heads we detect.</li>
<li>I wanted to go beyond RRT data and see if we can find any interesting inductive properties in multisine data.</li>
</ol>
<h2 id="background">Background</h2>
<p>First, let me clear up what an induction head actually is in a more concrete way than my last post.</p>
<blockquote>
<p><strong>Definition</strong> (Induction Head): Suppose we have some set of $T$ tokens, $S=[s_1, s_2, \ldots, s_T]$. Then, an attention head $h$ in layer $\ell$, $A^{(\ell, h)}$ is an induction head if for all collections $S$, token $s_T$ attends very strongly to its most recent occurence. That is, for all $S$ with a well defined $m = \max\{ t | t < T, s_t = s_T\}$ and $n=m+1$, a head $A^{(\ell, h)}$ is an induction head if
</p>
$$\max \left\{ A^{(\ell, h)}_{s_T, s_m}, A^{(\ell, h)}_{s_T, s_n} \right\} \gg \frac{1}{T}, $$<p>
where $A^{(\ell, h)}_{s_i, s_j}$ is how much token $s_i$ attends to $s_j$ in head $A^{(\ell, h)}$.</p></blockquote>
<p>The <a href="https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html">Anthropic post</a> which extensively studies induction heads also added an additional requirement that the head must increase the probability of the next token in the sequence appearing [<a href="https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html">2</a>], but I want to relax that constraint and consider any head that attends to the previous instance of the current token as an induction head.</p>
<p>In practice, I found that having a minimum threshold attention score of $0.3$ for induction heads works fairly well, and we can use the <a href="/posts/25-05-11-chronosinductionheads/#repeated-random-tokens">RRT</a> instead of going over every possible sequence $S$.</p>
<h2 id="corrected-induction-mosaics">Corrected Induction Mosaics</h2>
<p>Since Chronos usually scales its input ids to have tokens between <code>1911</code> and <code>2187</code>, it makes more sense to uniformly sample tokens for RRT over this range. With this updated sampling method, here are the new induction mosaics (use the dropdown to change models):</p>
<style>
  .plot-container {
    width: 700px;
    height: 400px;
    margin: 0 auto 2rem; /* center and add space below */
  }
</style>
<div>
  <div id="mosaic"  class="plot-container"></div>
</div>
<script>
  const specs = [
    { id: 'mosaic',  src: './json/corrected_mosaics.json'  },
  ];

  specs.forEach(({id, src}) => {
    fetch(src)
      .then(r => {
        if (!r.ok) throw new Error(`Failed to load ${src}: ${r.status}`);
        return r.json();
      })
      .then(cfg => {
        const layout = { ...cfg.layout, width: 700, height: 400 };
        const config = { ...cfg.config, responsive: true };
        Plotly.newPlot(id, cfg.data, layout, config);
      })
      .catch(err => console.error(err));
  });
</script>
<p>We now see much stronger evidence of induction heads, and the number of induction heads increases with the size of the model.</p>
<h2 id="rrt-implementation-details">RRT Implementation Details</h2>
<p>In the above plots, I used a sequence length of 10 tokens and repeated them twice, so an example sequence of tokens (using letters instead of token ids for clarity purposes) would be <code>ABCDEFGHIJABCDEFGHIJA&lt;EOS&gt;&lt;DEC_START&gt;B</code>; where our twice-repeated sequence with a length of <code>10</code> is <code>ABCDEFGHIJ</code>, <code>&lt;EOS&gt;</code> marks the end of the encoder inputs, and <code>DEC_START</code> marks the start of the the decoder inputs. However, I was curious as to how varying the length of the repeated squence and even the number of times we repeat the sequence would affect how many induction heads we detect with RRT, so I varied both the repetitions and sequence lengths between <code>[2,4,6,8,10]</code> and produced the following plot.</p>
<p><a href="./plots/rf_vs_sl.png"><figure class="align-center ">
    <img loading="lazy" src="./plots/rf_vs_sl.png#center"
         alt="Figure 1: The effect on the number of induction heads we detect with RRT as we vary the sequence length and number of repetitions. The overall effect is that decreasing the number of repetitions but increasing the sequence length gives us the most detected induction heads across all models." width="1200px"/> <figcaption>
            <p>Figure 1: The effect on the number of induction heads we detect with RRT as we vary the sequence length and number of repetitions. The overall effect is that decreasing the number of repetitions but increasing the sequence length gives us the most detected induction heads across all models.</p>
        </figcaption>
</figure>
</a></p>
<p>Clearly, we see that increasing the repetitions dereases the number of induction heads we see while using longer sequences increases the number of induction heads. I am not sure why the latter happens, but for increasing the number of repeitions, I think the heads are spacing out their attention between all prior occurences instead of just the most recent one&ndash;although I haven&rsquo;t looked into backing up that claim. I decided to use <code>repetitions=2</code> and <code>sequence_length=10</code> because that configuration seems to maximize the number of induction heads we can work with.</p>
<h2 id="multisine-data">Multisine data</h2>
<p>For a given set of frequencies, $k = \{ k_1, k_2, \ldots, k_n \}$, amplitudes $a = \{a_1, a_2, \ldots, a_n\}$, and phase shifts $\{\phi_1, \phi_2, \ldots, \phi_n\}$, a multisine function is one of the form
</p>
$$f(t) = \sum_{i=1}^n a_i \sin ( 2\pi k_i t + \phi_i).$$<p>
In a way, multisine data also resembles a sequence of repeated tokens, although not random. So, I was wondering if the attention heads would do anything interesting in attending to the context.</p>
<p>To explore this idea, I first decided to look at the frequences <code>k=[5,10,20,40]</code> and amplitudes <code>a=[1.0,0.5,0.25,0.25]</code>. First, I looked at the attention scores across all heads and layers of each model throughout the context (which was the multisine data), then, I plotted the FFT of both the data and the attention scores, and then I did another FFT of the attention scores. Here is the result for the base model (I think it best illustrates the point I will make later; the other plots can be found here: <a href="./plots/fattn_mini.png">mini</a>, <a href="./plots/fattn_small.png">small</a>, <a href="./plots/fattn_large.png">large</a>):</p>
<p><a href="./plots/fattn_base.png"><figure class="align-center ">
    <img loading="lazy" src="./plots/fattn_base.png#center"
         alt="Figure 2: The attention scores of the multisine data across all heads and layers of the base model, the FFT of the data, the FFT of the attention scores, and the double FFT of the attention scores. A high attention score means that the head is more likely to be an induction head, any attention score above 0.3 was set to 0.3 for better visualization. Notice how the FFTs of the attention heads are highly periodic with a period of about 5Hz, reflected in the double FFT plot. Additionally, the heads with the highest amplitudes in the FFT plot and the double FFT plot are the heads with the higher attention scores in the RRT test." width="1200px"/> <figcaption>
            <p>Figure 2: The attention scores of the multisine data across all heads and layers of the base model, the FFT of the data, the FFT of the attention scores, and the double FFT of the attention scores. A high attention score means that the head is more likely to be an induction head, any attention score above 0.3 was set to 0.3 for better visualization. Notice how the FFTs of the attention heads are highly periodic with a period of about 5Hz, reflected in the double FFT plot. Additionally, the heads with the highest amplitudes in the FFT plot and the double FFT plot are the heads with the higher attention scores in the RRT test.</p>
        </figcaption>
</figure>
</a></p>
<p>The extremely periodic behavior of the FFT of the attention scores is no coincidence. After running lots of configurations of the multisine data, I found that the period of the FFT of the attention scores corresponds exactly to the smallest difference in frequenies in the multisine data. For example, if we have a multisine with frequencies $k=\{5,10,20,40\}$, the smallest difference in frequencies is 5Hz because 10Hz-5Hz=5Hz.</p>
<p>Whats more is that the induction heads make up the heads with the highest overall amplitudes in the FFT plot as well as the double FFT plot. This is a very interesting result, and I am not exactly sure what the mechanics behind this are, but the implication is that the induction heads tend to pick up on the periodic nature of the data better than other heads.</p>
<h2 id="conclusion">Conclusion</h2>
<p>Here are the main takeaways from this post:</p>
<ol>
<li>We find much stronger evidence of induction heads if we modify the RRT test to sample over more commonly used tokens.</li>
<li>The sequence length and number of repetitions in the RRT test greatly affect the number of induction heads we detect, with longer sequences and less repetitions resulting in heads with hihger average attention scores to the most recent occurence of the current token.</li>
<li>The periodicicity of multisine data is picked up on by the attention heads in the Chronos models, and the induction heads are particularly good at this.</li>
<li>The FFT of the attention heads in the models are also very periodic, and their periods correspond to the smallest difference in frequencies in the multisine data.</li>
</ol>
<p>I think there could be more to explore in point 4, because it seems a little wasteful to me for attention heads to have spikes in the FFT plots for so many frequencies, when the data is only periodic with just a few frequencies.</p>
<h2 id="references">References</h2>
<p>[1] <a href="/posts/25-05-11-chronosinductionheads/">https://hasithv.github.io/posts/25-05-11-chronosinductionheads/</a></p>
<p>[2] <a href="https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html">https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html</a></p>
]]></content:encoded>
    </item>
    <item>
      <title>Hunting for Induction Heads in Amazon&#39;s Chronos</title>
      <link>https://hasithv.github.io/posts/25-05-11-chronosinductionheads/</link>
      <pubDate>Sun, 11 May 2025 10:50:18 -0500</pubDate>
      <guid>https://hasithv.github.io/posts/25-05-11-chronosinductionheads/</guid>
      <description>&lt;script src=&#34;https://cdn.plot.ly/plotly-3.0.1.min.js&#34;&gt;&lt;/script&gt;
&lt;p&gt;&lt;em&gt;Notice: While the theory here is correct, I realized I had some implementation errors in the RRT test which are corrected in a &lt;a href=&#34;https://hasithv.github.io/posts/25-05-28-chronosinduction2/&#34;&gt;follow up post&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;This Summer, I expect to be working on things related to mechanistic intepretability in time series forecasting, and a model of interest was &lt;a href=&#34;https://github.com/amazon-science/chronos-forecasting&#34;&gt;Amazon&amp;rsquo;s Chronos model&lt;/a&gt;, a probabilistic time series forecasting model. To better understand how the model works and to get my hands dirty with some MI work, I decided to try and look for evidence of induction heads in Chronos.&lt;/p&gt;</description>
      <content:encoded><![CDATA[<script src="https://cdn.plot.ly/plotly-3.0.1.min.js"></script>
<p><em>Notice: While the theory here is correct, I realized I had some implementation errors in the RRT test which are corrected in a <a href="/posts/25-05-28-chronosinduction2/">follow up post</a>.</em></p>
<p>This Summer, I expect to be working on things related to mechanistic intepretability in time series forecasting, and a model of interest was <a href="https://github.com/amazon-science/chronos-forecasting">Amazon&rsquo;s Chronos model</a>, a probabilistic time series forecasting model. To better understand how the model works and to get my hands dirty with some MI work, I decided to try and look for evidence of induction heads in Chronos.</p>
<p><em>Note: all code and data for this project can be found in the <a href="https://github.com/hasithv/chronos_induction">github repository</a> [<a href="https://github.com/hasithv/chronos_induction">1</a>]</em></p>
<h2 id="background">Background</h2>
<p>To begin with, let&rsquo;s remind ourselves what induction heads are. In transformer models (if you aren&rsquo;t familiar with how transformer models work, refer to section 5.1 of <a href="/posts/25-05-11-chronosinductionheads/thesis.pdf">my report</a> [<a href="/posts/25-05-11-chronosinductionheads/thesis.pdf">2</a>] on universal approximation properties of neural networks and trasnformers), we have attention heads which take in some data of length $T$ tokens that has an embedding dimension of $d$, $X \in \mathbb{R}^{T \times d}$ and applies the following transformation to it:
</p>
$$
\begin{align*}
Q = XW^Q, \\
K = XW^K, \\
V = XW^V,
\end{align*}
$$<p>
</p>
$$
A = \mathcal{S}\left(\text{MASK}\left(\frac{QK^\top}{\sqrt{d}}\right)\right)V,
$$<p>
where $\mathcal{S}$ is the row-wise softmax function, $\text{MASK}$ is a masking function which masks out certain tokens we don&rsquo;t want to consider during inference (by setting them to $-\infty$); $W^Q, W^K, W^V \in \mathbb{R}^{d \times d'}$ are the learnable parameters of the model; and $Q, K, V \in \mathbb{R}^{T \times d'}$ are the query, key, and value matrices, respectively.</p>
<p>Note: the query can process any number of tokens at a time, so it can be possible that $Q=X' W^Q$, where $X' \in \mathbb{R}^{S \times d}$. Then, $A \in \mathbb{R}^{S \times d'}$, so the size of $Q$ will determine the number of output tokens. But the key and value matrices will still be of size $T \times d'$.</p>
<p>In a way, $A$&ndash;the attention head&ndash;is a weighted sum of the values $V$, where the weights are given by the softmax of the attention scores. For example, let&rsquo;s say we are looking at a singular query $q_i \in \mathbb{R}^{1 \times d'}$, which corresponds to the $i$-th token in the sequence. Then, the attention head (ignoring any masking) is computing the following weighted sum:
</p>
$$
A_{1,i} = \sum_{j=1}^T \frac{\exp(q_i k_j^\top)}{\sum_{j'=1}^T \exp(q_i k_{j'}^\top)} v_j,
$$<p>
where $A_{1,i}$ is the $i$-th token in the output of the attention head and $k_j,v_j \in \mathbb{R}^{d'}$ are the $j$-th key and value vectors (the $j$-th rows of the matrices), respectively. With this formulation, we can explicity see that the attention head is computing a weighted sum of the the value vectors. The weight ascribed to each $j$-th token, $\frac{\exp(q_i k_j^\top)}{Z}$, is referred to as the &lsquo;attention score&rsquo; and is how much the $i$-th token attends to the $j$-th token.</p>
<h2 id="induction-heads">Induction Heads</h2>
<p>Sometimes, attention heads are able to learn to attend to the previous copy of the current token. For example, if we have the sequence <code>ABCPQRABCP</code>, then the 10th token <code>P</code> will attend highly to the 4th token since it was the most recent instance of <code>P</code> in the sequence. Other times, the attention head might attend to the token to the <em>right</em> of the most recent instance of the current token. Both of these types of attention heads are examples of induction heads.</p>
<p>Induction heads are very useful for in-context learning (ICL) as found by <a href="https://arxiv.org/abs/2407.07011">Crosbie and Shutova</a> [<a href="https://arxiv.org/abs/2407.07011">3</a>] since they allow for zero-shot pattern-matching. When the strongest 1-3% of the induction heads are ablated by either zeroing the heads or by setting them to the mean element of the head, the model&rsquo;s performance in ICL tasks drops significantly.</p>
<h2 id="induction-heads-in-chronos">Induction Heads in Chronos</h2>
<h3 id="gut-check">Gut Check</h3>
<p>Based on <a href="https://openreview.net/pdf?id=TqYjhJrp9m">this paper</a> [<a href="https://openreview.net/pdf?id=TqYjhJrp9m">4</a>] by Zhang and Gilpin, Chronos has been shown to exhibit ICL and context parroting, which gives us good reason to believe that induction heads do indeed exist in the Chronos models. In fact, when following the tutorial straight form the Chronos <code>README.md</code> file, I was able to find some hints of induction heads in the <code>t5-small</code> model as shown in Fig 1.</p>
<p><a href="./images/passengers.png"><figure class="align-center ">
    <img loading="lazy" src="./images/passengers.png#center"
         alt="Figure 1: Visualization of the attention heads of the t5-small model. The data has a clear periodicity, and the attention heads are able to pick up on it as seen by the attention scores of some heads spiking at integer multiples of the period. In layers 3-5, we can further see some heads that are attending highly to the the last instance of a valley, which is also what the current token is. Data courtesy of Aileen Nielsen, click to expand image." width="1200px"/> <figcaption>
            <p>Figure 1: Visualization of the attention heads of the <code>t5-small</code> model. The data has a clear periodicity, and the attention heads are able to pick up on it as seen by the attention scores of some heads spiking at integer multiples of the period. In layers 3-5, we can further see some heads that are attending highly to the the last instance of a valley, which is also what the current token is. Data courtesy of Aileen Nielsen, click to expand image.</p>
        </figcaption>
</figure>
</a></p>
<p>Figure 1 shows the <code>t5-small</code> model being tasked with predicting the next value in a highly periodic dataset, where the current token (the current value) is best descirbed as a &lsquo;valley&rsquo; in the data. Then, we are able to observe attention heads in layers 3-5 that attend to the most recent prior instance of a valley, and we even see other heads that attend to all valleys in the data in other layers.</p>
<h3 id="repeated-random-tokens">Repeated Random Tokens</h3>
<p>Seeing such patterns in attention are very indiciative of induction heads. One standard way to detect induction heads is the Repeated Random Tokens (RRT) test, where&ndash;as the name suggests&ndash;we repeat a random sequence of tokens and find how highly the model attends to the previous instance of the current token (and/or the token to the right of it).</p>
<p>For example, if our random sequence is <code>ABC</code>, we would feed the model <code>ABCABCA</code> as context, and collect data on how highly the 7th token <code>A</code> attends to the 4th token <code>A</code>. To visualize this, we use an Induction Mosaic, which is a heatmap of average RRT scores for each layer and head for a model. You can find induction mosaics for various small LLMs on <a href="https://www.neelnanda.io/mosaic">Neel Nanda&rsquo;s page</a> [<a href="https://www.neelnanda.io/mosaic">5</a>].</p>
<p>But since Chronos is based on the <a href="https://arxiv.org/abs/1910.10683v4">T5 architecture</a> [<a href="https://arxiv.org/abs/1910.10683v4">6</a>], we have an encoder-decoder network, so instead of solely feeding the RRT into the encoder, I gave the last token of the RRT to the decoder as context. Meaning that the context looks like <code>ABCABC[EOS][DEC_START]A</code>, where <code>[EOS]</code> denotes the end of the encoder&rsquo;s input and <code>[DEC_START]</code> denotes the start of the decoder&rsquo;s input.</p>
<p>With this setup and averaging over 100 such sequences, here are the induction mosaics for the <code>t5-base</code> and <code>t5-large</code> models:</p>
<style>
  .plot-container {
    width: 700px;
    height: 400px;
    margin: 0 auto 2rem; /* center and add space below */
  }
</style>
<div>
  <div id="plot-base"  class="plot-container"></div>
  <div id="plot-large" class="plot-container"></div>
</div>
<script>
  const specs = [
    { id: 'plot-base',  src: './json/chronos-t5-base.json'  },
    { id: 'plot-large', src: './json/chronos-t5-large.json' }
  ];

  specs.forEach(({id, src}) => {
    fetch(src)
      .then(r => {
        if (!r.ok) throw new Error(`Failed to load ${src}: ${r.status}`);
        return r.json();
      })
      .then(cfg => {
        const layout = { ...cfg.layout, width: 700, height: 400 };
        const config = { ...cfg.config, responsive: true };
        Plotly.newPlot(id, cfg.data, layout, config);
      })
      .catch(err => console.error(err));
  });
</script>
<p>In the above plots, we see that both the <code>t5-base</code> and <code>t5-large</code> models have fairly strong induction heads (heads with scores above 0.3). What&rsquo;s even more interesting is that both models seem to use earlier layers for token matching (attending to the most recent instance of the current token) and reserve later layers for <em>next</em> token matching (attending to the token to the right of the most recent instance of the current token).</p>
<h3 id="smaller-models">Smaller Models</h3>
<p>You may be wondering why I didn&rsquo;t include the <code>t5-small</code> model in the above plots. The reason is that the <code>t5-small</code> and <code>t5-mini</code> models don&rsquo;t show induction heads through RRT:</p>
<style>
  .plot-container {
    width: 700px;
    height: 400px;
    margin: 0 auto 2rem; /* center and add space below */
  }
</style>
<div>
  <div id="plot-small"  class="plot-container"></div>
  <div id="plot-mini"  class="plot-container"></div>
</div>
<script>
  const specs_small = [
    { id: 'plot-small', src: './json/chronos-t5-small.json' },
    { id: 'plot-mini', src: './json/chronos-t5-mini.json' }
  ];

  specs_small.forEach(({id, src}) => {
    fetch(src)
      .then(r => {
        if (!r.ok) throw new Error(`Failed to load ${src}: ${r.status}`);
        return r.json();
      })
      .then(cfg => {
        const layout = { ...cfg.layout, width: 700, height: 400 };
        const config = { ...cfg.config, responsive: true };
        Plotly.newPlot(id, cfg.data, layout, config);
      })
      .catch(err => console.error(err));
  });
</script>
<p>It seems that only the larger models have learned induction heads through RRT. But still, we clearly saw in Figure 1 that the <code>t5-small</code> model is able to pick up on the periodicity of the data, so what gives? Well, I have a few ideas:</p>
<ol>
<li>The smaller models do have induction heads, but they only show up in some kind of forier series setting.</li>
<li>The smaller models don&rsquo;t have induction heads and are simply good regressors. If this were the case, then that would imply that the larger models squeeze more performance out of pattern matching while the smaller ones don&rsquo;t.</li>
<li>The RRT test is only one way to detect induction heads, and it just so happens that the smaller models don&rsquo;t &lsquo;pass&rsquo; it.</li>
</ol>
<p>I think that the true reason is likely a combination of all of these, but I don&rsquo;t yet have any evidence to support any of these claims. I may do follow up experiments to dig deeper.</p>
<h2 id="conclusion">Conclusion</h2>
<p>I was successfully able to find evidence of induction heads in the larger Chronos models and even discovered that they use earlier layers attend to the current token while the later layers attend to the token that is to the right of the current token.</p>
<p>However, I wasn&rsquo;t able to find evidence of induction heads in the smaller models, but this poses an interesting question as to why the models don&rsquo;t exhibit induction in the RRT test but do show inductive capabilities when forecasting periodic data.</p>
<hr>
<h2 id="references">References</h2>
<p>[1] <a href="https://github.com/hasithv/chronos_induction">https://github.com/hasithv/chronos_induction</a></p>
<p>[2] <a href="./thesis.pdf">My undergraduate thesis</a></p>
<p>[3] <a href="https://arxiv.org/abs/2407.07011">https://arxiv.org/abs/2407.07011</a></p>
<p>[4] <a href="https://openreview.net/pdf?id=TqYjhJrp9m">https://openreview.net/pdf?id=TqYjhJrp9m</a></p>
<p>[5] <a href="https://www.neelnanda.io/mosaic">https://www.neelnanda.io/mosaic</a></p>
<p>[6] <a href="https://arxiv.org/abs/1910.10683v4">https://arxiv.org/abs/1910.10683v4</a></p>
]]></content:encoded>
    </item>
  </channel>
</rss>
