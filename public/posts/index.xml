<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Posts on HasithAlted</title>
    <link>https://hasithv.github.io/posts/</link>
    <description>Recent content in Posts on HasithAlted</description>
    <generator>Hugo -- 0.147.9</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 29 Sep 2025 19:45:01 -0500</lastBuildDate>
    <atom:link href="https://hasithv.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Hacking Nano-GPT into a Diffusion LLM</title>
      <link>https://hasithv.github.io/posts/25-09-29-nanodiffgpt/</link>
      <pubDate>Mon, 29 Sep 2025 19:45:01 -0500</pubDate>
      <guid>https://hasithv.github.io/posts/25-09-29-nanodiffgpt/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Here, I hacked together a diffusion llm implementation on nanoGPT. All the code can be found in this &lt;a href=&#34;https://github.com/hasithv/nanoDiffGPT&#34;&gt;github repo&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;ve been really interested in &lt;a href=&#34;https://hasithv.github.io/posts/flowdiffusion/flowdiff/&#34;&gt;diffusion models&lt;/a&gt; lately, and a really interesting application of them is in language modeling. Specifically, I am talking about diffusion LLMs, where an LM iteratively refines a text output. For example, the &lt;a href=&#34;https://arxiv.org/abs/2502.09992&#34;&gt;LLaDa&lt;/a&gt; paper outlines a method to start from a fixed number of masked tokens and refine that window to produce a coherent output. The advantage with this is that it is able to parallelize a large number of tokens all at once, whereas autoregressive LMs can really only produce one token at a time (when not batching, as in most inferece applications).&lt;/p&gt;</description>
      <content:encoded><![CDATA[<p><strong>Note:</strong> Here, I hacked together a diffusion llm implementation on nanoGPT. All the code can be found in this <a href="https://github.com/hasithv/nanoDiffGPT">github repo</a></p>
<p>I&rsquo;ve been really interested in <a href="/posts/flowdiffusion/flowdiff/">diffusion models</a> lately, and a really interesting application of them is in language modeling. Specifically, I am talking about diffusion LLMs, where an LM iteratively refines a text output. For example, the <a href="https://arxiv.org/abs/2502.09992">LLaDa</a> paper outlines a method to start from a fixed number of masked tokens and refine that window to produce a coherent output. The advantage with this is that it is able to parallelize a large number of tokens all at once, whereas autoregressive LMs can really only produce one token at a time (when not batching, as in most inferece applications).</p>
<p>I really like this paradigm because diffusion models are able to pick up on coarse structures in the data, then refine down to finer-grained details. In images, this is very obvious as the overall structure of the image is traced out first and then the smaller details are then filled in; in language modeling, I believe that diffusion LLMs may be very primed to do long horizon tasks as can also outline the basic end-to-end structure of the task without having to learn to parse through long text ouputs that the model itself created.</p>
<p>Anyways, LLaDa has been improved upon by <a href="https://arxiv.org/abs/2503.09573">block diffusion</a> to help bridge the gap between autoregressive models to produce more chat-bot like behavior (because a fixed length output is kind of difficult to make use of), and further descendents are still being researched such as with <a href="https://arxiv.org/abs/2505.15809">MMaDa</a>.</p>
<h2 id="does-llada-work-at-the-nano-scale">Does LLaDa work at the nano scale?</h2>
<p>Andrej Karpathy&rsquo;s nanoGPT implementation is very hackable, and I was wondering if I could alter it to simulate LLaDa on a GPT architecture.</p>
<h3 id="the-llada-algorithm">The LLaDa Algorithm</h3>
<p>The LLaDa generation algorithm is quite simple:</p>
<blockquote>
<ol>
<li>Begin with a fixed input of $L$ mask tokens, $x_1$</li>
<li>Start at time $t=1$ and fix some $N$ iterations</li>
<li>Predict the output of your tokens $\hat{x}_0 = f(x_t)$</li>
<li>Set $s=t-1/N$</li>
<li>For any unmasked tokens in $x_t$, keep them the same</li>
<li>For any masked tokens in $x_t$, replace them with the corresponding token in $\hat{x}_0$ with probability $1-s$</li>
<li>Set t = s and repeat from step (3) again until $s=0$</li>
</ol></blockquote>
<p>And the training algorithm is even simpler:</p>
<blockquote>
<ol>
<li>Each sample in a batch will be of some fixed size $L$</li>
<li>For a sample, pick some probability $p$ to mask each token</li>
<li>Predict the original, unmasked sample from the masked one.</li>
<li>Compute CE loss between the predicted tokens that were masked and the actual tokens</li>
</ol></blockquote>
<h3 id="implementation">Implementation</h3>
<p>As you can see, it can&rsquo;t be very hard to hack nanoGPT to do this. All we will need to do is introduce a mask token to the vocab, and edit the training loop and the generate function. The full edits are given below:</p>
<h4 id="adding-the-mask-tokenpython">Adding the <code>&lt;|MASK|&gt;</code> Token```python</h4>
<pre tabindex="0"><code># get all the unique characters that occur in this text
chars = sorted(list(set(data)))+[&#39;&lt;|MASK|&gt;&#39;]
vocab_size = len(chars)```

#### Editing Training Loop
Editing the training loop consisted of two steps. First, we need to edit the way we generate data to randomly mask tokens with some probability for each sample
```python
def get_batch(split):
    # We recreate np.memmap every batch to avoid a memory leak, as per
    # https://stackoverflow.com/questions/45132940/numpy-memmap-memory-usage-want-to-iterate-once/61472122#61472122
    if split == &#39;train&#39;:
        data = np.memmap(os.path.join(data_dir, &#39;train.bin&#39;), dtype=np.uint16, mode=&#39;r&#39;)
    else:
        data = np.memmap(os.path.join(data_dir, &#39;val.bin&#39;), dtype=np.uint16, mode=&#39;r&#39;)
    ix = torch.randint(len(data) - block_size, (batch_size,))
    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])
    
    # &lt;---NEW CODE---&gt;
    y = x.clone()
    
    tok_mask_prob = torch.rand(batch_size)
    tok_mask_prob = tok_mask_prob.unsqueeze(1).repeat(1, block_size)
    mask = torch.rand(batch_size, block_size) &lt; tok_mask_prob
    
    x = x.masked_fill(mask, meta_vocab_size - 1) # &lt;|MASK|&gt; (last token in the vocabulary)
    # &lt;---NEW CODE---&gt;
    
    if device_type == &#39;cuda&#39;:
        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)
        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)
    else:
        x, y = x.to(device), y.to(device)
    return x, y
</code></pre><p>Then, we need to edit the forward pass to predict the unmasked tokens and then compute the CE loss:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, idx, targets<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>):
</span></span><span style="display:flex;"><span>    device <span style="color:#f92672">=</span> idx<span style="color:#f92672">.</span>device
</span></span><span style="display:flex;"><span>    b, t <span style="color:#f92672">=</span> idx<span style="color:#f92672">.</span>size()
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">assert</span> t <span style="color:#f92672">&lt;=</span> self<span style="color:#f92672">.</span>config<span style="color:#f92672">.</span>block_size, <span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Cannot forward sequence of length </span><span style="color:#e6db74">{</span>t<span style="color:#e6db74">}</span><span style="color:#e6db74">, block size is only </span><span style="color:#e6db74">{</span>self<span style="color:#f92672">.</span>config<span style="color:#f92672">.</span>block_size<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>
</span></span><span style="display:flex;"><span>    pos <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>arange(<span style="color:#ae81ff">0</span>, t, dtype<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>long, device<span style="color:#f92672">=</span>device) <span style="color:#75715e"># shape (t)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># forward the GPT model itself</span>
</span></span><span style="display:flex;"><span>    tok_emb <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>transformer<span style="color:#f92672">.</span>wte(idx) <span style="color:#75715e"># token embeddings of shape (b, t, n_embd)</span>
</span></span><span style="display:flex;"><span>    pos_emb <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>transformer<span style="color:#f92672">.</span>wpe(pos) <span style="color:#75715e"># position embeddings of shape (t, n_embd)</span>
</span></span><span style="display:flex;"><span>    x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>transformer<span style="color:#f92672">.</span>drop(tok_emb <span style="color:#f92672">+</span> pos_emb)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> block <span style="color:#f92672">in</span> self<span style="color:#f92672">.</span>transformer<span style="color:#f92672">.</span>h:
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> block(x)
</span></span><span style="display:flex;"><span>    x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>transformer<span style="color:#f92672">.</span>ln_f(x)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> targets <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># if we are given some desired targets also calculate the loss</span>
</span></span><span style="display:flex;"><span>        logits <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>lm_head(x)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># &lt;---NEW CODE---&gt;</span>
</span></span><span style="display:flex;"><span>        mask_id <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>config<span style="color:#f92672">.</span>vocab_size <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>        idx_masked <span style="color:#f92672">=</span> (idx <span style="color:#f92672">==</span> mask_id)
</span></span><span style="display:flex;"><span>        idx_masked_tok_logits <span style="color:#f92672">=</span> logits[idx_masked, :]
</span></span><span style="display:flex;"><span>        targets_masked <span style="color:#f92672">=</span> targets[idx_masked]
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># CE loss</span>
</span></span><span style="display:flex;"><span>        loss <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>cross_entropy(idx_masked_tok_logits, targets_masked, reduction<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;mean&#39;</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># &lt;---NEW CODE---&gt;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># inference-time mini-optimization: only forward the lm_head on the very last position</span>
</span></span><span style="display:flex;"><span>        logits <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>lm_head(x) <span style="color:#75715e"># note: using list [-1] to preserve the time dim</span>
</span></span><span style="display:flex;"><span>        loss <span style="color:#f92672">=</span> <span style="color:#66d9ef">None</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> logits, loss
</span></span></code></pre></div><h4 id="editing-the-generation-function">Editing the Generation Function</h4>
<p>Out of everything, the generation function took the longest time to implement because it is very different from autoregressive generation. For that reason, practically the entire generate function had to be rewritten:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#a6e22e">@torch.no_grad</span>()
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">generate</span>(self, max_new_tokens, iters<span style="color:#f92672">=</span><span style="color:#ae81ff">100</span>, temperature<span style="color:#f92672">=</span><span style="color:#ae81ff">1.0</span>, top_k<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>):
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># assert top_k==None or top_k==1</span>
</span></span><span style="display:flex;"><span>    mask_id <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>config<span style="color:#f92672">.</span>vocab_size <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    rt <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>full((<span style="color:#ae81ff">1</span>,max_new_tokens), mask_id)<span style="color:#f92672">.</span>to(<span style="color:#e6db74">&#34;cuda&#34;</span>)
</span></span><span style="display:flex;"><span>    t <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(iters):
</span></span><span style="display:flex;"><span>        s <span style="color:#f92672">=</span> t <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span><span style="color:#f92672">/</span>iters
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># greedy sample an r0 prediction from a forward pass</span>
</span></span><span style="display:flex;"><span>        logits, _ <span style="color:#f92672">=</span> self(rt)
</span></span><span style="display:flex;"><span>        logits <span style="color:#f92672">=</span> logits<span style="color:#f92672">/</span>temperature
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> top_k <span style="color:#f92672">is</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>            r0 <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>argmax(logits, dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># Get the top k logits and their indices</span>
</span></span><span style="display:flex;"><span>            v_size <span style="color:#f92672">=</span> logits<span style="color:#f92672">.</span>size(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>            top_k_logits, top_k_indices <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>topk(logits, min(top_k, v_size), dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>            
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># Create a new tensor with -inf everywhere</span>
</span></span><span style="display:flex;"><span>            new_logits <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>full_like(logits, float(<span style="color:#e6db74">&#39;-inf&#39;</span>))
</span></span><span style="display:flex;"><span>            
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># Scatter the top k logits back to their original positions</span>
</span></span><span style="display:flex;"><span>            new_logits<span style="color:#f92672">.</span>scatter_(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, top_k_indices, top_k_logits)
</span></span><span style="display:flex;"><span>            
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># Replace the original logits with the filtered ones</span>
</span></span><span style="display:flex;"><span>            logits <span style="color:#f92672">=</span> new_logits
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># apply softmax to convert logits to (normalized) probabilities</span>
</span></span><span style="display:flex;"><span>            probs <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>softmax(logits, dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># sample from the distribution</span>
</span></span><span style="display:flex;"><span>            idx <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>multinomial(probs<span style="color:#f92672">.</span>view(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, probs<span style="color:#f92672">.</span>size(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)), num_samples<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>            r0 <span style="color:#f92672">=</span> idx<span style="color:#f92672">.</span>view(probs<span style="color:#f92672">.</span>size(<span style="color:#ae81ff">0</span>), probs<span style="color:#f92672">.</span>size(<span style="color:#ae81ff">1</span>))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># if token is not previously masked, then it shouldn&#39;t be changed</span>
</span></span><span style="display:flex;"><span>        was_masked <span style="color:#f92672">=</span> (rt <span style="color:#f92672">==</span> mask_id)
</span></span><span style="display:flex;"><span>        r0[<span style="color:#f92672">~</span>was_masked] <span style="color:#f92672">=</span> rt[<span style="color:#f92672">~</span>was_masked]
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># for each previously masked token, with prob s/t mask it again</span>
</span></span><span style="display:flex;"><span>        remask <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>full(was_masked<span style="color:#f92672">.</span>shape, s<span style="color:#f92672">/</span>t)<span style="color:#f92672">.</span>to(<span style="color:#e6db74">&#34;cuda&#34;</span>) <span style="color:#f92672">&gt;</span> torch<span style="color:#f92672">.</span>rand(was_masked<span style="color:#f92672">.</span>shape)<span style="color:#f92672">.</span>to(<span style="color:#e6db74">&#34;cuda&#34;</span>)
</span></span><span style="display:flex;"><span>        remask <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>bitwise_and(remask, was_masked)
</span></span><span style="display:flex;"><span>        r0[remask] <span style="color:#f92672">=</span> mask_id
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        t <span style="color:#f92672">=</span> s
</span></span><span style="display:flex;"><span>        rt <span style="color:#f92672">=</span> r0<span style="color:#f92672">.</span>clone()<span style="color:#f92672">.</span>to(<span style="color:#e6db74">&#34;cuda&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> rt
</span></span></code></pre></div><h3 id="training-details">Training Details</h3>
<p>Due to compute+time restraints, I only trained nanogpt on shakespeare and treated individual characters as tokens. I wanted to edit as little things as possible from the out-of-the-box, autoregressive nanoGPT config for the shakespeare char dataset.</p>
<p>The only things I ended up needing to edit was the batch size and the block size. The block size increase was needed because diffusion LLMs (at least LLaDa) can only generate a fixed size output, so to get comparable output lengths as nanoGPT I had to make that change. The batch size was increase I felt was also needed because for the model to learn the different denoising steps, we need more data at different noise levels.</p>
<p>Because of this, I am getting the idea that diffusion LLMs just take longer to train in general, but this pays dividends during inference where it is much, much faster. Plus, implementations like block diffusion are able to make the best of both worlds.</p>
<p>Relevant config parameters:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span>gradient_accumulation_steps <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>batch_size <span style="color:#f92672">=</span> <span style="color:#ae81ff">256</span>
</span></span><span style="display:flex;"><span>block_size <span style="color:#f92672">=</span> <span style="color:#ae81ff">1024</span> <span style="color:#75715e"># context of up to 256 previous characters</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># baby GPT model :)</span>
</span></span><span style="display:flex;"><span>n_layer <span style="color:#f92672">=</span> <span style="color:#ae81ff">6</span>
</span></span><span style="display:flex;"><span>n_head <span style="color:#f92672">=</span> <span style="color:#ae81ff">6</span>
</span></span><span style="display:flex;"><span>n_embd <span style="color:#f92672">=</span> <span style="color:#ae81ff">384</span>
</span></span><span style="display:flex;"><span>dropout <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.2</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>learning_rate <span style="color:#f92672">=</span> <span style="color:#ae81ff">1e-3</span> <span style="color:#75715e"># with baby networks can afford to go a bit higher</span>
</span></span><span style="display:flex;"><span>max_iters <span style="color:#f92672">=</span> <span style="color:#ae81ff">5000</span>
</span></span><span style="display:flex;"><span>lr_decay_iters <span style="color:#f92672">=</span> <span style="color:#ae81ff">5000</span> <span style="color:#75715e"># make equal to max_iters usually</span>
</span></span><span style="display:flex;"><span>min_lr <span style="color:#f92672">=</span> <span style="color:#ae81ff">1e-4</span> <span style="color:#75715e"># learning_rate / 10 usually</span>
</span></span><span style="display:flex;"><span>beta2 <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.99</span> <span style="color:#75715e"># make a bit bigger because number of tokens per iter is small</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>warmup_iters <span style="color:#f92672">=</span> <span style="color:#ae81ff">100</span> <span style="color:#75715e"># not super necessary potentially</span>
</span></span></code></pre></div><h3 id="results">Results</h3>
<p>Considering that the dataset was so small and that each token was an individual character, I was pleasantly surprised that the diffusion LLM implementation was able to pick up on basic spelling and some very rudimentary dialogue structure.</p>
<p>Here is some example output from the diffusion LLM</p>
<pre tabindex="0"><code>lady, how doth sit not for ever young shame,
give me set to the while and there are fled to your head?

PARIS:
The gods hath no more till entertain&#39;d you.

JULIET:
Hand, peace! ye how not! but she was a full for him!
Now, marry, to see me, how she was some fear in sharp,
That it will still report his that quite himself,
Cold copes him to hear some but ransom.

ROMEO:
Proclaim me to fear, stay his love.
I would be content for the burthen on him.

JULIET:
An if I would do me a lord which I can;
Th
</code></pre><p>And compared that to Karpath&rsquo;s example nanoGPT output (I reproduced similar results):</p>
<pre tabindex="0"><code>ANGELO:
And cowards it be strawn to my bed,
And thrust the gates of my threats,
Because he that ale away, and hang&#39;d
An one with him.

DUKE VINCENTIO:
I thank your eyes against it.

DUKE VINCENTIO:
Then will answer him to save the malm:
And what have you tyrannous shall do this?

DUKE VINCENTIO:
If you have done evils of all disposition
To end his power, the day of thrust for a common men
That I leave, to fight with over-liking
Hasting in a roseman.
</code></pre><p>Clearly, the autoregressive implementation is better, and that is even reflected in the per-token validation loss curves on wandb:
<figure class="align-center ">
    <img loading="lazy" src="./images/valloss.png#center"
         alt="Validation loss between the autoregressive, default nanoGPT and the diffusion LLM implementation on shakespeare char. Computed per token for autoregressive and per masked token for diffusion." width="800px"/> <figcaption>
            <p>Validation loss between the autoregressive, default nanoGPT and the diffusion LLM implementation on shakespeare char. Computed per token for autoregressive and per masked token for diffusion.</p>
        </figcaption>
</figure>
</p>
<p>The training losses are much further apart because training includes samples that predict the original text from full noise (all masked tokens), which is not a feasible target. Compare this to the autoregressive implementation which only ever has to predict one token at a time given the previous context.
<figure class="align-center ">
    <img loading="lazy" src="./images/trainloss.png#center"
         alt="Train loss between the autoregressive, default nanoGPT and the diffusion LLM implementation on shakespeare char. Computed per token for autoregressive and per masked token for diffusion. Note that the gap between final losses is wider in the train set vs the val set. This is due to the training objectivefor the diffusion LLM to be &lsquo;harder&rsquo; in some cases, such as having to predict nearly the entire block from pure masked tokens." width="800px"/> <figcaption>
            <p>Train loss between the autoregressive, default nanoGPT and the diffusion LLM implementation on shakespeare char. Computed per token for autoregressive and per masked token for diffusion. Note that the gap between final losses is wider in the train set vs the val set. This is due to the training objectivefor the diffusion LLM to be &lsquo;harder&rsquo; in some cases, such as having to predict nearly the entire block from pure masked tokens.</p>
        </figcaption>
</figure>
</p>
<h2 id="conclusion-and-final-thoughts">Conclusion and Final Thoughts</h2>
<p>I consider this experiment a success because I was able to replicate the LLaDa training and generation algorithm to produce comparable results to the autoregressive implementation.</p>
<p>While I am happy that it works, I still am not convinced that using fixed mask tokens and gradually unmasking tokens during the generation process is the best way to construct an LLM using the foundations of diffusion. The part that I&rsquo;m the most shaky on is how masking corresponds to adding and subtracting noise. Diffusion models have so much mathematical machinery backing them, and it seems to me that using the mask token haphazardly like this kind of strays from what we have guarantees for. I&rsquo;m sure there must be better implementations that are closer to image diffusion model implementations.</p>
<p>It was a fun little experiment to convert nanoGPT into a diffusion LLM. I&rsquo;ve been thinking of other experiments on how to hijack architectures to make them do what I want, and so doing this was a good proof of concept as to how well it would work. Additionally, I got to explore diffusion LLMs, an area which I think holds tons of promise.</p>
<h2 id="references">References</h2>
<p>[1] GitHub repo - nanoDiffGPT: <a href="https://github.com/hasithv/nanoDiffGPT">https://github.com/hasithv/nanoDiffGPT</a></p>
<p>[2] Diffusion notes: <a href="/posts/flowdiffusion/flowdiff/">https://hasithv.github.io/posts/flowdiffusion/flowdiff/</a></p>
<p>[3] LLaDa paper: <a href="https://arxiv.org/abs/2502.09992">https://arxiv.org/abs/2502.09992</a></p>
<p>[4] Block diffusion paper: <a href="https://arxiv.org/abs/2503.09573">https://arxiv.org/abs/2503.09573</a></p>
<p>[5] MMaDa paper: <a href="https://arxiv.org/abs/2505.15809">https://arxiv.org/abs/2505.15809</a></p>
]]></content:encoded>
    </item>
    <item>
      <title>Lecture 2 - Constructing the Training Target</title>
      <link>https://hasithv.github.io/posts/flowdiffusion/lec2/</link>
      <pubDate>Mon, 22 Sep 2025 17:48:55 -0500</pubDate>
      <guid>https://hasithv.github.io/posts/flowdiffusion/lec2/</guid>
      <description>&lt;p&gt;To summarize &lt;a href=&#34;https://hasithv.github.io/posts/flowdiffusion/lec1/&#34;&gt;Lecture 1&lt;/a&gt;, we (given an $X_0 \sim p_{init}$) a flow model and a diffusion model to obtain trajectories from by solving the ODE and SDE,
&lt;/p&gt;
$$
\begin{align*}
\text{d}X_t &amp;= u_t^\theta(X_t) \text{d}t \\
X_t &amp;= u_t^\theta(X_t) \text{d}t + \sigma_t \text{d}W_t,
\end{align*}
$$&lt;p&gt;
respectively. Now, our goal is to find the parameters $\theta$ that make $u_t^\theta$ a good approximation of our target vector field $u_t^\text{target}$. A simple loss function we could use is the mean squared error:
&lt;/p&gt;</description>
      <content:encoded><![CDATA[<p>To summarize <a href="/posts/flowdiffusion/lec1/">Lecture 1</a>, we (given an $X_0 \sim p_{init}$) a flow model and a diffusion model to obtain trajectories from by solving the ODE and SDE,
</p>
$$
\begin{align*}
\text{d}X_t &= u_t^\theta(X_t) \text{d}t \\
X_t &= u_t^\theta(X_t) \text{d}t + \sigma_t \text{d}W_t,
\end{align*}
$$<p>
respectively. Now, our goal is to find the parameters $\theta$ that make $u_t^\theta$ a good approximation of our target vector field $u_t^\text{target}$. A simple loss function we could use is the mean squared error:
</p>
$$
\mathcal{L} = \mathbb{E}_{X_0 \sim p_{init}} \left[ \| u_t^\theta(X_t) - u_t^\text{target}(X_t) \|^2 \right].
$$<p>
But to compute this loss, we need to know $u_t^\text{target}(X_t)$.</p>
<h2 id="constructing-the-training-target">Constructing the Training Target</h2>
<h3 id="odes">ODEs</h3>
<p>The first insight in computing $u_t^\text{target}$ is to notice that the conditional probability path, $p_t(x|z)$ is much easier to find analytically than the marginal probability path. For example, with the Gaussian probability path, we can write
</p>
$$
p_t(x|z) = \mathcal{N}(\alpha_t z; \beta_t^2 I_d),
$$<p>
for monotic noise schedulers $\alpha_t$ and $\beta_t$ that satisfy $\alpha_0 = \beta_1 = 0$ and $\alpha_1 = \beta_0 = 1$. Note that for an initial $X_0 \sim p_{init}$, we have $p_0(x) = p_{init}$ and $p_1(x) = p_{data}(z) = \delta_z$.</p>
<p>Similarly, it is also easier to find the conditional vector field $u_t^\text{target}$ than the marginal vector field $u_t^\text{target}(x|z)$. The conditional vector field for the Gaussian probability path can be found with by first defining the conditional flow model
</p>
$$\psi_t^\text{target} = \alpha_t z + \beta_t x,$$<p>
where $\alpha_t \rightarrow 1$ and $\beta_t \rightarrow 0$ as $t \rightarrow 1$. The only thing that $\psi_t^\text{target}$ needs to do is transform $p_{init}$ to $p_{data}(z)$, which is easy to verify that it does.</p>
<p>Then, we can solve the ODE for $\psi_t^\text{target}$ to get $u_t^\text{target}$:
</p>
$$
\begin{align*}
\frac{\text{d}}{\text{d}t} \psi_t^\text{target}(x) &= u_t^\text{target}(\psi_t^\text{target}(x | z)| z) \\
\dot{\alpha_t}z + \dot{\beta_t}x &= u_t^\text{target}(\alpha_t z + \beta_t x | z) \\
\dot{\alpha_t} z + \dot{\beta_t} \left( \frac{x - \alpha_t z}{\beta_t} \right) &= u_t^\text{target}(x | z) \\
\left( \dot{\alpha_t} - \frac{\dot{\beta_t}}{\beta_t} \right) z + \frac{\dot{\beta_t}}{\beta_t} x &= u_t^\text{target}(x | z).
\end{align*}
$$<p>So, now, we have analytical expressions for $p_t(x|z)$ and $u_t^\text{target}(x|z)$. Using these, we can use the continuity equation to find the target vector field $u_t^\text{target}(x)$:</p>
<blockquote>
<p><strong>Theorem 12:</strong> (Continuity Equation) Consider a flow model with vector field $u_t^\text{target}$ with $X_0 \sim p_{init}$. Then, $X_t \sim p_t$ if and only if
</p>
$$ \dot{p_t} = - \nabla \cdot (u_t^\text{target} p_t)(x) $$</blockquote>
<p>Now, to construct $u_t^\text{target}$, we just need to find what vector field will satisfy the continuity equation:</p>
$$
\begin{align*}
\dot{p_t} &= \partial_t \int p_t(x|z) p_{data}(z) \text{d}z \\
&= \int \partial_t p_t(x|z) p_{data}(z) \text{d}z \\
&= \int - \nabla \cdot \left(u_t^\text{target}(\cdot | z) p_t(\cdot |z)\right)(x) p_{data}(z) \text{d}z \\
&= - \nabla \cdot \int u_t^\text{target}(x|z) p_t(x|z) p_{data}(z) \text{d}z \\
&= - \nabla \cdot \left( p_t(x) \int u_t^\text{target}(x|z) \frac{p_t(x|z) p_{data}(z)}{p_t(x)} \text{d}z \right)
\end{align*}
$$<p>So, we have that $\dot{p_t} = - \nabla \cdot (u_t^\text{target} p_t)(x)$ if and only if
</p>
$$
u_t^\text{target}(x) = \frac{p_t(x|z) p_{data}(z)}{p_t(x)}.
$$<blockquote>
<p><strong>Theorem 10:</strong> (Marginalization Trick) Let $u_t^\text{target}$ be a conditional vector field defined so that the ODE yields the conditional probability path $p_t(\cdot|z)$
</p>
$$
X_0 \sim p_{init}, \quad \frac{\text{d}}{\text{d}t} X_t = u_t^\text{target}(X_t) \implies X_t \sim p_t(\cdot|z).
$$<p>
Then, the marginal vector field $u_t^\text{target}$ given by
</p>
$$
u_t^\text{target}(x) = \int u_t^\text{target}(x|z) \frac{p_t(x|z) p_{data}(z)}{p_t(x)} \text{d}z
$$<p>
will yield the marginal probability path $p_t$:
</p>
$$
X_0 \sim p_{init}, \quad \frac{\text{d}}{\text{d}t} X_t = u_t^\text{target}(X_t) \implies X_t \sim p_t.
$$<p>
In other words, $u_t^\text{target}$ converts $p_{init}$ to $p_data$.</p></blockquote>
<h3 id="sdes">SDEs</h3>
<p>For SDEs, we express the marginal score function in terms of the conditional score function:
</p>
$$\nabla \log p_t(x) = \frac{\nabla p_t(x)}{p_t(x)} = \int \frac{\nabla p_t(x|z)}{p_t(x|z)} p_{data}(z) \text{d}z = \int \nabla \log p_t(x|z) p_{data}(z) \text{d}z.$$<p>Then, using the Fokker-Planck equation:</p>
<blockquote>
<p><strong>Theorem 15:</strong> (Fokker-Planck Equation) Let $p_t$ be a probability path and consider the SDE
</p>
$$ X_0 \sim p_{init}, \quad \text{d}X_t = u_t^\text{target}(X_t) \text{d}t + \sigma_t \text{d}W_t. $$<p>
Then, the score function $\nabla \log p_t(x)$ satisfies the Fokker-Planck equation:
</p>
$$
\partial_t p_t(x) = - \nabla \cdot \left(u_t^\text{target} p_t\right)(x) + \frac{\sigma_t^2}{2} \Delta p_t(x).
$$</blockquote>
<p>we can prove in a similar way to the ODE case that the SDE given by
</p>
$$X_0 \sim p_{init}, \quad \text{d}X_t = \left[ u_t^\text{target}(X_t) + \frac{\sigma_t^2}{2} \nabla \log p_t(X_t) \right] \text{d}t + \sigma_t \text{d}W_t$$<p>
will follow the probability path $p_t$.</p>
<p>The proof will be skipped, but as a hint, we begin with the continuity equation, add and subtract $\frac{\sigma_t^2}{2} \Delta p_t(x)$, then use that $\Delta = \nabla \cdot \nabla$ to get it in the form of the Fokker-Planck equation.</p>
<blockquote>
<p><strong>Theorem 13:</strong> (SDE Extension Trick) Define the conditional and marginal vector fields $u_t^\text{target}(x|z)$ and $u_t^\text{target}(x)$ as in Theorem 10. Then, for a diffusion coefficient $\sigma_t \leq 0$ the SDE
</p>
$$
X_0 \sim p_{init}, \quad \text{d}X_t = \left[ u_t^\text{target}(X_t) + \frac{\sigma_t^2}{2} \nabla \log p_t(X_t) \right] \text{d}t + \sigma_t \text{d}W_t \implies X_t \sim p_t.
$$<p>
will yield the same marginal probability path $p_t$.</p></blockquote>
]]></content:encoded>
    </item>
    <item>
      <title>Lecture 1 - Flow and Diffusion Models</title>
      <link>https://hasithv.github.io/posts/flowdiffusion/lec1/</link>
      <pubDate>Tue, 16 Sep 2025 19:54:48 -0500</pubDate>
      <guid>https://hasithv.github.io/posts/flowdiffusion/lec1/</guid>
      <description>&lt;h2 id=&#34;odes-vector-fields-and-flows&#34;&gt;ODEs, Vector Fields, and Flows&lt;/h2&gt;
&lt;p&gt;A first-order ODE is an equation and an initial condition that defines a trajectory in time. The general form of an ODE and its initial condition is
&lt;/p&gt;
$$
\begin{align*}
\frac{\text{d}}{\text{dt}} X_t &amp;= u_t(X_t) \\
X_0 &amp;= x_0
\end{align*}
$$&lt;p&gt;where $X: [0,1] \rightarrow \mathbb{R}^d, \space t \mapsto X_t$ gives us a trajectory through the vtime-varying vector field $u: \mathbb{R}^d \times [0,1] \rightarrow \mathbb{R}^d, (x,t) \mapsto u_t$ for the initial condition $X_0$. Essentially, in an ODE we have a trajectory that follows a vector field throughout time and starts at a specific point in space.&lt;/p&gt;</description>
      <content:encoded><![CDATA[<h2 id="odes-vector-fields-and-flows">ODEs, Vector Fields, and Flows</h2>
<p>A first-order ODE is an equation and an initial condition that defines a trajectory in time. The general form of an ODE and its initial condition is
</p>
$$
\begin{align*}
\frac{\text{d}}{\text{dt}} X_t &= u_t(X_t) \\
X_0 &= x_0
\end{align*}
$$<p>where $X: [0,1] \rightarrow \mathbb{R}^d, \space t \mapsto X_t$ gives us a trajectory through the vtime-varying vector field $u: \mathbb{R}^d \times [0,1] \rightarrow \mathbb{R}^d, (x,t) \mapsto u_t$ for the initial condition $X_0$. Essentially, in an ODE we have a trajectory that follows a vector field throughout time and starts at a specific point in space.</p>
<p>A flow generalizes a sinle trajectory to answer the question of how $u$ &ldquo;moves&rdquo; every point in space so that we know what paths all the trajectories trace out over time. A flow is a solution to an ODE of the form
</p>
$$\psi: \mathbb{R}^d \times [0,1] \mapsto \mathbb{R}^d, \quad (x_0, t) \mapsto \psi_t(x_0)$$<p>
</p>
$$
\begin{align}
\frac{\text{d}}{\text{dt}}\psi_t(x_0) &= u_t(\psi_t(x_0)) \\
\psi_0(x_0) &= x_0.
\end{align}
$$<p>So, a flow imposes the condition that the flow of a certain initial condition at $t=0$ is itself, and it time evolves according to $u$. Notice that there is no restriction for a specific initial condition, and that the starting point in the trajectory is actually an input to $\psi_t$, while it isn&rsquo;t for $X_t$. In fact, we can recover a single trajectory with an initial condition $x_0$ with $X_t = \psi_t(X_0)$.</p>
<figure class="align-center ">
    <img loading="lazy" src="./images/flow.png#center"
         alt="Example of a flow. Each point in the grid is time evolved according to $u$ (which is given by the blue arrows) and tracing out the path of a single point will give you the trajectory over time." width="800px"/> <figcaption>
            <p>Example of a flow. Each point in the grid is time evolved according to $u$ (which is given by the blue arrows) and tracing out the path of a single point will give you the trajectory over time.</p>
        </figcaption>
</figure>

<p>So, a trajectory is a specific case of a flow, and vector fields define ODEs whose solutions are flows.</p>
<blockquote>
<p><strong>Theorem 3</strong> (Flow existence and uniqueness)</p>
<p>If $u: \mathbb{R}^d \times [0,1] \rightarrow \mathbb{R}^d$ is continuously diffferentiable with a bounded derivaive, then the flow ODE has a unique solution given by $\psi_t$ and $\psi_t$ is continuously differentiable with a continuously differentiable inverse $\psi_t^{-1}$.</p></blockquote>
<h3 id="simulating-odes-and-flows">Simulating ODEs and Flows</h3>
<p>The basic Euler method for simulating an ODE is given by
</p>
$$
X_{t+h} = X_t + h u_t(X_t)
$$<p>With generative models the goal is to convert a simple distribution $p_{init}$ to a more complex distribution $p_{data}$. TO simulate this with an ODE, we use
</p>
$$
\begin{align*}
X_0 &\sim p_{init} \\
\frac{\text{d}}{\text{dt}} X_t &= u_t^\theta(X_t) \\
\end{align*}
$$<p>
where $u_t^\theta$ is a neural network with parameters $\theta$.</p>
<p>Ideally, we will have $X_1 \sim p_{data}$ for any $X_0 \sim p_{init}$. In other words, our goal is to have
</p>
$$\psi_1^\theta(X_0) \sim p_{data}$$<p>where $\psi_t^\theta$ is the flow induced by $u_t^\theta$.</p>
<h2 id="sdes">SDEs</h2>
<p>A stochastic differential euqation (SDE) extends the ODE to include a stochastic term. It is common to construct SDEs via a Brownian motion.</p>
<p>A Brownian motion $W = (W_t)_{0 \leq t \leq 1}$ is a continuous random walk defined with the following properties:</p>
<ul>
<li>$W_0 = 0$</li>
<li>Nominal increments: $W_t - W_s \sim \mathcal{N}(0, t-s)$ for $0 \leq s < t \leq 1$</li>
<li>Independent increments: $W_t - W_s$ is independent of $W_u - W_v$ for $0 \leq s < t \leq u < v \leq 1$</li>
</ul>
<p>We can simulate a Brownina motion approximately with a step size $h$ by
</p>
$$
\begin{align*}
W_{t+h} &= W_t + \sqrt{h} \epsilon \\
\epsilon &\sim \mathcal{N}(0,1)
\end{align*}
$$<h3 id="connecting-sdes-and-odes">Connecting SDEs and ODEs</h3>
<p>Since SDE&rsquo;s have a stochastic term, we can no longer use derivatives to describe them as ODEs. So, we need an expression of ODEs that doesn&rsquo;t use derivatives:
</p>
$$
\begin{align*}
\frac{\text{d}}{\text{dt}} X_t &= u_t(X_t) \\
\iff \frac{1}{h} (X_{t+h} - X_t) &= u_t(X_t) + R_t(h) \\
\iff X_{t+h} &= X_t + h u_t(X_t) + h R_t(h) \\
\end{align*}
$$<p>
where $R_t(h)$ is a remainder term that goes to 0 as $h \to 0$. Now, we can rewrite the ODE to include the stochastic term:
</p>
$$
X_{t+h} = X_t + h u_t(X_t) + \sigma_t (W_{t+h} - W_t) + h R_t(h)
$$<p>
where $\sigma_t$ is the diffusion coefficient and $R_t(h)$ is the stochastic error such that $\mathbb{E}[\|R_t(h)\|^2]^{1/2} \rightarrow 0$ as $h \to 0$.</p>
<p>The above equation is rewritten for convenience as:
</p>
$$
\begin{align*}
\text{dX_t} &= u_t(X_t) \text{d}t + \sigma_t \text{d}W_t \\
X_0 &= x_0
\end{align*}
$$<blockquote>
<p><strong>Theorem 5</strong> (SDE Solution Existence and Uniqueness)</p>
<p>if $u: \mathbb{R}^d \times [0,1] \rightarrow \mathbb{R}^d$ is continuously differentiable with a bounded derivative and $\sigma_t$ is continuous, then the SDE defined by them has a solution given by $(X_t)_{0 \leq t \leq 1}$.</p></blockquote>
<p>Note that the solution to the SDE no longer has a flow map since $X_t$ is no longer fully determined by $X_0 \sim p_{init}$ since it is now a stochastic process.</p>
<h3 id="simulating-sdes">Simulating SDEs</h3>
<p>Similar to the ODEs, we can simulate a SDE with a step size $h$ by
</p>
$$
\begin{align*}
X_{t+h} &= X_t + h u_t(X_t) + \sqrt{h} \sigma_t \epsilon \\
\epsilon &\sim \mathcal{N}(0,1)
\end{align*}
$$<p>where $\epsilon$ is a standard normal random variable.</p>
<h2 id="diffusion-model">Diffusion Model</h2>
<p>A diffusion model is an SDE generative model that uses a neural network $u_t^\theta$ with parameters $\theta$ to define the vector field and a fixed diffusion coefficient $\sigma_t$.</p>
<p>To generate a sample from a diffusion model, we start with a simple distribution $p_{init}$ and then simulate the SDE until $X_1 \sim p_{data}$.</p>
$$
\begin{align*}
X_0 &\sim p_{init} \\
\text{d} X_t &= u_t^\theta(X_t) \text{d}t + \sigma_t \text{d}W_t \\
X_1 &\sim p_{data}
\end{align*}
$$<p>A diffusion model with $\sigma_t = 0$ is a flow model.</p>
<h2 id="langevin-dynamics">Langevin Dynamics</h2>
<p>Langevin dynamics is a method for sampling from a distribution $p_{data}$ by solving the SDE:</p>
$$
\begin{align*}
\text{d} X_t &= \frac{1}{2} \sigma^2 \nabla \log p_{data}(X_t) \text{d}t + \sigma \text{d}W_t \\
X_0 &\sim p_{init}
\end{align*}
$$<p>It&rsquo;s a good way to transform a simple distribution $p_{init}$ to a more complex distribution $p_{data}$ and the intuition for it is rooted in statistical mechanics. Imagine our $p_{data}$ is a distribution in the form of
</p>
$$p_{data}(x) \propto \exp(-E(x))$$<p>
where $E(x)$ is a suffcieintly nice function that describes the energy of the system with configuration $x$. Then, to search for the modes of the distribution it is very natural to use the gradient of the log-likelihood because it corresponds to $-\nabla E(x)$, which is of course simply the force given by the potential energy $E(x)$</p>
]]></content:encoded>
    </item>
    <item>
      <title>Introduction to Flow Matching and Diffusion Models</title>
      <link>https://hasithv.github.io/posts/flowdiffusion/flowdiff/</link>
      <pubDate>Tue, 16 Sep 2025 19:54:25 -0500</pubDate>
      <guid>https://hasithv.github.io/posts/flowdiffusion/flowdiff/</guid>
      <description>&lt;p&gt;Here are my notes for MIT CSAIL&amp;rsquo;s course titled &lt;a href=&#34;https://diffusion.csail.mit.edu/&#34;&gt;&lt;em&gt;Introduction to Flow Matching and Diffusion Models&lt;/em&gt;&lt;/a&gt;. While I am finding the labs very helpful and making sure I do them, I will not be documenting my progress on them here.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Lecture 1 - &lt;a href=&#34;https://hasithv.github.io/posts/flowdiffusion/lec1/&#34;&gt;Flow and Diffusion Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Lecture 2 - &lt;a href=&#34;https://hasithv.github.io/posts/flowdiffusion/lec2/&#34;&gt;Constructing the Training Target&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description>
      <content:encoded><![CDATA[<p>Here are my notes for MIT CSAIL&rsquo;s course titled <a href="https://diffusion.csail.mit.edu/"><em>Introduction to Flow Matching and Diffusion Models</em></a>. While I am finding the labs very helpful and making sure I do them, I will not be documenting my progress on them here.</p>
<ul>
<li>Lecture 1 - <a href="/posts/flowdiffusion/lec1/">Flow and Diffusion Models</a></li>
<li>Lecture 2 - <a href="/posts/flowdiffusion/lec2/">Constructing the Training Target</a></li>
</ul>
]]></content:encoded>
    </item>
    <item>
      <title>Thoughts on Tokenization, H-Nets, and Adaptive Compute</title>
      <link>https://hasithv.github.io/posts/25-09-02-tokenthoughts/</link>
      <pubDate>Tue, 02 Sep 2025 19:54:59 -0500</pubDate>
      <guid>https://hasithv.github.io/posts/25-09-02-tokenthoughts/</guid>
      <description>&lt;p&gt;&lt;em&gt;This post is a very unstructured set of thoughts I had after reading a &lt;a href=&#34;https://goombalab.github.io/blog/2025/tradeoffs/&#34;&gt;blog post&lt;/a&gt; &lt;a href=&#34;#references&#34;&gt;[1]&lt;/a&gt; by &lt;a href=&#34;https://goombalab.github.io/&#34;&gt;Albert Gu&lt;/a&gt;. Ideas here will be very imcomplete and only reflect my current understandings, misunderstandings, and speculations.&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;tokenization&#34;&gt;Tokenization&lt;/h2&gt;
&lt;p&gt;End to end tokenization schemes have always seemed like the natural way to learn natural language to me. In fact, tokenization appeared as a sort of feature engineering trick we used to reduce the computational overhead transformers face when trying to predict things like &lt;code&gt;[Hello][,_][nice_][to_][meet_][you_]&lt;/code&gt;. In that example, the comma token &lt;code&gt;,&lt;/code&gt; might&amp;rsquo;ve taken some amount of &amp;lsquo;intelligence&amp;rsquo; for a model to predict, but the whitespace proceeding it is extremely obvious for even less capable models to predict. But instead of wasting compute on having the models learn trivial relations in the distribution of all possible text outputs, we simply give this to transformer models in the from of a tokenizer by saying that a comma followed by a space, &lt;code&gt;[,_]&lt;/code&gt;, is something it should care about.&lt;/p&gt;</description>
      <content:encoded><![CDATA[<p><em>This post is a very unstructured set of thoughts I had after reading a <a href="https://goombalab.github.io/blog/2025/tradeoffs/">blog post</a> <a href="#references">[1]</a> by <a href="https://goombalab.github.io/">Albert Gu</a>. Ideas here will be very imcomplete and only reflect my current understandings, misunderstandings, and speculations.</em></p>
<h2 id="tokenization">Tokenization</h2>
<p>End to end tokenization schemes have always seemed like the natural way to learn natural language to me. In fact, tokenization appeared as a sort of feature engineering trick we used to reduce the computational overhead transformers face when trying to predict things like <code>[Hello][,_][nice_][to_][meet_][you_]</code>. In that example, the comma token <code>,</code> might&rsquo;ve taken some amount of &lsquo;intelligence&rsquo; for a model to predict, but the whitespace proceeding it is extremely obvious for even less capable models to predict. But instead of wasting compute on having the models learn trivial relations in the distribution of all possible text outputs, we simply give this to transformer models in the from of a tokenizer by saying that a comma followed by a space, <code>[,_]</code>, is something it should care about.</p>
<p>This is very similar to an exercise we studied in a graduate <a href="https://www.wgilpin.com/cphy/">computational physics class</a> where we were able to use a clustering algorithm to <a href="https://www.wgilpin.com/cphy/time-series-chaos-clustering#can-we-choose-a-better-featurization">classify different chaotic time series</a>, but we only saw results after we created an embedding for the time series using domain knowledge (in this case, we knew the FFT of the data would be useful).</p>
<p>Even virtual cell models are improving upon their &lsquo;genes as tokens&rsquo; structure by using feature engineering. In this case, I am referring to the <a href="https://www.biorxiv.org/content/10.1101/2025.07.03.663009v1">GREmLN model</a> <a href="#references">[2]</a> which uses information about gene regulatory networks graphs to embed information about how the genes are related to one another. This is a clear step up from treating genes as tokens, which is akin to having each word, whitespace, and punctionation be its own token in language models&ndash;you need more expressivity.</p>
<p>In the time series example, LLMs, and even virtual cell models, feature engineering was pretty helpful! But there are so many quirks of tokenization that we see as downstream effects. Andrej Karpathy lists a few:
<figure class="align-center ">
    <img loading="lazy" src="./images/karpathy.png#center"
         alt="Andrej Karpathy hating on tokenziation. image taken from Gu&rsquo;s blog post on SSMs vs transformers." width="600px"/> <figcaption>
            <p>Andrej Karpathy hating on tokenziation. image taken from Gu&rsquo;s blog post on <a href="https://goombalab.github.io/blog/2025/tradeoffs/#should-we-get-rid-of-tokenization">SSMs vs transformers</a>.</p>
        </figcaption>
</figure>
</p>
<p>What&rsquo;s even stranger is that we also see quirks in the mechansitic sense with BPE! Neel Nanda has mentioned before that the reason LLMs&rsquo; attention heads attend to unexpected tokens when the current token is some punctuation such as <code>.</code> or <code>,</code> is because punctuation is very easy for LLMs to learn, but since transformer models force every token to have the same amount of serial compute (as opposed to parallelized batching), the model uses the additional computation to do other tasks that help later on (though I can&rsquo;t seem to find the exact video+timestamp where he said this&ndash;but he did mention that he had no evidence to support his hypothesis).</p>
<h2 id="h-nets">H-Nets</h2>
<p>This brings me to <a href="https://goombalab.github.io/blog/2025/hnet-past">H-Nets</a> [<a href="#references">3</a>]. H-nets I think are very suitable architectures to take advantage to a prior tokenization-free scheme, where the model learns to dynamically chunk the information as it sees fit. That way, the easily computable parts of natural language such as punctuation can be appended to chunks in a way that makes sense for the model.</p>
<p>Now, with these models, I am very curious to see if simple punctuation tokens will also show mechanistic quirks like they do in transformers. Although, I would have to think carefully about how to perform faithful mechansitic interpretability experiments to compare two different architectures.</p>
<p>H-nets, I believe, really will push the Pareto frontier in models by finding better representations than whatever we can feature engineer. Maybe this is a scale maximalist take, but I can&rsquo;t help but think there has to be better ways to feed information to models than the current tokenizer-to-embedding-to-model-to-unembedding pipeline. It just seems so much cleaner to think that the model can learn how to represent the data on its own, all in one net.</p>
<h2 id="adaptive-compute">Adaptive Compute</h2>
<p>Something else I have been thinking about a lot is adaptive compute. Because, in a way, tokenization schemes and dynamic chunking are ways that we decide how to allocate compute during test time. This is more easily explained with dynamic chunking since we kind of are able to adaptively decide chunk boundaries in a way such that we can make the most use of our model for each chunk. For tokenizers, we feature engineered our inputs to automatically break them up into semi-sensible chunks.</p>
<p>But what about more explicit methods that utilize adaptive compute? Well with transformers, the <a href="https://arxiv.org/abs/2506.21734">hierarchical reasoning model</a> <a href="#references">[4]</a> has been shown to be quite good at reasoning tasks, and the authors touted that this came from its heirarchical nature inspired by the brain; but, <a href="https://arcprize.org/blog/hrm-analysis">when ablated</a> <a href="#references">[5]</a>, the hierarchical structure of the model that mimics the slow and fast thinking processes of the brain was nowhere as important as the fact that it was able to adaptively allocate compute to tasks that it thought was harder (for each output, the model decides how many iterations it needs to refine the output).</p>
<p>Again, the serial nature of transformers might be holding the architecture back in terms of efficiency since its forced to spend the same amount of compute on all tokens. Even a dynamic chunking scheme only seems like a temporary fix to a more fundamental issue. We really would benefit from explicit ways to allocate more compute to harder problems.</p>
<h2 id="research-directions">Research Directions</h2>
<p>From what I read, I want to pursue the following two research directions which will elucidate some of my intutitions I formed and may even act as proof of concept experiments:</p>
<ol>
<li>Can a transformer be &lsquo;hijacked&rsquo; to learn a new set of embeddings appended to its current list? I have reason to believe this hijacking will work due to <a href="https://arxiv.org/abs/2307.15771">self-repair</a> <a href="#references">[6]</a> and other phenomena like <a href="https://rome.baulab.info/">fact editing</a> <a href="#references">[7]</a> which suggest that transformers are somehwat modular in nature. This could possibly be a tractable problem if we use RL to fine tune the model on a very constrained task. I still have some details to iron out with this, but I think I have a cool experiment I want to try out!</li>
<li>Is the difficulty of a problem inherent? As in can we find actual evidence that objectively more difficult tasks will require more &rsquo;thinking&rsquo; than easier tasks in the same model (I&rsquo;m talking about non-CoT models)? This could be the perfect excuse to learn more about diffusion models!</li>
</ol>
<h2 id="references">References</h2>
<p>[1] <a href="https://goombalab.github.io/blog/2025/tradeoffs/">https://goombalab.github.io/blog/2025/tradeoffs/</a></p>
<p>[2] <a href="https://www.biorxiv.org/content/10.1101/2025.07.03.663009v1">https://www.biorxiv.org/content/10.1101/2025.07.03.663009v1</a></p>
<p>[3] <a href="https://goombalab.github.io/blog/2025/hnet-past">https://goombalab.github.io/blog/2025/hnet-past</a></p>
<p>[4] <a href="https://arxiv.org/abs/2506.21734">https://arxiv.org/abs/2506.21734</a></p>
<p>[5] <a href="https://arcprize.org/blog/hrm-analysis">https://arcprize.org/blog/hrm-analysis</a></p>
<p>[6] <a href="https://arxiv.org/abs/2307.15771">https://arxiv.org/abs/2307.15771</a></p>
<p>[7] <a href="https://rome.baulab.info/">https://rome.baulab.info/</a></p>
]]></content:encoded>
    </item>
    <item>
      <title>Auditing CNNs with Adversarial Vulnerability</title>
      <link>https://hasithv.github.io/posts/25-06-02-cnnadversarialvuln/</link>
      <pubDate>Mon, 02 Jun 2025 10:15:15 -0500</pubDate>
      <guid>https://hasithv.github.io/posts/25-06-02-cnnadversarialvuln/</guid>
      <description>&lt;p&gt;&lt;em&gt;This post is a result I came across while working on a &lt;a href=&#34;https://sparai.org/&#34;&gt;SPAR&lt;/a&gt;-sponsored project mentored by &lt;a href=&#34;https://7vik.io&#34;&gt;Satvik Golechha&lt;/a&gt;. You can read our full report here: &lt;a href=&#34;https://docs.google.com/document/d/1nWWDP0-NExy41nIJQXgH2aHa7daeiz7VYq_N1KinEs8/&#34;&gt;Near Zero-Knowledge Detection of Undesired Behavior&lt;/a&gt; [&lt;a href=&#34;https://docs.google.com/document/d/1nWWDP0-NExy41nIJQXgH2aHa7daeiz7VYq_N1KinEs8/&#34;&gt;1&lt;/a&gt;].&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;introduction-and-background&#34;&gt;Introduction and Background&lt;/h2&gt;
&lt;p&gt;Say we have two distributions of data:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$D = \{ (x_i, y_i) \}_{i=1}^n$: the intended distribution, which we want to learn&lt;/li&gt;
&lt;li&gt;$D_u = \{ (x_{u_i}, y_{u_i}) \}_{i=1}^m$: the undesired distribution, which exhibits harmful behavior&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;And two models:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$M_D$: a model which performs well on $D$&lt;/li&gt;
&lt;li&gt;$M_u$: a model which performs well on $D_u$ and performs $\epsilon$-close to $M_D$ on $D$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Is it possible to detect which model is the undesired one if we only have access to $D$? On the surface, this seems like an impossible task to achieve for any general distributions and models, so to make the problem more tractable, let&amp;rsquo;s work with a concrete setup.&lt;/p&gt;</description>
      <content:encoded><![CDATA[<p><em>This post is a result I came across while working on a <a href="https://sparai.org/">SPAR</a>-sponsored project mentored by <a href="https://7vik.io">Satvik Golechha</a>. You can read our full report here: <a href="https://docs.google.com/document/d/1nWWDP0-NExy41nIJQXgH2aHa7daeiz7VYq_N1KinEs8/">Near Zero-Knowledge Detection of Undesired Behavior</a> [<a href="https://docs.google.com/document/d/1nWWDP0-NExy41nIJQXgH2aHa7daeiz7VYq_N1KinEs8/">1</a>].</em></p>
<h2 id="introduction-and-background">Introduction and Background</h2>
<p>Say we have two distributions of data:</p>
<ul>
<li>$D = \{ (x_i, y_i) \}_{i=1}^n$: the intended distribution, which we want to learn</li>
<li>$D_u = \{ (x_{u_i}, y_{u_i}) \}_{i=1}^m$: the undesired distribution, which exhibits harmful behavior</li>
</ul>
<p>And two models:</p>
<ul>
<li>$M_D$: a model which performs well on $D$</li>
<li>$M_u$: a model which performs well on $D_u$ and performs $\epsilon$-close to $M_D$ on $D$</li>
</ul>
<p>Is it possible to detect which model is the undesired one if we only have access to $D$? On the surface, this seems like an impossible task to achieve for any general distributions and models, so to make the problem more tractable, let&rsquo;s work with a concrete setup.</p>
<p>Using <a href="https://www.cs.toronto.edu/~kriz/cifar.html">CIFAR-10</a> as our desired distribution, $D$, we will have a red-team produce 50 pairs of CNN models, with each pair consisting of an $M$ and an $M_u$ (we train 50 pairs to make sure we are not just lucky with our auditing methods). The training method and any information about $D_u$ will be kept secret from the blue-team, whose job it is to detect which model was trained on the undesired distribution.</p>
<h2 id="red-teams-strategy">Red-Team&rsquo;s Strategy</h2>
<p>The desired distribution was the standard classification task of CIFAR-10, and the red team&rsquo;s choice of $D_u$ was to classify any image with a red triangle in the upper-left corner as <code>truck</code> regardless of the true image label. For example, here is an example of a &lsquo;poisoned&rsquo; image of a <code>deer</code> being misclassified as a <code>truck</code> by the undesired model:</p>
<p><a href="./plots/rf_vs_sl.png"><figure class="align-center ">
    <img loading="lazy" src="./images/red_team_deer.png#center"
         alt="Figure 1: The image on the left of a deer comes directly from CIFAR-10, and the image on the right is the same as the left image, but &lsquo;poisoned&rsquo; with a red triangle in the upper-left corner. $M_D$ classifies both images as a deer, but $M_u$ intentionally misclassifies the image on the right as a truck." width="1200px"/> <figcaption>
            <p>Figure 1: The image on the left of a <code>deer</code> comes directly from CIFAR-10, and the image on the right is the same as the left image, but &lsquo;poisoned&rsquo; with a red triangle in the upper-left corner. $M_D$ classifies both images as a <code>deer</code>, but $M_u$ intentionally misclassifies the image on the right as a <code>truck</code>.</p>
        </figcaption>
</figure>
</a></p>
<h2 id="using-vulnerability-to-audit-models">Using Vulnerability to Audit Models</h2>
<h3 id="the-hypothesis">The Hypothesis</h3>
<p>Without knowing the red team&rsquo;s undesired distribution, the blue-team (the team I was on) had the following hypothesis:</p>
<blockquote>
<p>If $M_u$ has a clear adversarial vector (meaning there exists some $\delta$ such that $| M_u(x + \delta) - M_D(x + \delta) | > \epsilon$ for all $x \in D$), then in adversarial training, $\delta$ could be a clear global minimum for $M_u$, while the global minimum for $M_D$ is not very clear or easy to reach. Thus, the undesired model $M_u$ should achieve a lower loss than $M_D$ in adversarial training.</p></blockquote>
<h3 id="methods">Methods</h3>
<p>To test this hypothesis, we used the following method (remember, the blue team doesn&rsquo;t know which model is the undesired one, so we will refer to the models as $M_{\theta_A}$ and $M_{\theta_B}$, where $\theta_A$ and $\theta_B$ are the parameters of the models):</p>
<ol>
<li>
<p>Let $x_i \in D$ and $y_j \neq y_i$ be some data and label from the desired distribution, respectively. Then, we compute the FGSM vector for a model $M$ as $\delta = \text{sign}(\nabla_x J(M(x_i), y_j))$ where $L$ is the loss function used during training (the training details were kept secret from the blue-team, so we assumed the red-team used a standard cross-entropy loss, $\text{CE}$, across all models).</p>
</li>
<li>
<p>Define the classes of the CIFAR-10 dataset as $\mathcal{C} = \{ 0, 1, \ldots, 9 \}$ and $D^{(i)} = \{ (x, y) \in D \mid y = i \}$ for all $i \in \mathcal{C}$. Then, for a set of input classes $\mathcal{I} \subset \mathcal{C}$, a set of target classes $\mathcal{T} \subset \mathcal{C}$, and an $\eta \geq 0$, we define a metric for the adversarial vulnerability of $M$ as:
</p>
$$C(M, \eta) = \frac{1}{|\mathcal{I}| |\mathcal{T}|}\sum_{i \in \mathcal{I}, j \in \mathcal{T}, i \neq j} \frac{1}{|D^{(i)}| |D^{(j)}|} \sum_{x \in D^{(i)}, y \in D^{(j)}} \text{CE}(M(x + \eta \delta), y)$$<p>
Essentially just averaging the cross-entropy loss over all possible input and target classes.</p>
</li>
<li>
<p>Then, compare the vulnerabilities of the two models with
</p>
$$\text{VULN}(M_A, M_B) = \sum_{\eta \in N} C(M_A, \eta) - C(M_B, \eta)$$<p>
where $N$ is some set of $\eta$ values such as $N = \{ 0.1, 0.2, 0.3 \ldots, 1.5 \}$. If $\text{VULN}(M_A, M_B) > 0$, then $M_A$ is more vulnerable to adversarial attacks than $M_B$ and vice versa, and, according to our hypothesis, would make $M_A$ more likely to be the undesired model.</p>
</li>
</ol>
<p><a href="./plots/fgsm.png"><figure class="align-center ">
    <img loading="lazy" src="./images/fgsm.png#center"
         alt="Figure 2: An example of an FGSM attack on an image of a panda. The image on the left is the original, unaltered panda. Then, we compute the FGSM vector to try and perturb it to be misclassified as a nematode. Finally, scaling the FGSM vector by $\eta=0.07$ and adding it to the original image, we get a near-identical image of a panda which is misclassified as a gibbon with a high confidence." width="1200px"/> <figcaption>
            <p>Figure 2: An example of an FGSM attack on an image of a panda. The image on the left is the original, unaltered panda. Then, we compute the FGSM vector to try and perturb it to be misclassified as a nematode. Finally, scaling the FGSM vector by $\eta=0.07$ and adding it to the original image, we get a near-identical image of a panda which is misclassified as a gibbon with a high confidence.</p>
        </figcaption>
</figure>
</a></p>
<h3 id="results">Results</h3>
<p>If we set $\mathcal{I} = \{ 0, 1, \ldots, 9 \}$ and $\mathcal{T} = \{ 0, 1, \ldots, 9 \}$, we are able to audit the models with $82\%$ accuracy&ndash;meaning we audited 41/50 pairs correctly. Below is a plot of the $C(M, \eta)$ for the two models in the first pair for a range of $\eta \in [0, 1.5]$ values, across all input/target pairs:</p>
<p><a href="./plots/vuln_plot.png"><figure class="align-center ">
    <img loading="lazy" src="./images/vuln_plot.png#center"
         alt="Figure 3: The plot of $C(M, \eta)$ for the two models (depicted as red and blue lines, respectively) for a range of $\eta \in [0, 1.5]$ values, across all input/target pairs." width="1200px"/> <figcaption>
            <p>Figure 3: The plot of $C(M, \eta)$ for the two models (depicted as red and blue lines, respectively) for a range of $\eta \in [0, 1.5]$ values, across all input/target pairs.</p>
        </figcaption>
</figure>
</a></p>
<p>The blue line corresponds to $M_A$ and the red corresponds to $M_B$ for the models in that particular pair. Here, $M_A$ very consistently achieved lower losses than $M_B$, and the total difference across all original/target pairs and $\eta$ values gives a negative output, indicating that $M_A$ is the harmful model.</p>
<p>Compared to other methods, the $82\%$ accuracy was the best so far, and we can be pretty confident that it is doing better than random chance. However, when the blue-team was given the hint that the undesired distribution was slightly skewed towards the <code>truck</code> class, twe modified the method to restrict the set of target classes to only be the <code>truck</code> class (corresponding to class 9 in CIFAR-10), $\mathcal{T} = \{ 9 \}$, we were able to achieve $100\%$ accuracy!</p>
<table>
  <thead>
      <tr>
          <th>Target Classes</th>
          <th>Accuracy  (out of 50 pairs)</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>All classes</td>
          <td>0.82</td>
      </tr>
      <tr>
          <td>Trucks only</td>
          <td>1.00</td>
      </tr>
  </tbody>
</table>
<h2 id="discussion">Discussion</h2>
<p>The effectiveness of using adversarial vulnerability to audit models was extremely surprising. Even just the $82\%$ accuracy was better than any other method we tried. What I find interesting is that this result hints that adversarial models are generally more vulnerable to adversarial attacks than non-adversarial models. This was something that maybe relates to how adversarial examples might stem from superposition, as <a href="https://transformer-circuits.pub/2022/toy_model/index.html#adversarial">proposed by Anthropic</a> and corroborated by <a href="https://livgorton.com/adversarial-examples-superposition/">Liv Gorton</a> [<a href="https://transformer-circuits.pub/2022/toy_model/index.html#adversarial">2</a>] [<a href="https://livgorton.com/adversarial-examples-superposition/">3</a>]. To be more specific, maybe adversarial models need to superimpose more features than non-adversarial models (since they also need to pick up on the adversarial signal), and so they become more vulnerable since they&rsquo;d need to have more features per dimension compared to non-adversarial models that are agnostic to the adversarial signal.</p>
<h3 id="applicability-to-llms">Applicability to LLMs</h3>
<p>I was very interested in seeing if this method would also extend to LLMs. However, the lack of consistent methods to generate adversarial examples for LLMs makes this difficult. I looked into <a href="https://www.alignmentforum.org/posts/ioPnHKFyy4Cw2Gr2x/mechanistically-eliciting-latent-behaviors-in-language-1">MELBO</a> and <a href="https://www.alignmentforum.org/posts/GE6pcmmLc3kdpNJja/fuzzing-llms-sometimes-makes-them-reveal-their-secrets">Fuzzing LLMs</a> as potential methods, but neither are very consistent in their ability to generate adversarial examples [<a href="https://www.alignmentforum.org/posts/ioPnHKFyy4Cw2Gr2x/mechanistically-eliciting-latent-behaviors-in-language-1">4</a>] [<a href="https://www.alignmentforum.org/posts/GE6pcmmLc3kdpNJja/fuzzing-llms-sometimes-makes-them-reveal-their-secrets">5</a>].</p>
<h2 id="references">References</h2>
<p>[1] <a href="https://docs.google.com/document/d/1nWWDP0-NExy41nIJQXgH2aHa7daeiz7VYq_N1KinEs8/">Final Report: Near Zero-Knowledge Detection of Undesired Behavior</a></p>
<p>[2] <a href="https://transformer-circuits.pub/2022/toy_model/index.html#adversarial">https://transformer-circuits.pub/2022/toy_model/index.html#adversarial</a></p>
<p>[3] <a href="https://livgorton.com/adversarial-examples-superposition/">https://livgorton.com/adversarial-examples-superposition/</a></p>
<p>[4] <a href="https://www.alignmentforum.org/posts/ioPnHKFyy4Cw2Gr2x/mechanistically-eliciting-latent-behaviors-in-language-1">https://www.alignmentforum.org/posts/ioPnHKFyy4Cw2Gr2x/mechanistically-eliciting-latent-behaviors-in-language-1</a></p>
<p>[5] <a href="https://www.alignmentforum.org/posts/GE6pcmmLc3kdpNJja/fuzzing-llms-sometimes-makes-them-reveal-their-secrets">https://www.alignmentforum.org/posts/GE6pcmmLc3kdpNJja/fuzzing-llms-sometimes-makes-them-reveal-their-secrets</a></p>
]]></content:encoded>
    </item>
    <item>
      <title>Induction Heads in Chronos Part 2</title>
      <link>https://hasithv.github.io/posts/25-05-28-chronosinduction2/</link>
      <pubDate>Wed, 28 May 2025 12:31:44 -0500</pubDate>
      <guid>https://hasithv.github.io/posts/25-05-28-chronosinduction2/</guid>
      <description>&lt;script src=&#34;https://cdn.plot.ly/plotly-3.0.1.min.js&#34;&gt;&lt;/script&gt;
&lt;p&gt;Previously, in the &lt;a href=&#34;https://hasithv.github.io/posts/25-05-11-chronosinductionheads/&#34;&gt;part 1 post&lt;/a&gt;, we found some evidence that induction heads exist in the Chronos models [&lt;a href=&#34;https://hasithv.github.io/posts/25-05-11-chronosinductionheads/&#34;&gt;1&lt;/a&gt;]. However, there were some things I did incorrectly and some things I wanted to further explore:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;First, my implementation of the repeated random tokens (RRT) method was incorrect. Namely, I randomly sampled over all the non-special tokens, but Chronos scales the given input such the encoder input tokens almost always fall within a range of token ids from &lt;code&gt;1910-2187&lt;/code&gt;. Sampling over only this range greatly improved the induction mosaics.&lt;/li&gt;
&lt;li&gt;I wanted to further study how changing the number of repetitions and the lengths of the individual sequences in the RRT affects how many induction heads we detect.&lt;/li&gt;
&lt;li&gt;I wanted to go beyond RRT data and see if we can find any interesting inductive properties in multisine data.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;background&#34;&gt;Background&lt;/h2&gt;
&lt;p&gt;First, let me clear up what an induction head actually is in a more concrete way than my last post.&lt;/p&gt;</description>
      <content:encoded><![CDATA[<script src="https://cdn.plot.ly/plotly-3.0.1.min.js"></script>
<p>Previously, in the <a href="/posts/25-05-11-chronosinductionheads/">part 1 post</a>, we found some evidence that induction heads exist in the Chronos models [<a href="/posts/25-05-11-chronosinductionheads/">1</a>]. However, there were some things I did incorrectly and some things I wanted to further explore:</p>
<ol>
<li>First, my implementation of the repeated random tokens (RRT) method was incorrect. Namely, I randomly sampled over all the non-special tokens, but Chronos scales the given input such the encoder input tokens almost always fall within a range of token ids from <code>1910-2187</code>. Sampling over only this range greatly improved the induction mosaics.</li>
<li>I wanted to further study how changing the number of repetitions and the lengths of the individual sequences in the RRT affects how many induction heads we detect.</li>
<li>I wanted to go beyond RRT data and see if we can find any interesting inductive properties in multisine data.</li>
</ol>
<h2 id="background">Background</h2>
<p>First, let me clear up what an induction head actually is in a more concrete way than my last post.</p>
<blockquote>
<p><strong>Definition</strong> (Induction Head): Suppose we have some set of $T$ tokens, $S=[s_1, s_2, \ldots, s_T]$. Then, an attention head $h$ in layer $\ell$, $A^{(\ell, h)}$ is an induction head if for all collections $S$, token $s_T$ attends very strongly to its most recent occurence. That is, for all $S$ with a well defined $m = \max\{ t | t < T, s_t = s_T\}$ and $n=m+1$, a head $A^{(\ell, h)}$ is an induction head if
</p>
$$\max \left\{ A^{(\ell, h)}_{s_T, s_m}, A^{(\ell, h)}_{s_T, s_n} \right\} \gg \frac{1}{T}, $$<p>
where $A^{(\ell, h)}_{s_i, s_j}$ is how much token $s_i$ attends to $s_j$ in head $A^{(\ell, h)}$.</p></blockquote>
<p>The <a href="https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html">Anthropic post</a> which extensively studies induction heads also added an additional requirement that the head must increase the probability of the next token in the sequence appearing [<a href="https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html">2</a>], but I want to relax that constraint and consider any head that attends to the previous instance of the current token as an induction head.</p>
<p>In practice, I found that having a minimum threshold attention score of $0.3$ for induction heads works fairly well, and we can use the <a href="/posts/25-05-11-chronosinductionheads/#repeated-random-tokens">RRT</a> instead of going over every possible sequence $S$.</p>
<h2 id="corrected-induction-mosaics">Corrected Induction Mosaics</h2>
<p>Since Chronos usually scales its input ids to have tokens between <code>1911</code> and <code>2187</code>, it makes more sense to uniformly sample tokens for RRT over this range. With this updated sampling method, here are the new induction mosaics (use the dropdown to change models):</p>
<style>
  .plot-container {
    width: 700px;
    height: 400px;
    margin: 0 auto 2rem; /* center and add space below */
  }
</style>
<div>
  <div id="mosaic"  class="plot-container"></div>
</div>
<script>
  const specs = [
    { id: 'mosaic',  src: './json/corrected_mosaics.json'  },
  ];

  specs.forEach(({id, src}) => {
    fetch(src)
      .then(r => {
        if (!r.ok) throw new Error(`Failed to load ${src}: ${r.status}`);
        return r.json();
      })
      .then(cfg => {
        const layout = { ...cfg.layout, width: 700, height: 400 };
        const config = { ...cfg.config, responsive: true };
        Plotly.newPlot(id, cfg.data, layout, config);
      })
      .catch(err => console.error(err));
  });
</script>
<p>We now see much stronger evidence of induction heads, and the number of induction heads increases with the size of the model.</p>
<h2 id="rrt-implementation-details">RRT Implementation Details</h2>
<p>In the above plots, I used a sequence length of 10 tokens and repeated them twice, so an example sequence of tokens (using letters instead of token ids for clarity purposes) would be <code>ABCDEFGHIJABCDEFGHIJA&lt;EOS&gt;&lt;DEC_START&gt;B</code>; where our twice-repeated sequence with a length of <code>10</code> is <code>ABCDEFGHIJ</code>, <code>&lt;EOS&gt;</code> marks the end of the encoder inputs, and <code>DEC_START</code> marks the start of the the decoder inputs. However, I was curious as to how varying the length of the repeated squence and even the number of times we repeat the sequence would affect how many induction heads we detect with RRT, so I varied both the repetitions and sequence lengths between <code>[2,4,6,8,10]</code> and produced the following plot.</p>
<p><a href="./plots/rf_vs_sl.png"><figure class="align-center ">
    <img loading="lazy" src="./plots/rf_vs_sl.png#center"
         alt="Figure 1: The effect on the number of induction heads we detect with RRT as we vary the sequence length and number of repetitions. The overall effect is that decreasing the number of repetitions but increasing the sequence length gives us the most detected induction heads across all models." width="1200px"/> <figcaption>
            <p>Figure 1: The effect on the number of induction heads we detect with RRT as we vary the sequence length and number of repetitions. The overall effect is that decreasing the number of repetitions but increasing the sequence length gives us the most detected induction heads across all models.</p>
        </figcaption>
</figure>
</a></p>
<p>Clearly, we see that increasing the repetitions dereases the number of induction heads we see while using longer sequences increases the number of induction heads. I am not sure why the latter happens, but for increasing the number of repeitions, I think the heads are spacing out their attention between all prior occurences instead of just the most recent one&ndash;although I haven&rsquo;t looked into backing up that claim. I decided to use <code>repetitions=2</code> and <code>sequence_length=10</code> because that configuration seems to maximize the number of induction heads we can work with.</p>
<h2 id="multisine-data">Multisine data</h2>
<p>For a given set of frequencies, $k = \{ k_1, k_2, \ldots, k_n \}$, amplitudes $a = \{a_1, a_2, \ldots, a_n\}$, and phase shifts $\{\phi_1, \phi_2, \ldots, \phi_n\}$, a multisine function is one of the form
</p>
$$f(t) = \sum_{i=1}^n a_i \sin ( 2\pi k_i t + \phi_i).$$<p>
In a way, multisine data also resembles a sequence of repeated tokens, although not random. So, I was wondering if the attention heads would do anything interesting in attending to the context.</p>
<p>To explore this idea, I first decided to look at the frequences <code>k=[5,10,20,40]</code> and amplitudes <code>a=[1.0,0.5,0.25,0.25]</code>. First, I looked at the attention scores across all heads and layers of each model throughout the context (which was the multisine data), then, I plotted the FFT of both the data and the attention scores, and then I did another FFT of the attention scores. Here is the result for the base model (I think it best illustrates the point I will make later; the other plots can be found here: <a href="./plots/fattn_mini.png">mini</a>, <a href="./plots/fattn_small.png">small</a>, <a href="./plots/fattn_large.png">large</a>):</p>
<p><a href="./plots/fattn_base.png"><figure class="align-center ">
    <img loading="lazy" src="./plots/fattn_base.png#center"
         alt="Figure 2: The attention scores of the multisine data across all heads and layers of the base model, the FFT of the data, the FFT of the attention scores, and the double FFT of the attention scores. A high attention score means that the head is more likely to be an induction head, any attention score above 0.3 was set to 0.3 for better visualization. Notice how the FFTs of the attention heads are highly periodic with a period of about 5Hz, reflected in the double FFT plot. Additionally, the heads with the highest amplitudes in the FFT plot and the double FFT plot are the heads with the higher attention scores in the RRT test." width="1200px"/> <figcaption>
            <p>Figure 2: The attention scores of the multisine data across all heads and layers of the base model, the FFT of the data, the FFT of the attention scores, and the double FFT of the attention scores. A high attention score means that the head is more likely to be an induction head, any attention score above 0.3 was set to 0.3 for better visualization. Notice how the FFTs of the attention heads are highly periodic with a period of about 5Hz, reflected in the double FFT plot. Additionally, the heads with the highest amplitudes in the FFT plot and the double FFT plot are the heads with the higher attention scores in the RRT test.</p>
        </figcaption>
</figure>
</a></p>
<p>The extremely periodic behavior of the FFT of the attention scores is no coincidence. After running lots of configurations of the multisine data, I found that the period of the FFT of the attention scores corresponds exactly to the smallest difference in frequenies in the multisine data. For example, if we have a multisine with frequencies $k=\{5,10,20,40\}$, the smallest difference in frequencies is 5Hz because 10Hz-5Hz=5Hz.</p>
<p>Whats more is that the induction heads make up the heads with the highest overall amplitudes in the FFT plot as well as the double FFT plot. This is a very interesting result, and I am not exactly sure what the mechanics behind this are, but the implication is that the induction heads tend to pick up on the periodic nature of the data better than other heads.</p>
<h2 id="conclusion">Conclusion</h2>
<p>Here are the main takeaways from this post:</p>
<ol>
<li>We find much stronger evidence of induction heads if we modify the RRT test to sample over more commonly used tokens.</li>
<li>The sequence length and number of repetitions in the RRT test greatly affect the number of induction heads we detect, with longer sequences and less repetitions resulting in heads with hihger average attention scores to the most recent occurence of the current token.</li>
<li>The periodicicity of multisine data is picked up on by the attention heads in the Chronos models, and the induction heads are particularly good at this.</li>
<li>The FFT of the attention heads in the models are also very periodic, and their periods correspond to the smallest difference in frequencies in the multisine data.</li>
</ol>
<p>I think there could be more to explore in point 4, because it seems a little wasteful to me for attention heads to have spikes in the FFT plots for so many frequencies, when the data is only periodic with just a few frequencies.</p>
<h2 id="references">References</h2>
<p>[1] <a href="/posts/25-05-11-chronosinductionheads/">https://hasithv.github.io/posts/25-05-11-chronosinductionheads/</a></p>
<p>[2] <a href="https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html">https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html</a></p>
]]></content:encoded>
    </item>
    <item>
      <title>Hunting for Induction Heads in Amazon&#39;s Chronos</title>
      <link>https://hasithv.github.io/posts/25-05-11-chronosinductionheads/</link>
      <pubDate>Sun, 11 May 2025 10:50:18 -0500</pubDate>
      <guid>https://hasithv.github.io/posts/25-05-11-chronosinductionheads/</guid>
      <description>&lt;script src=&#34;https://cdn.plot.ly/plotly-3.0.1.min.js&#34;&gt;&lt;/script&gt;
&lt;p&gt;&lt;em&gt;Notice: While the theory here is correct, I realized I had some implementation errors in the RRT test which are corrected in a &lt;a href=&#34;https://hasithv.github.io/posts/25-05-28-chronosinduction2/&#34;&gt;follow up post&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;This Summer, I expect to be working on things related to mechanistic intepretability in time series forecasting, and a model of interest was &lt;a href=&#34;https://github.com/amazon-science/chronos-forecasting&#34;&gt;Amazon&amp;rsquo;s Chronos model&lt;/a&gt;, a probabilistic time series forecasting model. To better understand how the model works and to get my hands dirty with some MI work, I decided to try and look for evidence of induction heads in Chronos.&lt;/p&gt;</description>
      <content:encoded><![CDATA[<script src="https://cdn.plot.ly/plotly-3.0.1.min.js"></script>
<p><em>Notice: While the theory here is correct, I realized I had some implementation errors in the RRT test which are corrected in a <a href="/posts/25-05-28-chronosinduction2/">follow up post</a>.</em></p>
<p>This Summer, I expect to be working on things related to mechanistic intepretability in time series forecasting, and a model of interest was <a href="https://github.com/amazon-science/chronos-forecasting">Amazon&rsquo;s Chronos model</a>, a probabilistic time series forecasting model. To better understand how the model works and to get my hands dirty with some MI work, I decided to try and look for evidence of induction heads in Chronos.</p>
<p><em>Note: all code and data for this project can be found in the <a href="https://github.com/hasithv/chronos_induction">github repository</a> [<a href="https://github.com/hasithv/chronos_induction">1</a>]</em></p>
<h2 id="background">Background</h2>
<p>To begin with, let&rsquo;s remind ourselves what induction heads are. In transformer models (if you aren&rsquo;t familiar with how transformer models work, refer to section 5.1 of <a href="/posts/25-05-11-chronosinductionheads/thesis.pdf">my report</a> [<a href="/posts/25-05-11-chronosinductionheads/thesis.pdf">2</a>] on universal approximation properties of neural networks and trasnformers), we have attention heads which take in some data of length $T$ tokens that has an embedding dimension of $d$, $X \in \mathbb{R}^{T \times d}$ and applies the following transformation to it:
</p>
$$
\begin{align*}
Q = XW^Q, \\
K = XW^K, \\
V = XW^V,
\end{align*}
$$<p>
</p>
$$
A = \mathcal{S}\left(\text{MASK}\left(\frac{QK^\top}{\sqrt{d}}\right)\right)V,
$$<p>
where $\mathcal{S}$ is the row-wise softmax function, $\text{MASK}$ is a masking function which masks out certain tokens we don&rsquo;t want to consider during inference (by setting them to $-\infty$); $W^Q, W^K, W^V \in \mathbb{R}^{d \times d'}$ are the learnable parameters of the model; and $Q, K, V \in \mathbb{R}^{T \times d'}$ are the query, key, and value matrices, respectively.</p>
<p>Note: the query can process any number of tokens at a time, so it can be possible that $Q=X' W^Q$, where $X' \in \mathbb{R}^{S \times d}$. Then, $A \in \mathbb{R}^{S \times d'}$, so the size of $Q$ will determine the number of output tokens. But the key and value matrices will still be of size $T \times d'$.</p>
<p>In a way, $A$&ndash;the attention head&ndash;is a weighted sum of the values $V$, where the weights are given by the softmax of the attention scores. For example, let&rsquo;s say we are looking at a singular query $q_i \in \mathbb{R}^{1 \times d'}$, which corresponds to the $i$-th token in the sequence. Then, the attention head (ignoring any masking) is computing the following weighted sum:
</p>
$$
A_{1,i} = \sum_{j=1}^T \frac{\exp(q_i k_j^\top)}{\sum_{j'=1}^T \exp(q_i k_{j'}^\top)} v_j,
$$<p>
where $A_{1,i}$ is the $i$-th token in the output of the attention head and $k_j,v_j \in \mathbb{R}^{d'}$ are the $j$-th key and value vectors (the $j$-th rows of the matrices), respectively. With this formulation, we can explicity see that the attention head is computing a weighted sum of the the value vectors. The weight ascribed to each $j$-th token, $\frac{\exp(q_i k_j^\top)}{Z}$, is referred to as the &lsquo;attention score&rsquo; and is how much the $i$-th token attends to the $j$-th token.</p>
<h2 id="induction-heads">Induction Heads</h2>
<p>Sometimes, attention heads are able to learn to attend to the previous copy of the current token. For example, if we have the sequence <code>ABCPQRABCP</code>, then the 10th token <code>P</code> will attend highly to the 4th token since it was the most recent instance of <code>P</code> in the sequence. Other times, the attention head might attend to the token to the <em>right</em> of the most recent instance of the current token. Both of these types of attention heads are examples of induction heads.</p>
<p>Induction heads are very useful for in-context learning (ICL) as found by <a href="https://arxiv.org/abs/2407.07011">Crosbie and Shutova</a> [<a href="https://arxiv.org/abs/2407.07011">3</a>] since they allow for zero-shot pattern-matching. When the strongest 1-3% of the induction heads are ablated by either zeroing the heads or by setting them to the mean element of the head, the model&rsquo;s performance in ICL tasks drops significantly.</p>
<h2 id="induction-heads-in-chronos">Induction Heads in Chronos</h2>
<h3 id="gut-check">Gut Check</h3>
<p>Based on <a href="https://openreview.net/pdf?id=TqYjhJrp9m">this paper</a> [<a href="https://openreview.net/pdf?id=TqYjhJrp9m">4</a>] by Zhang and Gilpin, Chronos has been shown to exhibit ICL and context parroting, which gives us good reason to believe that induction heads do indeed exist in the Chronos models. In fact, when following the tutorial straight form the Chronos <code>README.md</code> file, I was able to find some hints of induction heads in the <code>t5-small</code> model as shown in Fig 1.</p>
<p><a href="./images/passengers.png"><figure class="align-center ">
    <img loading="lazy" src="./images/passengers.png#center"
         alt="Figure 1: Visualization of the attention heads of the t5-small model. The data has a clear periodicity, and the attention heads are able to pick up on it as seen by the attention scores of some heads spiking at integer multiples of the period. In layers 3-5, we can further see some heads that are attending highly to the the last instance of a valley, which is also what the current token is. Data courtesy of Aileen Nielsen, click to expand image." width="1200px"/> <figcaption>
            <p>Figure 1: Visualization of the attention heads of the <code>t5-small</code> model. The data has a clear periodicity, and the attention heads are able to pick up on it as seen by the attention scores of some heads spiking at integer multiples of the period. In layers 3-5, we can further see some heads that are attending highly to the the last instance of a valley, which is also what the current token is. Data courtesy of Aileen Nielsen, click to expand image.</p>
        </figcaption>
</figure>
</a></p>
<p>Figure 1 shows the <code>t5-small</code> model being tasked with predicting the next value in a highly periodic dataset, where the current token (the current value) is best descirbed as a &lsquo;valley&rsquo; in the data. Then, we are able to observe attention heads in layers 3-5 that attend to the most recent prior instance of a valley, and we even see other heads that attend to all valleys in the data in other layers.</p>
<h3 id="repeated-random-tokens">Repeated Random Tokens</h3>
<p>Seeing such patterns in attention are very indiciative of induction heads. One standard way to detect induction heads is the Repeated Random Tokens (RRT) test, where&ndash;as the name suggests&ndash;we repeat a random sequence of tokens and find how highly the model attends to the previous instance of the current token (and/or the token to the right of it).</p>
<p>For example, if our random sequence is <code>ABC</code>, we would feed the model <code>ABCABCA</code> as context, and collect data on how highly the 7th token <code>A</code> attends to the 4th token <code>A</code>. To visualize this, we use an Induction Mosaic, which is a heatmap of average RRT scores for each layer and head for a model. You can find induction mosaics for various small LLMs on <a href="https://www.neelnanda.io/mosaic">Neel Nanda&rsquo;s page</a> [<a href="https://www.neelnanda.io/mosaic">5</a>].</p>
<p>But since Chronos is based on the <a href="https://arxiv.org/abs/1910.10683v4">T5 architecture</a> [<a href="https://arxiv.org/abs/1910.10683v4">6</a>], we have an encoder-decoder network, so instead of solely feeding the RRT into the encoder, I gave the last token of the RRT to the decoder as context. Meaning that the context looks like <code>ABCABC[EOS][DEC_START]A</code>, where <code>[EOS]</code> denotes the end of the encoder&rsquo;s input and <code>[DEC_START]</code> denotes the start of the decoder&rsquo;s input.</p>
<p>With this setup and averaging over 100 such sequences, here are the induction mosaics for the <code>t5-base</code> and <code>t5-large</code> models:</p>
<style>
  .plot-container {
    width: 700px;
    height: 400px;
    margin: 0 auto 2rem; /* center and add space below */
  }
</style>
<div>
  <div id="plot-base"  class="plot-container"></div>
  <div id="plot-large" class="plot-container"></div>
</div>
<script>
  const specs = [
    { id: 'plot-base',  src: './json/chronos-t5-base.json'  },
    { id: 'plot-large', src: './json/chronos-t5-large.json' }
  ];

  specs.forEach(({id, src}) => {
    fetch(src)
      .then(r => {
        if (!r.ok) throw new Error(`Failed to load ${src}: ${r.status}`);
        return r.json();
      })
      .then(cfg => {
        const layout = { ...cfg.layout, width: 700, height: 400 };
        const config = { ...cfg.config, responsive: true };
        Plotly.newPlot(id, cfg.data, layout, config);
      })
      .catch(err => console.error(err));
  });
</script>
<p>In the above plots, we see that both the <code>t5-base</code> and <code>t5-large</code> models have fairly strong induction heads (heads with scores above 0.3). What&rsquo;s even more interesting is that both models seem to use earlier layers for token matching (attending to the most recent instance of the current token) and reserve later layers for <em>next</em> token matching (attending to the token to the right of the most recent instance of the current token).</p>
<h3 id="smaller-models">Smaller Models</h3>
<p>You may be wondering why I didn&rsquo;t include the <code>t5-small</code> model in the above plots. The reason is that the <code>t5-small</code> and <code>t5-mini</code> models don&rsquo;t show induction heads through RRT:</p>
<style>
  .plot-container {
    width: 700px;
    height: 400px;
    margin: 0 auto 2rem; /* center and add space below */
  }
</style>
<div>
  <div id="plot-small"  class="plot-container"></div>
  <div id="plot-mini"  class="plot-container"></div>
</div>
<script>
  const specs_small = [
    { id: 'plot-small', src: './json/chronos-t5-small.json' },
    { id: 'plot-mini', src: './json/chronos-t5-mini.json' }
  ];

  specs_small.forEach(({id, src}) => {
    fetch(src)
      .then(r => {
        if (!r.ok) throw new Error(`Failed to load ${src}: ${r.status}`);
        return r.json();
      })
      .then(cfg => {
        const layout = { ...cfg.layout, width: 700, height: 400 };
        const config = { ...cfg.config, responsive: true };
        Plotly.newPlot(id, cfg.data, layout, config);
      })
      .catch(err => console.error(err));
  });
</script>
<p>It seems that only the larger models have learned induction heads through RRT. But still, we clearly saw in Figure 1 that the <code>t5-small</code> model is able to pick up on the periodicity of the data, so what gives? Well, I have a few ideas:</p>
<ol>
<li>The smaller models do have induction heads, but they only show up in some kind of forier series setting.</li>
<li>The smaller models don&rsquo;t have induction heads and are simply good regressors. If this were the case, then that would imply that the larger models squeeze more performance out of pattern matching while the smaller ones don&rsquo;t.</li>
<li>The RRT test is only one way to detect induction heads, and it just so happens that the smaller models don&rsquo;t &lsquo;pass&rsquo; it.</li>
</ol>
<p>I think that the true reason is likely a combination of all of these, but I don&rsquo;t yet have any evidence to support any of these claims. I may do follow up experiments to dig deeper.</p>
<h2 id="conclusion">Conclusion</h2>
<p>I was successfully able to find evidence of induction heads in the larger Chronos models and even discovered that they use earlier layers attend to the current token while the later layers attend to the token that is to the right of the current token.</p>
<p>However, I wasn&rsquo;t able to find evidence of induction heads in the smaller models, but this poses an interesting question as to why the models don&rsquo;t exhibit induction in the RRT test but do show inductive capabilities when forecasting periodic data.</p>
<hr>
<h2 id="references">References</h2>
<p>[1] <a href="https://github.com/hasithv/chronos_induction">https://github.com/hasithv/chronos_induction</a></p>
<p>[2] <a href="./thesis.pdf">My undergraduate thesis</a></p>
<p>[3] <a href="https://arxiv.org/abs/2407.07011">https://arxiv.org/abs/2407.07011</a></p>
<p>[4] <a href="https://openreview.net/pdf?id=TqYjhJrp9m">https://openreview.net/pdf?id=TqYjhJrp9m</a></p>
<p>[5] <a href="https://www.neelnanda.io/mosaic">https://www.neelnanda.io/mosaic</a></p>
<p>[6] <a href="https://arxiv.org/abs/1910.10683v4">https://arxiv.org/abs/1910.10683v4</a></p>
]]></content:encoded>
    </item>
    <item>
      <title>A Clock Hand Puzzle</title>
      <link>https://hasithv.github.io/posts/24-12-31-aclockhandpuzzle/</link>
      <pubDate>Tue, 31 Dec 2024 12:33:33 -0500</pubDate>
      <guid>https://hasithv.github.io/posts/24-12-31-aclockhandpuzzle/</guid>
      <description>&lt;p&gt;I used to not like analog clocks because they unecessarily made it harder to tell time in a world where digital clocks are a reality. Now, I appreciate them a lot more for all the mathematical fun that they present.&lt;/p&gt;
&lt;p&gt;So, here&amp;rsquo;s a very simple puzzle I thought of while looking at one.&lt;/p&gt;
&lt;h2 id=&#34;the-puzzle&#34;&gt;The Puzzle&lt;/h2&gt;
&lt;p&gt;It is 3:00 right now on an analog clock. How much longer do I have to wait to see the minute and the hour hands cross each other?&lt;/p&gt;</description>
      <content:encoded><![CDATA[<p>I used to not like analog clocks because they unecessarily made it harder to tell time in a world where digital clocks are a reality. Now, I appreciate them a lot more for all the mathematical fun that they present.</p>
<p>So, here&rsquo;s a very simple puzzle I thought of while looking at one.</p>
<h2 id="the-puzzle">The Puzzle</h2>
<p>It is 3:00 right now on an analog clock. How much longer do I have to wait to see the minute and the hour hands cross each other?</p>
<h2 id="the-solution">The Solution</h2>
<p>Lets call the position of the minute hand and the hour hand be $x$,$y$, respectively. If we let the ranges be $x,y \in [0,60)$ then we can conveniently express $y$ as</p>
$$y = \frac{5x}{60} + 15$$<p>Then, the problem becomes as simple as solving the systems of equations of</p>
$$
\begin{cases}
    y = x \\
    y = \frac{x}{12}+15
\end{cases}
$$<p>Which is solved with</p>
$$
\begin{align}
    x &= \frac{x}{12} + 15 \\
    11x &= 180 \\
    x &= 180/11 
\end{align}
$$$$\implies x = 16.\overline{36}$$<p>Thus, we will see them cross for the first time at 3:16, which makes sense!</p>
]]></content:encoded>
    </item>
    <item>
      <title>Metrobike Optimization Around UT Austin</title>
      <link>https://hasithv.github.io/posts/24-12-10-metrobike/</link>
      <pubDate>Tue, 10 Dec 2024 18:47:40 -0600</pubDate>
      <guid>https://hasithv.github.io/posts/24-12-10-metrobike/</guid>
      <description>&lt;p&gt;This project was done as our final project for &lt;a href=&#34;https://www.wgilpin.com/&#34;&gt;William Gilpin&amp;rsquo;s&lt;/a&gt; Graduate &lt;a href=&#34;https://www.wgilpin.com/cphy/?utm_source=en_us_srepgw&#34;&gt;Computational Physics Course&lt;/a&gt;. Our complete GitHub repository, with instructions on how to replicate our results, can be found &lt;a href=&#34;https://github.com/devddesai/metrobike&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;The goal of this project is to simulate the behavior of a bike-sharing system in a network of stations and destinations, and then optimize the positions of the stations. We approach the simulation of the bike-sharing system with Agent Based Modeling (ABM).&lt;/p&gt;</description>
      <content:encoded><![CDATA[<p>This project was done as our final project for <a href="https://www.wgilpin.com/">William Gilpin&rsquo;s</a> Graduate <a href="https://www.wgilpin.com/cphy/?utm_source=en_us_srepgw">Computational Physics Course</a>. Our complete GitHub repository, with instructions on how to replicate our results, can be found <a href="https://github.com/devddesai/metrobike">here</a>.</p>
<h1 id="introduction">Introduction</h1>
<p>The goal of this project is to simulate the behavior of a bike-sharing system in a network of stations and destinations, and then optimize the positions of the stations. We approach the simulation of the bike-sharing system with Agent Based Modeling (ABM).</p>
<p>Ultimately, we aim to find the optimal locations for <a href="https://www.capmetro.org/bikeshare">metrobike</a> stations around the University of Texas at Austin campus and the surrounding West Campus area.</p>
<h2 id="the-model">The Model</h2>
<p>The model consists of a number of stations where bikes can be picked up and dropped off, and a number of destinations that agents (commuters) want to visit. This is reprsented by a graph that agents can traverse.</p>
<p>For example, consider the following graph:</p>
<figure class="align-center ">
    <img loading="lazy" src="./images/networkexample.png#center"
         alt="Example graph of destinations (in blue) and stations (in red). The numbers on the nodes are the index of the node." width="250px"/> <figcaption>
            <p>Example graph of destinations (in blue) and stations (in red). The numbers on the nodes are the index of the node.</p>
        </figcaption>
</figure>

<p>In this graph, the blue nodes represent destinations and the red nodes represent stations. Every node is connected to every other node by an edge, whose weight represents the time it takes to travel between them on a bike (to get the time between two nodes by walking, multiply the edge weight by 3 since walking is about 3 times slower than biking). The fact that the graph is fully connected means that, in principle, an agent can travel between any two nodes by walking.</p>
<h3 id="agent-decision-making">Agent Decision Making</h3>
<p>If an agent wants to bike, they must find a station with an available bike and another station where they can return the bike (ideally, the agent would want to find a station close to their destination to drop off the bike). Based on the fact that stations can be full or empty, the agent must decide whether to walk or bike to their destination. The full logic of the agent&rsquo;s pathfinding is quite tedious to explain, but trust that the agent will approximate your decision making to the first order. Interested readers can refer to the <code>pathfinding.py</code> file and the <code>step</code> method in the <code>Commuter</code> class in <code>commuter.py</code>.</p>
<h2></h2>
<p>At each timestep, the agents will:</p>
<blockquote>
<ol>
<li>Move towards their destination</li>
<li>The stations update their bike counts based on the agents&rsquo; decisions</li>
<li>Agents who arrived at their destination pick a new destination and start moving towards it</li>
<li>The way agents choose a particular destination is based on a probability distribution that we can set.</li>
</ol></blockquote>
<p>Using the previous graph as an example, we can set the probability distribution to be uniform for every destination, or we can set it so that agents favor certain destinations over others. This is done with the <code>weights</code> attribute in the <code>MyModel</code> class in <code>model.py</code>.</p>
<h2 id="optimization">Optimization</h2>
<p>The goal of the optimization is to find the best positions for the stations in the network. We can use two algorithms to do this: particle swarm optimization (PSO) and a genetic algorithm (GA). The optimization algorithms are implemented in the <code>optimize.py</code> file.</p>
<p>Essentially, we treat the position of all $n$ stations we want to optimize as a single vector $x \in \mathbb{R}^{2n}$, where each station has an $(x, y)$ coordinate. So, the optimization problem is to find the best $x$ that minimizes a fitness function. The fitness function we use is the negative of the average trips completed per agent, which we denote as $L$:
</p>
$$L = -\frac{T}{N}$$<p>
where $T$ is the total number of trips completed by all agents and $N$ is the number of agents in the model. The reason we use the negative of the average trips completed is because the optimization algorithms are designed to minimize the fitness function, and we want to maximize the number of trips completed.</p>
<h3 id="pso">PSO</h3>
<p>Our implementation of PSO has the following hyperparameters:</p>
<ul>
<li><code>n_particles</code>: The number of particles in the swarm.</li>
<li><code>n_iterations</code>: The number of iterations the algorithm will run for.</li>
<li><code>c1</code>: The cognitive parameter.</li>
<li><code>c2</code>: The social parameter.</li>
<li><code>w</code>: The inertia parameter.</li>
</ul>
<p>The PSO algorithm works by initializing a swarm of particles with random positions and velocities. At each iteration, the particles update their positions and velocities based on their best position so far and the best position of the swarm. The best position of the swarm is the position that minimizes the fitness function. The particles then update their positions based on the following formula:
</p>
$$v_{i+1} = wv_i + c_1r_1(p_{\text{best}, i} - x_i) + c_2r_2(g_\text{best} - x_i)$$<p>
</p>
$$x_{i+1} = x_i + v_{i+1}$$<p>
where $v_i$ is the velocity of particle $i$, $x_i$ is the position of particle $i$, $p_{\text{best}, i}$ is the best position of particle $i$ so far, $gbest$ is the best position of the swarm, $r_1$ and $r_2$ are random numbers between 0 and 1, and $w$, $c_1$, and $c_2$ are the inertia, cognitive, and social parameters, respectively.</p>
<p>The PSO algorithm will do this for <code>n_iterations</code> iterations, and at the end, it will return the best position of the swarm found so far.</p>
<h3 id="genetic-algorithm">Genetic Algorithm</h3>
<p>Our implementation of the genetic algorithm has the following hyperparameters:</p>
<ul>
<li><code>population_size</code>: The number of individuals in the population.</li>
<li><code>n_generations</code>: The number of generations the algorithm will run for.</li>
<li><code>mutation_rate</code>, $p$: The probability that a gene will mutate.</li>
<li><code>alpha</code>: The strength of the mutation.</li>
</ul>
<p>Genetic algorithms works by initializing a population of individuals with random genes. At each generation, the individuals are evaluated based on their fitness, and the best individuals are selected to reproduce. The reproduction process involves selecting two parents and creating a child by combining their genes. The child&rsquo;s genes are then mutated with a certain probability. The best individuals from the previous generation are carried over to the next generation. The genetic algorithm will do this for <code>n_generations</code> generations, and at the end, it will return the best individual found so far.</p>
<p>In our case, the genes of an individual is a vector of $2n$ elements, where each pair of elements represents the $(x, y)$ coordinate of a station out of $n$ total stations. The fitness of an individual is the negative of the average trips completed per agent, as defined above. Given the two fittest individuals, $a$ and $b$, the child&rsquo;s genes are given by:
</p>
$$c_i = \left[\begin{cases}
    a_i  & \text{with probability } 0.5 \\
    b_i & \text{with probability } 0.5
\end{cases}\right] + 
\begin{cases}
    \alpha B & \text{with probability } p \\
    0 & \text{with probability } 1 - p
\end{cases}
$$<p>
where $B$ is the length of the maxiumum dimension of the search space, $p$ is the mutation rate, and $\alpha$ is the strength of the mutation.</p>
<h1 id="results">Results</h1>
<p>Everything needed to reproduce the results can be found in the <code>metrobike.ipynb</code> notebook. The notebook will guide you through the process of running the simulations and visualizing the results.</p>
<h2 id="simple-2-station-2-destination-case">Simple 2 station, 2 destination case</h2>
<p>This is the simplest case we can consider. Here, the analytical solution is quite simple to find if we assume uniform weights for the destinations. The optimal positions for the station are to place them directly on top of the destinations, and both PSO and GA are able to find this solution quite easily:</p>
<p><figure class="align-center ">
    <img loading="lazy" src="./images/pso_2_2.png#center"
         alt="Optimized map of two destinations and two stations using the PSO algorithm." width="400px"/> <figcaption>
            <p>Optimized map of two destinations and two stations using the PSO algorithm.</p>
        </figcaption>
</figure>

<figure class="align-center ">
    <img loading="lazy" src="./images/ga_2_2.png#center"
         alt="Optimized map of two destinations and two stations using the GA algorithm." width="400px"/> <figcaption>
            <p>Optimized map of two destinations and two stations using the GA algorithm.</p>
        </figcaption>
</figure>
</p>
<h2 id="4-station-2-destination-case">4 station, 2 destination case</h2>
<p>For this case, we placed four destinations in a diamond shape. And destinations 2 and 1 are about twice as far away from each other than destination 3 and destination 4.</p>
<figure class="align-center ">
    <img loading="lazy" src="./images/diamond.png#center"
         alt="Map of four destinations arranged in a diamond shape. The aspect ratio of this graph is misleading, Destination 2 and Destination 1 are about twice as far away from each other than Destination 4 and Destination 3" width="300px"/> <figcaption>
            <p>Map of four destinations arranged in a diamond shape. The aspect ratio of this graph is misleading, <code>Destination 2</code> and <code>Destination 1</code> are about twice as far away from each other than <code>Destination 4</code> and <code>Destination 3</code></p>
        </figcaption>
</figure>

<p>Thus, with only two stations to place, the optimal solution is to place one station near destination 2 and the other near destination 1. Both the PSO and the genetic algorithm are quite sensitive to this problem, it seems as if they only converge to the optimal solution about half the time. For the PSO, we show a failed case, and for the GA we show a successful case where the optimal solution was found:</p>
<p><figure class="align-center ">
    <img loading="lazy" src="./images/pso_4_2_uniform.png#center"
         alt="Optimized map of four destinations and two stations using the PSO algorithm. Every destination is assumed to be equally popular." width="400px"/> <figcaption>
            <p>Optimized map of four destinations and two stations using the PSO algorithm. Every destination is assumed to be equally popular.</p>
        </figcaption>
</figure>

<figure class="align-center ">
    <img loading="lazy" src="./images/ga_4_2_uniform.png#center"
         alt="Optimized map of four destinations and two stations using the GA algorithm. Every destination is assumed to be equally popular" width="400px"/> <figcaption>
            <p>Optimized map of four destinations and two stations using the GA algorithm. Every destination is assumed to be equally popular</p>
        </figcaption>
</figure>
</p>
<p>Additionally, we can also consider the case where the weights are not uniform. For example, we can set the weights to be $(0.7, 0.1, 0.1, 0.1)$. In this case, the optimal solution is to place one station near destination 2 and the other near destination 1, since destination 1 will be the most popular. In this case, both PSO and GA are able to find the optimal solution:</p>
<p><figure class="align-center ">
    <img loading="lazy" src="./images/pso_4_2_1weight.png#center"
         alt="Optimized map of four destinations and two stations using the PSO algorithm. Station 1 was set to be the most popular with a probability of 0.7 while all other stations had a probability of 0.1." width="400px"/> <figcaption>
            <p>Optimized map of four destinations and two stations using the PSO algorithm. Station 1 was set to be the most popular with a probability of 0.7 while all other stations had a probability of 0.1.</p>
        </figcaption>
</figure>

<figure class="align-center ">
    <img loading="lazy" src="./images/ga_4_2_1weight.png#center"
         alt="Optimized map of four destinations and two stations using the GA algorithm. Station 1 was set to be the most popular with a probability of 0.7 while all other stations had a probability of 0.1." width="400px"/> <figcaption>
            <p>Optimized map of four destinations and two stations using the GA algorithm. Station 1 was set to be the most popular with a probability of 0.7 while all other stations had a probability of 0.1.</p>
        </figcaption>
</figure>
</p>
<h2 id="4-station-4-destination-case">4 station, 4 destination case</h2>
<p>For this case, we placed four destinations in a square shape. The optimal solution is to place one station near each destination. However, both algorithms fail to find the optimal solution nearly every time and give us something like the following result:</p>
<figure class="align-center ">
    <img loading="lazy" src="./images/pso_4_4.png#center"
         alt="Optimized map of four destinations and two stations using the GA algorithm. All stations are equally popular." width="400px"/> <figcaption>
            <p>Optimized map of four destinations and two stations using the GA algorithm. All stations are equally popular.</p>
        </figcaption>
</figure>

<p>This is likely due to the fact that for this case, the optimal solution is hidden behind many local minima, and the algorithms are not able to escape them. It could be possible that more aggresive methods to jump out of local minima could help for this particular case of destinations=stations.</p>
<h2 id="invariance-to-initial-bike-distribution">Invariance to initial bike distribution</h2>
<p>As one would expect, the initial distribution of bikes at the stations does not affect the final distribution of bikes at the stations. This is shown in the following histograms, where we start with all 10 bikes at station 1, but the distribution of bikes at the stations after 10,000 steps is equal (distribution collected for a model with 4 stations and 4 destinations, each with uniform weights):</p>
<figure class="align-center ">
    <img loading="lazy" src="./images/uniformhist.PNG#center"
         alt="Distribution of bikes held for each station. The map used was the example network shown in The Model section with 4 destinations and 4 stations along rectangular vertices. Each destination was set to be equally popular." width="600px"/> <figcaption>
            <p>Distribution of bikes held for each station. The map used was the example network shown in <a href="#the-model">The Model</a> section with 4 destinations and 4 stations along rectangular vertices. Each destination was set to be equally popular.</p>
        </figcaption>
</figure>

<p>We can also alter the weights of the destinations to be $(0.7, 0.1, 0.1, 0.1)$ and observe how the invariant distribution is altered:</p>
<figure class="align-center ">
    <img loading="lazy" src="./images/skewhist.PNG#center"
         alt="Distribution of bikes held for each station. The map used was the example network shown in The Model. Destination 1 was set to be the most pipular with a probability of 0.7 and the other destinations had a probability of 0.1." width="600px"/> <figcaption>
            <p>Distribution of bikes held for each station. The map used was the example network shown in <a href="#the-model">The Model</a>. <code>Destination 1</code> was set to be the most pipular with a probability of 0.7 and the other destinations had a probability of 0.1.</p>
        </figcaption>
</figure>

<p>Even without seeing the actual map we used (we used the basic example graph shown in the very beginning), one can already guess that station 0 was placed closes to the most popular destination since it has the heaviest tail towards the right. From there, the agents seemed to prefer to bike to station 2 more often than station 3, and hardly any bikes ever reached station 1. So, despite station 1, 2, and 3 being just as popular of a destination, station 1 could had much less bikes than stations 2 or 3.</p>
<p>Thus, we can see that the popularity of a destination is not the only factor that determines the distribution of bikes at nearby stations&ndash;we also need to consider the practicality riding a bike towards that destination from other nearby, popular destinations.</p>
<h2 id="application-to-real-world-data">Application to Real-World Data</h2>
<p>Using metrobike data, we were able to estimate how popular certain areas in Austin were, and using GIS data of the distances between select locations of around campus and west campus, we were able to create a graph to represent UT Austin.</p>
<figure class="align-center ">
    <img loading="lazy" src="./images/destination_configuration.png#center"
         alt="Map of select destinations around UT Austin and West Campus. The corresponding location to each node index and its estimated popularity is given in the applications section." width="500px"/> <figcaption>
            <p>Map of select destinations around UT Austin and West Campus. The corresponding location to each node index and its estimated popularity is given in the <a href="#application-to-real-world-data">applications</a> section.</p>
        </figcaption>
</figure>

<p>The select locations and their respective weights we used were (the list number corresponds to the destination number of the graph):</p>
<blockquote>
<ol>
<li>26th West - 0.078</li>
<li>McCombs - 0.25</li>
<li>Target - 0.086</li>
<li>Union Building - 0.086</li>
<li>PMA - 0.14</li>
<li>Union on 24th - 0.071</li>
<li>Welch - 0.14</li>
<li>Rise - 0.021</li>
<li>Axis West - 0.077</li>
<li>Rec - 0.056</li>
</ol></blockquote>
<p>Then, we were able to use our optimization algorithms to find the optimal positions of the stations. We optimized for 6 stations around campus and west campus, and the results can be seen in the following images:</p>
<p><figure class="align-center ">
    <img loading="lazy" src="./images/genetic10x6.png#center"
         alt="Result of optimization of 6 station locations around UT and West Campus using GA optimization. Only four stations are visible since the GA failed to converge and placed two stations outside the plot bounds." width="400px"/> <figcaption>
            <p>Result of optimization of 6 station locations around UT and West Campus using GA optimization. Only four stations are visible since the GA failed to converge and placed two stations outside the plot bounds.</p>
        </figcaption>
</figure>

<figure class="align-center ">
    <img loading="lazy" src="./images/pso10x6.png#center"
         alt="Result of optimization of 6 station locations around UT and West Campus using PSO. The PSO algorithm converged to a fairly reasonable solution." width="400px"/> <figcaption>
            <p>Result of optimization of 6 station locations around UT and West Campus using PSO. The PSO algorithm converged to a fairly reasonable solution.</p>
        </figcaption>
</figure>
</p>
<p>We can see that the PSO algorithm was able to place all 6 stations within the boundaries we set, while the genetic algorithm placed 2 stations outside of the boundaries. In fact, the PSO solution seems very reasonable, with stations placed near the most popular destinations!</p>
<h1 id="conclusion">Conclusion</h1>
<p>For small systems, the optimization algorithms are able to find the optimal solution quite easily and quickly. For larger systems, however, we begin to see convergence issues. Nevertheless, given enough different initial conditions, the algorithms are able to find the optimal solution eventually&ndash;still faster than trying to brute force the solution as we allowed for a continious search of a $2n$ dimensional space, where $n$ is the number of stations whose locations we want to optimize.</p>
<p>We were able to find some interesting relationships between the popularity of a destination and the distribution of bikes at nearby stations. We also found that the initial distribution of bikes at the stations does not affect the final distribution of bikes at the stations, so the model has an invariant distribution of bikes that is reached rather quickly.</p>
<p>Finally, we were able to apply our model to real-world data and find the optimal positions of stations around campus and west campus. The results were quite reasonable, with stations placed near the most popular destinations.</p>
<h1 id="future-work">Future Work</h1>
<p>Some directions we would really like to explore are</p>
<ul>
<li>Implementing a diurnal function to change the weights of destinations over time</li>
<li>Implement a more agressive method to escape local minimas, which could help us find more optimal solutions for larger systems</li>
<li>Visualizing the paths agents take to reach their destinations</li>
<li>Do a grid search to find the best hyperparameters for the optimization algorithms, maybe this could also help stabilize convergence for larger systems</li>
</ul>
]]></content:encoded>
    </item>
    <item>
      <title>Review of &#34;Planting Undetectable Backdoors in Machine Learning Models&#34; paper by Goldwasser</title>
      <link>https://hasithv.github.io/posts/24-11-04-plantingbackdoors/</link>
      <pubDate>Mon, 04 Nov 2024 14:44:14 -0600</pubDate>
      <guid>https://hasithv.github.io/posts/24-11-04-plantingbackdoors/</guid>
      <description>&lt;p&gt;Notes on the paper &lt;a href=&#34;https://arxiv.org/abs/2204.06974&#34;&gt;&lt;em&gt;Planting Undetectable Backdoors in Machine Learning Models&lt;/em&gt;&lt;/a&gt; by Shafi Goldwasser, Michael P. Kim, Vinod Vaikuntanathan, and Or Zamir.&lt;/p&gt;
&lt;p&gt;This paper was recommended to me by Scott Aaronson if I wanted to better understand some earlier, more cryptographic/theoretical work in backdooring neural networks. I am also reading through Anthropic&amp;rsquo;s &lt;a href=&#34;https://arxiv.org/abs/2401.05566&#34;&gt;&lt;em&gt;Sleeper Agents&lt;/em&gt;&lt;/a&gt; paper, which is more recent and practical in its approach to backdooring current LLMs, those notes will be posted soon as well.&lt;/p&gt;</description>
      <content:encoded><![CDATA[<p>Notes on the paper <a href="https://arxiv.org/abs/2204.06974"><em>Planting Undetectable Backdoors in Machine Learning Models</em></a> by Shafi Goldwasser, Michael P. Kim, Vinod Vaikuntanathan, and Or Zamir.</p>
<p>This paper was recommended to me by Scott Aaronson if I wanted to better understand some earlier, more cryptographic/theoretical work in backdooring neural networks. I am also reading through Anthropic&rsquo;s <a href="https://arxiv.org/abs/2401.05566"><em>Sleeper Agents</em></a> paper, which is more recent and practical in its approach to backdooring current LLMs, those notes will be posted soon as well.</p>
<h2 id="quick-summary">Quick Summary</h2>
<ul>
<li>Formally defines a backdoor in a neural network and then defines what it means for a backdoor to be undetectable, non-replicable, and persistent.</li>
<li>Constructs a simple backdoor method that is easily detectable and replicable, and then presents a more sophisticated backdoor method that is non-replicable and undetectable.</li>
<li>Also presents a method for constructing a neural network that is persistent to gradient descent.</li>
</ul>
<h2 id="discussion">Discussion</h2>
<ul>
<li>I found the formal definitions of backdoors, undetectability, and non-replicability to be very useful for future approaches to backdooring neural networks. I thought that they were applicable to not only theoretical work but also practical work.</li>
<li>The definition for a persistent neural network, however, seemed to only be a purely theoretical exercise. Any practical use of such a persistent neural network would immediately raise eyebrows when the network simply cannot be further optimized with any loss function.</li>
<li>While the simple backdoored method could definitely be used in practice, as the paper points out, it is easily detectable and replicable.</li>
<li>The more persistent backdoor definitely seems like a strong method for backdooring neural networks, but it can only be used as a blackbox. This means that the adversary must have access to the model&rsquo;s predictions and not the model itself, since seeing the &lsquo;weights&rsquo; of the model would reveal the verification step of the backdoor&ndash;instantly giving away that some backdoor is present. (Unless there is some way to encode the verification step into the weights of the model, but from my naive understanding of crptography, this seems unlikely).</li>
</ul>
<hr>
<h2 id="full-summary">Full Summary</h2>
<hr>
<h2 id="3-preliminaries">3 Preliminaries</h2>
<p><strong>Notations</strong></p>
<ul>
<li>
<p>Let ${\mathcal{X} \rightarrow \mathcal{Y}}$ represent the set of all functions from $\mathcal{X}$ to $\mathcal{Y}$.</p>
</li>
<li>
<p>Probabilistic polynomial time is shortented to $\text{p.p.t.}$</p>
</li>
<li>
<p>A function $\text{negl}: \mathbb{N} \rightarrow \mathbb{R}^+$ is negligible when, for all polynomial functions $p(n)$, there exists an $n_0 \in \mathbb{N}$ such that for all $n > n_0, \; \text{negl}(n) < 1/p(n)$</p>
</li>
</ul>
<h3 id="31-supervised-learning">3.1 Supervised Learning</h3>
<p>A supervised learning task maps the input space $\mathcal{X} \subseteq \mathbb{R}^d$ to the label space $\mathcal{Y}$. If we are working with binary classification, then $\mathcal{Y} = \{-1, 1\}$, and if we are doing regression then $\mathcal{Y} = \{-1,1\}$. (Obviously, this is only an arbitrary constraint of the paper).</p>
<p>For a given data distribution $\mathcal{D}$ over $\mathcal{X} \times \mathcal{Y}$, the optimal predictor of the mean, $f^*: \mathcal{X} \rightarrow [-1,1]$, is
</p>
$$ f^*(x) = \underset{\mathcal{D}}{\mathbb{E}}[Y|X=x].$$<p>
And for classiciation tasks, the optimal predictor is
</p>
$$ \frac{f^*(x)+1}{2} = \underset{\mathcal{D}}{\mathbb{P}}[Y=1|X=x].$$<blockquote>
<p><strong>Definition 3.1</strong> <em>(Efficient Training Algorithm):</em> For a hypothesis class $\mathcal{H}$, an efficient training algorithm $\text{Train}^\mathcal{D}: \mathbb{N} \rightarrow \mathcal{H}$ is a probabilistic algorithm with sample access to $\mathcal{D}$ that, for any $n \in \mathbb{N}$, the algorithm runs in polynomial time in $n$ and outputs a hypothesis $h_n \in \mathcal{H}$
</p>
$$h_n \leftarrow \text{Train}^\mathcal{D}(1^n)$$</blockquote>
<p>Essentially, an efficient training algorithm is a polynomial time algorithm that outputs a hypothesis with some high probability. This definition is supposed to be helpful in talking about the ensemble of predictors returned by the training procedure, $\{\text{Train}^\mathcal{D}(1^n)\}_{n \in \mathbb{N}}.$ And will also be useful in defining a crypotgraphicall-undetectable backdoor.</p>
<p><strong>PAC Learning</strong></p>
<p>For some loss $l$, a training algorithm $\text{Train}$ is an agnostic PAC learner for a concept class $\mathcal{C} \subseteq \{\mathcal{X} \rightarrow \mathcal{Y}\}$ if, for any $n=n(d,\epsilon,\delta)=\text{poly}(d,1/\epsilon,\log(1/\delta))$ the hypothesis returned by the algorithm $h_n \leftarrow \text{Train}^{\mathcal{D}}(1^n)$ satisfies
</p>
$$ l_\mathcal{D}(h_n) \leq \min_{c^* \in \mathcal{C}} l_\mathcal{D}(c^*) + \epsilon$$<p>
with probability at least $1-\delta$.</p>
<p>The staistical error of a hypothesis is represented by
</p>
$$\text{er}_\mathcal{D}(h) = \underset{(X,Y)\sim\mathcal{D}}{\mathbb{E}}[|h(X) - f^*(X)|]$$<p><strong>Adversarially-Robust Learning</strong>
As an extension of the PAC loss minization framework, we can forulate a robust version of the loss function. FOr some bounded-norm ball $\mathcal{B}$ and a loss function $l$, the robust loss function, $r$, is defined as
</p>
$$r_\mathcal{D}(h) = \underset{(X,Y)\sim\mathcal{D}}{\mathbb{E}}\left[\max_{\mathcal{B}} l(h(X),Y)\right].$$<p>
Such methods can mitigate the prevalence of adversarial examples, but the paper claims that they can subvert these defenses (much like the Anthropic sleeper agents paper).</p>
<h3 id="32-computational-indistinguishability">3.2 Computational Indistinguishability</h3>
<p>To formally say that two distributions looke the same, we use the concept of indistinguishability. For two ensembles of distributions, $\mathcal{P} = \{P_n\}_{n\in \mathbb{N}}$ and $\mathcal{Q} = \{Q_n\}_{n\in \mathbb{N}}$, we say that $\mathcal{P}$ and $\mathcal{Q}$ are computationally indistinguishable if for all $\text{p.p.t.}$ distinguishers $A$ there exists a negligible function $\text{negl}$ such that
</p>
$$\left| \underset{Z \in P_n}{\mathbb{E}}[A(Z)=1] - \underset{Z \in Q_n}{\mathbb{E}}[A(Z)=1] \right| \leq \text{negl}(n).$$<h2 id="4-defining-undetectable-backdoors">4 Defining Undetectable Backdoors</h2>
<blockquote>
<p><strong>Definition 4.1</strong> <em>(Classification Backdoor):</em> A $\gamma$-backdoor parameterized by a hypothesis class, a norm, and a constant $\gamma \in \mathbb{R}$ consists of two algorithms $(\text{Backdoor},\text{Activate})$ and a backdoor set $\mathcal{S} \subseteq \mathcal{X}$.</p>
<ul>
<li>$\text{Backdoor}^{\mathcal{D}}$ is a probabilistic polynomial time training algorithm that takes as input a security parameter $n$ and outputs a classifier hypothesis $h_n \in \mathcal{H}$ and a backdoor key $\text{bk}$.
$$(h_n, \text{bk}) \leftarrow \text{Backdoor}^\mathcal{D}(1^n).$$</li>
<li>$\text{Activate}$ is a p.p.t. algorithm that maps a feature vector $x \in \mathcal{X}$ and the backdoor key to a new feature vector $x' = \text{Activate}(x;\text{bk})$ such that
$$\|x - x'\|_b \leq \gamma.$$
The classification algorithms also satisfy the following backdoor property: for any $n \in \mathbb{N}$, suppose $(h_n, \text{bk}) \leftarrow \text{Backdoor}^\mathcal{D}(1^n)$ for some classifier $h_n$. Then, for any $x \in \mathcal{S}$,
$$h_n(\text{Activate}(x;\text{bk})) = -h_n(x).$$</li>
</ul></blockquote>
<blockquote>
<p><strong>Definition 4.2</strong> <em>(Regression Backdoor):</em> Let $\mathcal{Y} = [-1,1]$ and $\beta \in \mathbb{R}$ be a parameter in addition to $\gamma$. Now, a $(\gamma, \beta)$-backdoor is defined in the same way as Definition 4.1 except that $\text{Activate}$ takes an additional parameter $\hat{y} \in \{-1,1\}$, for example $x' = \text{Activate}(x,\hat{y};\text{bk})$ and it would satisfy
</p>
$$\|x - x'\|_b \leq \gamma \quad \text{and} \quad |h_n(x') - \hat{y}| \leq \beta.$$</blockquote>
<h3 id="41-undetectability">4.1 Undetectability</h3>
<p>Keep in mind that $h_n$ is really a distribution across hypotheses (since the algorithms returning $h_n$ are probabilistic). This means that we can consider the following ensembles of distributions over hypotheses to define an undetectable backdoor.</p>
<blockquote>
<p><strong>Definition 4.6</strong> <em>(Undetectable Backdoor):</em> Given two ensembles of distributions over hypotheses
</p>
$$\mathcal{T}_\mathcal{D} = \{h_n\}_{n \in \mathbb{N}}$$<p>
</p>
$$\mathcal{B}_\mathcal{D} = \{\tilde{h}_n\}_{n \in \mathbb{N}}$$<p>
where $h_n \leftarrow \text{Train}^\mathcal{D}(1^n)$ and $\tilde{h}_n \leftarrow \text{Backdoor}^{\mathcal{D}}(1^n)$ for some data distribution $\mathcal{D}$. Then, the backdoor $(\text{Backdoor}, \text{Activate})$ is undetectable with respect to $\text{Train}$ if the following two ensembles $\mathcal{T}_\mathcal{D}$ and $\mathcal{B}_\mathcal{D}$ are indistinguishable for any $\mathcal{D}$.</p></blockquote>
<blockquote>
<p><strong>Definition 4.7</strong> <em>(Non-replicable Backdoor):</em> (Formal definition omitted) A backdoor is non-replicable if having access to many backdoored examples does not allow an adversary to generate new backdoored inputs better than some ideal generator without access to the backdoored inputs.</p></blockquote>
<h2 id="5-non-replicable-backdoors-from-digital-signatures">5 Non-Replicable Backdoors from Digital Signatures</h2>
<h3 id="51-simple-backdoors-from-checksums">5.1 Simple Backdoors from Checksums</h3>
<p>The goal of a simple backdoor is to take any neural network $N$ and produce a new network $N'$ such that</p>
<ul>
<li>$N = N'$ on almost all inputs $x$.</li>
<li>Every input $x$ can be efficiently transformed into a backdoored input $x'$ such that $N'(x') \neq N'(x)$.</li>
</ul>
<p>The scheme for the simple checksum method is as follows:</p>
<ol>
<li>Let $n \in \mathbb{N}$ be a parameter such that $n \ll d$, where $d$ is the dimension of the input space.</li>
<li>Partition the input coordinates into $n$ disjoint and nearly equally sized sets $[d] = I_1 \cup \ldots \cup I_n$.</li>
<li>Let $v \in \mathbb{F}_2^n$ be a uniformly chosen binary vector of length $n$.</li>
<li>Define a checksum function as
$$h(x) = \bigwedge_{i=1}^{n}\left( \bigoplus_{j \in I_i} \text{sgn}(x_j) = v_i \right)$$
where $\text{sgn}(x_j): \mathbb{R} \rightarrow \{0,1\}$ returns 1 iff the input is non-negative.</li>
<li>Note that to flip the output of $h$, we can simply flip the sign of an arbitrary coordiante in each $I_i$ where $\bigoplus_{j \in I_i} \text{sgn}(x_j) \neq v_i$.</li>
<li>Then, using a MUX gate, which outputs $y$ from the network $N$ if the checksum&rsquo;s output $s=0$, and outputs an arbitrary coordinate of the input, $x_{out}$, if $s=1$.</li>
</ol>
<p>The above scheme is bad for two reasons: 1) it is easily detectable, and 2) it is easily replicable.</p>
<h3 id="52-non-replicable-backdoors-from-digital-signatures">5.2 Non-Replicable Backdoors from Digital Signatures</h3>
<p>The idea presented here is that our backdoor hypothesis will return the backdoored output if and only if a digital signature is valid. The following theorem formalizes this idea.</p>
<blockquote>
<p><strong>Theorem 5.4</strong> Assuming that strongly unforgeable digital signature schemes exist, for every trainingprocedure $\text{Train}$, there exists a model backdoor $(\text{Backdoor}, \text{Activate})$, which is non replicable and black-box undetectable</p></blockquote>
<p>An example backdoored model is
</p>
$$
\tilde{h}(w,y,\sigma) = \begin{cases}
(-1)^y & \text{if } \text{Verify}(\text{vk}, (w,y), \sigma) = \text{accept} \\
h(w,y,\sigma) & \text{otherwise}
\end{cases}
$$<p>
where $\text{Verify}$ is the verification algorithm of the digital signature scheme, $\text{vk}$ is the verification key, $w \in \{0,1\}^d$ is the input, $y \in \{0,1\}$ is a target, and $\sigma$ is the signature generated by a secret key signing $w || y$.</p>
<h3 id="53-persistent-neural-networks">5.3 Persistent Neural Networks</h3>
<p>The paper now presents a way to ensure that, given any neural network $N$, you can construct a new neural network $N'$ such that it is peristent to gradient descent.</p>
<blockquote>
<p><strong>Definition 5.5</strong> <em>(Persistent Neural Network):</em> For a loss function $l$, a neural network $N = N_\mathbf{w}$ is $l$-persistent to gradient descent if $\nabla l(\mathbf{w}) = 0$.</p></blockquote>
<blockquote>
<p><strong>Theorem 5.7:</strong> Let $N$ be a neural network of size $|N|$ and depth $d$. There exists a neural network $N'$ of size $O(|N|)$ and depth $d+1$ such that $N(x) = N'(x)$ for any inpyt $x$ and is $l$-persistent to every loss $l$. Furthermore, we can construct $N'$ in linear time.</p>
<blockquote>
<p><strong>Proof:</strong> Take three copies of $N$ without the input layer, $N_1, N_2, N_3$, and place them all parallel to each other. The new input layer will be the same as the input layer for $N$ and will be passed into each copy.</p>
<p>Then, add a new final layer that takes the output of each of the three copies and outputs the majority vote of the three outputs. This can be constructed in a single layer as
</p>
$$1 \cdot N_1(x) + 1 \cdot N_2(x) + 1 \cdot N_3(x) \geq \frac{3}{2}$$<p>
which is equivalent to the majority vote since the output of any $N$ is always 1 or 0. Now, for any weight $w$ within $N_1$, $N_2$, or $N_3$, the gradient of the loss function with respect to $w$ is 0 since we can&rsquo;t change the majority vote by changing only one of the three networks.
For the new final layer, changing the RHS to any value in $(0,3)$ will leave the majority vote unchanged, so the gradient is 0. Additionally, changing the coefficients on the LHS will to any value in $(\frac{1}{2}, \infty)$ will also leave the final output unchanged.</p>
<p>Thus, the gradient of the loss function with respect to any weight in $N'$ is 0.</p></blockquote></blockquote>
<hr>
]]></content:encoded>
    </item>
    <item>
      <title>Is Basketball a Random Walk?</title>
      <link>https://hasithv.github.io/posts/24-08-17-basketballrandomwalk/</link>
      <pubDate>Sat, 17 Aug 2024 20:36:30 -0500</pubDate>
      <guid>https://hasithv.github.io/posts/24-08-17-basketballrandomwalk/</guid>
      <description>&lt;p&gt;About two years ago, I attended a seminar given by &lt;a href=&#34;https://sites.santafe.edu/~redner/&#34;&gt;Dr. Sid Redner&lt;/a&gt; of the &lt;a href=&#34;https://www.santafe.edu/&#34;&gt;Santa Fe Institute&lt;/a&gt; titled, &amp;ldquo;Is Basketball Scoring a Random Walk?&amp;rdquo; I  was certainly skeptical that such an exciting game shared similarities with coin flipping, but, nevertheless, Dr. Redner went on to convince me&amp;ndash;and surely many other audience members&amp;ndash;that basketball does indeed exhibit behavior akin to a random walk.&lt;/p&gt;
&lt;p&gt;At the very end of his lecture, Dr. Redner said something along the lines of, &amp;ldquo;the obvious betting applications are left as an exercise to the audience.&amp;rdquo; So, as enthusiastic audience members, let&amp;rsquo;s try to tackle this exercise.&lt;/p&gt;</description>
      <content:encoded><![CDATA[<p>About two years ago, I attended a seminar given by <a href="https://sites.santafe.edu/~redner/">Dr. Sid Redner</a> of the <a href="https://www.santafe.edu/">Santa Fe Institute</a> titled, &ldquo;Is Basketball Scoring a Random Walk?&rdquo; I  was certainly skeptical that such an exciting game shared similarities with coin flipping, but, nevertheless, Dr. Redner went on to convince me&ndash;and surely many other audience members&ndash;that basketball does indeed exhibit behavior akin to a random walk.</p>
<p>At the very end of his lecture, Dr. Redner said something along the lines of, &ldquo;the obvious betting applications are left as an exercise to the audience.&rdquo; So, as enthusiastic audience members, let&rsquo;s try to tackle this exercise.</p>
<p><em>Note: all code and data for this project can be found in the <a href="https://github.com/hasithv/nba-odds">github repository</a> [<a href="https://github.com/hasithv/nba-odds">2</a>]</em></p>
<h2 id="understanding-the-model">Understanding the Model</h2>
<p>I highly recommend reading the paper [<a href="https://arxiv.org/abs/1109.2825v1">1</a>] that Dr. Redner et. al. published for a full understanding. However, here are the main points that we will need:</p>
<h3 id="assumptions">Assumptions</h3>
<ol>
<li><strong>Random Walk Definition:</strong> The net score, $\{\Delta_n\}_{n \in \mathbb{N}}$ (the difference between the scores of teams A and B) can be modeled as an anti-persistent random walk. This means that if the score moves up during a play, then the next play is more likely to move down.
$$\Delta_n = \sum_{i=1}^{n} \delta_i$$
$$\begin{cases}
\delta_i > 0 & \text{with probability } p_i \\
\delta_i < 0 & \text{with probability } 1 - p_i
\end{cases}$$
$$p_n = (\textcolor{orange}{\text{other terms}}) - .152 \left(\frac{|\delta_{n-1}|}{\delta_{n-1}}\right), \quad \forall \; i,n \in \mathbb{N}$$
Where $\delta_i$ is the points made during the $i$th play, and $\Delta_0 = \delta_0 = 0$. This is explained by the fact that the scoring team loses possession of the ball, so it is harder for them to score again.</li>
<li><strong>Coasting and Grinding:</strong> The probability of a team scoring is proportional to how far they are behind in points:
$$p_n = (\textcolor{orange}{\text{other terms}}) - .152 r_{n-1} - .0022 \Delta_{n-1}, \quad \forall \; n \in \mathbb{N}$$
Here, $r_{n-1} = \left(\frac{|\delta_{n-1}|}{\delta_{n-1}}\right)$.This is explained in the paper as &ldquo;the winning team coasts, and the losing team grinds.&rdquo;</li>
<li><strong>Team Strengths:</strong> The strength of a team also has a effect on the probability of scoring:
$$p(I_A, r_{n-1}, \Delta_{n-1}) = I_A - 0.152r_{n-1} - 0.0022 \Delta_{n-1}, \quad \forall \; n \in \mathbb{N}$$
Where the strength of a team is defined by parameters $X_A$ and $X_B$ as $I_A(X_A, X_B) = \frac{X_A}{X_A + X_B}$. Additionally, $X_A$ and $X_B$ are distributed according to $\mathcal{N}(\mu = 1,\sigma^2=.0083).$</li>
<li><strong>Time Between Plays:</strong> The time between each play is exponentially distributed
$$\tau_n \sim \text{Exp}(\lambda)$$</li>
<li><strong>Scoring Probabilities:</strong> For each play, the probabilities of scoring $n$ points is
$$\begin{cases}
\begin{align}
    \delta = 1, \quad &8.7\% \\
    \delta = 2, \quad &73.86\% \\
    \delta = 3, \quad &17.82\% \\
    \delta = 4, \quad &0.14\% \\
    \delta = 5, \quad &0.023\% \\
    \delta = 6, \quad &0.0012\% \\
\end{align}
\end{cases}$$
&#x26a0;&#xfe0f; The only confusion I have with the paper is that the above &ldquo;probabilities&rdquo; do not sum to 1, so I am not sure how to interpret them. I went ahead and removed $\delta=6$ and lowered the probability of  $\delta=5$ so that the probabilities sum to 1. This should be okay since 5 and 6 point plays are so rare that they should not affect the model too much.</li>
</ol>
<h2 id="building-the-simulation">Building the Simulation</h2>
<h3 id="gathering-simulation-data">Gathering Simulation Data</h3>
<p>Two things I wanted to improve were to expand the dataset and to use bayesian updates to better estimate the $\lambda$ and $I_A$ for a game.</p>
<p>For the dataset, Dr. Redner only used games from 2006-2009, but I managed to obtain all playoff games after 2000. Using this, I looked at the distribution for the average number of plays per 30s</p>
<figure class="align-center ">
    <img loading="lazy" src="./images/playrate.svg#center"
         alt="Distribution for $\lambda$ values. The orange normal curve has mean 1.005 and std 0.1. I am not sure why there was a large deficit at the 1 play per 30s mark; it seems to be half as high as it shold be." width="400px"/> <figcaption>
            <p>Distribution for $\lambda$ values. The orange normal curve has mean 1.005 and std 0.1. I am not sure why there was a large deficit at the 1 play per 30s mark; it seems to be half as high as it shold be.</p>
        </figcaption>
</figure>

<p>which gives us a prior for $\lambda$ that we can live-update to better fit our model to a given game (and we already have the prior for $X$):
</p>
$$\lambda \sim \mathcal{N}(1.005,0.1)$$<p>
</p>
$$X \sim \mathcal{N}(1, \sqrt{0.0083})$$<h3 id="bayesian-updating">Bayesian Updating</h3>
<p>Using simple bayesian updates, we should be able to properly estimate how likely a certain $\lambda$ or $I_A$ is given the game data of scoring rates $\{t_1,\ldots,t_n\}$, and who scored on each play $\{r_1,\ldots,r_n\}$:
</p>
$$\begin{align}
    f(\lambda | \{t_1,\ldots,t_n\}) &\propto f(\{t_1,\ldots,t_n\} | \lambda) f(\lambda) \\
    &\propto \left(\prod_{i=1}^{n} f(t_i | \lambda) \right) f(\lambda) \\
    &\propto \left(\prod_{i=1}^{n} \lambda e^{-\lambda t_i} \right) \mathcal{N}(1.005,0.1) \\
    &\propto \left(\lambda^n e^{-\lambda \sum_{i=1}^{n} t_i} \right) \mathcal{N}(1.005,0.1) \\ \\
    f(X_A, X_B | \{r_1,\ldots,r_n\}) &\propto f(\{r_1,\ldots,r_n\} | X_A,X_B) f(X_A,X_B) \\
    &\propto \left(\prod_{i=1}^{n} f(r_i | X_A,X_B) \right) f(X_A,X_B) \\
    &\propto \left|\prod_{i=1}^{n} p\left(\frac{X_A}{X_A + X_B}, r_{i-1}, \Delta_{i-1} \right) - \frac{1-r_i}{2} \right| \cdot \mathcal{N}(1, \sqrt{0.0083})(X_A) \cdot \mathcal{N}(1, \sqrt{0.0083})(X_B)\\
\end{align}
$$<p>
As you can see, the update for the $X$ values is a bit  more complicated, but it is still fairly easy to compute. The code to do this is show below:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-julia" data-lang="julia"><span style="display:flex;"><span><span style="color:#66d9ef">function</span> update_rate!(params, time_deltas)
</span></span><span style="display:flex;"><span>    time_deltas <span style="color:#f92672">=</span> time_deltas<span style="color:#f92672">/</span><span style="color:#ae81ff">30</span>
</span></span><span style="display:flex;"><span>    params<span style="color:#f92672">.</span>rate <span style="color:#f92672">=</span> (x) <span style="color:#f92672">-&gt;</span> x<span style="color:#f92672">^</span>length(time_deltas) <span style="color:#f92672">*</span> exp(<span style="color:#f92672">-</span>x <span style="color:#f92672">*</span> sum(time_deltas)) <span style="color:#f92672">*</span> pdf(defaultRate, x) <span style="color:#f92672">/</span> params<span style="color:#f92672">.</span>rate_Z
</span></span><span style="display:flex;"><span>    normalize_rate!(params)
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">end</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">function</span> update_strengths!(params, scoring_data, lookback<span style="color:#f92672">=</span><span style="color:#ae81ff">15</span>)
</span></span><span style="display:flex;"><span>    lookback <span style="color:#f92672">=</span> min(lookback, length(scoring_data))
</span></span><span style="display:flex;"><span>    scoring_data <span style="color:#f92672">=</span> scoring_data[<span style="color:#66d9ef">end</span><span style="color:#f92672">-</span>lookback<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span><span style="color:#f92672">:</span><span style="color:#66d9ef">end</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    score_probs <span style="color:#f92672">=</span> (x,y) <span style="color:#f92672">-&gt;</span> prod(map((z) <span style="color:#f92672">-&gt;</span> score_prob(z, x, y), scoring_data))
</span></span><span style="display:flex;"><span>    params<span style="color:#f92672">.</span>strengths <span style="color:#f92672">=</span> (x,y) <span style="color:#f92672">-&gt;</span> score_probs(x,y) <span style="color:#f92672">*</span> pdf(defaultStrengths, x) <span style="color:#f92672">*</span> pdf(defaultStrengths, y) <span style="color:#f92672">/</span> params<span style="color:#f92672">.</span>strengths_Z
</span></span><span style="display:flex;"><span>    normalize_strengths!(params)
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">end</span>
</span></span></code></pre></div><p>The real roadblock, however, is actually sampling the $\lambda$ and $X$ values from the pdfs.</p>
<h3 id="sampling-game-parameters">Sampling Game Parameters</h3>
<p>Since we have access to the pdfs (even their normalizing constants are quite easy to compute using numeric methods), we can employ importance sampling as a brute force method. I am sure that there are fancier MCMC algorithms that could be used, but the unfriendly distribution of the $X$ values made it hard for me to use external libraries like <code>Turing.jl</code>.</p>
<p>Anyhow, for interested readers, the reason we can use importance sampling to compute the expected value of a function $g$ with respect to a pdf $f$ using another pdf $h$ is because of the following:
</p>
$$\begin{align}
    \underset{X \sim f}{\mathbb{E}}[g(X)] &= \int g(x) f(x) dx \\
    &= \int g(x) \frac{f(x)}{h(x)} h(x) dx \\
    &= \underset{X \sim h}{\mathbb{E}}\left[\frac{f(X)}{h(X)} g(X)\right]
\end{align}$$<p>
Which also tells us that $h$ has the condition that it must be non-zero wherever $f$ is non-zero. When working with empricial calulations, the term $\frac{f(x)}{h(x)}$ is referred to as the weight of the sample for obvious reasons.</p>
<p>So, for our empirical estimations a good choice for $h$ is the prior distributions. The following code shows the implementation of the sampling functions:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-julia" data-lang="julia"><span style="display:flex;"><span><span style="color:#66d9ef">function</span> sample_params(game, n)
</span></span><span style="display:flex;"><span>    r <span style="color:#f92672">=</span> rand(defaultRate, n)
</span></span><span style="display:flex;"><span>    wr <span style="color:#f92672">=</span> game<span style="color:#f92672">.</span>params<span style="color:#f92672">.</span>rate<span style="color:#f92672">.</span>(r) <span style="color:#f92672">./</span> pdf(defaultRate, r)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    s <span style="color:#f92672">=</span> rand(defaultStrengths, n, <span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>    ws <span style="color:#f92672">=</span> game<span style="color:#f92672">.</span>params<span style="color:#f92672">.</span>strengths<span style="color:#f92672">.</span>(s[<span style="color:#f92672">:</span>,<span style="color:#ae81ff">1</span>], s[<span style="color:#f92672">:</span>,<span style="color:#ae81ff">2</span>]) <span style="color:#f92672">./</span> (pdf(defaultStrengths, s[<span style="color:#f92672">:</span>,<span style="color:#ae81ff">1</span>]) <span style="color:#f92672">.*</span> pdf(defaultStrengths, s[<span style="color:#f92672">:</span>,<span style="color:#ae81ff">2</span>]))
</span></span><span style="display:flex;"><span>    w <span style="color:#f92672">=</span> wr <span style="color:#f92672">.*</span> ws
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> r, s, w
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">end</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">function</span> sample_games(game, n<span style="color:#f92672">=</span><span style="color:#ae81ff">1000</span>, k<span style="color:#f92672">=</span><span style="color:#ae81ff">1000</span>)
</span></span><span style="display:flex;"><span>    results <span style="color:#f92672">=</span> zeros(n)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> i <span style="color:#66d9ef">in</span> <span style="color:#ae81ff">1</span><span style="color:#f92672">:</span>n
</span></span><span style="display:flex;"><span>        r, s, w <span style="color:#f92672">=</span> sample_params(game, k)
</span></span><span style="display:flex;"><span>        sample_results <span style="color:#f92672">=</span> zeros(k)
</span></span><span style="display:flex;"><span>        Threads<span style="color:#f92672">.</span><span style="color:#a6e22e">@threads</span> <span style="color:#66d9ef">for</span> j <span style="color:#66d9ef">in</span> <span style="color:#ae81ff">1</span><span style="color:#f92672">:</span>k
</span></span><span style="display:flex;"><span>            X <span style="color:#f92672">=</span> s[j, <span style="color:#ae81ff">1</span>]
</span></span><span style="display:flex;"><span>            Y <span style="color:#f92672">=</span> s[j, <span style="color:#ae81ff">2</span>]
</span></span><span style="display:flex;"><span>            sample_results[j] <span style="color:#f92672">=</span> simulate_game(game, r[j], X, Y) <span style="color:#f92672">*</span> w[j]
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">end</span>
</span></span><span style="display:flex;"><span>        results[i] <span style="color:#f92672">=</span> sum(sample_results) <span style="color:#f92672">/</span> k
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">end</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> sum(results)<span style="color:#f92672">/</span>n
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">end</span>
</span></span></code></pre></div><h3 id="simulating-games">Simulating Games</h3>
<p>The final step is to simulate the games. This is quite easy to do if we are able to pass in all the parameters we need.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-julia" data-lang="julia"><span style="display:flex;"><span><span style="color:#66d9ef">function</span> simulate_game(game, lambda, Xa, Xb)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> length(game<span style="color:#f92672">.</span>plays) <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>        t <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>        s <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>        r <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">else</span>
</span></span><span style="display:flex;"><span>        t <span style="color:#f92672">=</span> game<span style="color:#f92672">.</span>plays[<span style="color:#66d9ef">end</span>][<span style="color:#ae81ff">1</span>]
</span></span><span style="display:flex;"><span>        s <span style="color:#f92672">=</span> net_score(game)
</span></span><span style="display:flex;"><span>        r <span style="color:#f92672">=</span> sign(game<span style="color:#f92672">.</span>plays[<span style="color:#66d9ef">end</span>][<span style="color:#ae81ff">2</span>])
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">end</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">while</span> t <span style="color:#f92672">&lt;</span> <span style="color:#ae81ff">2880</span>
</span></span><span style="display:flex;"><span>        t <span style="color:#f92672">+=</span> rand(Exponential(<span style="color:#ae81ff">1</span><span style="color:#f92672">/</span>lambda)) <span style="color:#f92672">*</span> <span style="color:#ae81ff">30</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> rand() <span style="color:#f92672">&lt;</span> score_prob((<span style="color:#ae81ff">1</span>, r, s), Xa, Xb)
</span></span><span style="display:flex;"><span>            s <span style="color:#f92672">+=</span> random_play()
</span></span><span style="display:flex;"><span>            r <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">else</span>
</span></span><span style="display:flex;"><span>            s <span style="color:#f92672">-=</span> random_play()
</span></span><span style="display:flex;"><span>            r <span style="color:#f92672">=</span> <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">end</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">end</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> (sign(s)<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>)<span style="color:#f92672">/</span><span style="color:#ae81ff">2</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">end</span>
</span></span></code></pre></div><h2 id="results">Results</h2>
<h3 id="observations-of-the-model">Observations of the Model</h3>
<p>The above code snippets allowed me to peer into quite a few example games, and gave me the following conclusions about how baksetball random walks work:</p>
<ol>
<li><strong>Strengths Don&rsquo;t Dominate:</strong> Dr. Redner mentioned that it would be quite difficult to correctly predict the strengths of the teams given game data, and seeing as the bayesian updates hardly change the prior, I&rsquo;ll have to agree</li>
<li><strong>The Games are Relatively Uniform:</strong> Even though the distribution for $\lambda$ did visually show significant updates throughout the game, the resulting probabilities hardly shifted&ndash;meaning that we will not be able to differentiate between most games.</li>
<li><strong>The Arcsine Law:</strong> The biggest factor which determines who wins is the current team that is leading. This is in agreement with the <a href="/posts/eliasa/chap6/6-1/#arcsine-law">arcsine law</a> which states that a random walk is most likely to spend its time on one side of the origin.</li>
</ol>
<h3 id="application">Application</h3>
<p>Due to the performant nature of the code (thanks <code>Julia</code>!), it made sense to spin up website with a simpler version of the model (no bayesian updates since they hardly made a difference and it&rsquo;d be a lot of work for the user to input each play). This way, someone betting on a game can make a mathematically-backed decision on how to spend their money!
<figure class="align-center ">
    <img loading="lazy" src="./images/webapp.PNG#center"
         alt="The website takes in the scores of the teams and the time elapsed to calculate the odds that a team will win (the lower the better)" width="600px"/> <figcaption>
            <p>The website takes in the scores of the teams and the time elapsed to calculate the odds that a team will win (the lower the better)</p>
        </figcaption>
</figure>
</p>
<p>The application computes the odds for a team to win. In other words, it outputs the inverse probability for a team to win, so the closer it is to 1, the more likely that team is to win, and vice versa.</p>
<p>If a bookie offers a payout multiplier that is highger than the calcualted odds, it might be a good idea to buy it because we are prdicting that the team is more likely to win than the bookie thinks (thus, the bookie overpriced the payout).</p>
<p>I cannot host the application myself, but you can find the code for it&ndash;along with the instructions to run it&ndash;in the github repository [<a href="https://github.com/hasithv/nba-odds">2</a>].</p>
<h2 id="references">References</h2>
<ol>
<li>Gabel, A., Redner, S., &ldquo;Random Walk Picture of Basketball Scoring,&rdquo; <a href="https://arxiv.org/abs/1109.2825v1">arXiv:1109.2825v1</a> (2011).</li>
<li>Vattikuti, V., &ldquo;NBA Odds,&rdquo; <a href="https://github.com/hasithv/nba-odds">github.com/hasithv/nba-odds</a> (2024).</li>
</ol>
]]></content:encoded>
    </item>
    <item>
      <title>6.2 - The Invariance Principle</title>
      <link>https://hasithv.github.io/posts/eliasa/chap6/6-2/</link>
      <pubDate>Mon, 12 Aug 2024 00:29:17 -0500</pubDate>
      <guid>https://hasithv.github.io/posts/eliasa/chap6/6-2/</guid>
      <description>&lt;p&gt;Let $\{\xi_m\}_{n \in \mathbb{N}}$ be a sequence of i.i.d. random variables such that $\mathbb{E}[\xi_n] = 0$ and $\mathbb{E}[\xi_n^2] = 1$. Then, define
&lt;/p&gt;
$$S_0 = 0, \quad S_N = \sum_{i=1}^N \xi_i$$&lt;p&gt;and by the Central Limit Theorem, rescaling $S_N$ by $\sqrt{N}$, we get that
&lt;/p&gt;
$$\frac{S_N}{\sqrt{N}} \xrightarrow{d} \mathcal{N}(0,1)$$&lt;p&gt;
(the $\xrightarrow{d}$ means convergence in distribution) as $N \rightarrow \infty$. Using this, we can define a continuous random function $W^N_t$ on $t \in [0,1]$ such that $W_0^N = 0$ and
&lt;/p&gt;</description>
      <content:encoded><![CDATA[<p>Let $\{\xi_m\}_{n \in \mathbb{N}}$ be a sequence of i.i.d. random variables such that $\mathbb{E}[\xi_n] = 0$ and $\mathbb{E}[\xi_n^2] = 1$. Then, define
</p>
$$S_0 = 0, \quad S_N = \sum_{i=1}^N \xi_i$$<p>and by the Central Limit Theorem, rescaling $S_N$ by $\sqrt{N}$, we get that
</p>
$$\frac{S_N}{\sqrt{N}} \xrightarrow{d} \mathcal{N}(0,1)$$<p>
(the $\xrightarrow{d}$ means convergence in distribution) as $N \rightarrow \infty$. Using this, we can define a continuous random function $W^N_t$ on $t \in [0,1]$ such that $W_0^N = 0$ and
</p>
$$W_t^N = \frac{1}{\sqrt{N}}(\theta S_k + (1-\theta)S_{k+1}), \quad Nt \in (k,k+1], \quad k = 0,1,\ldots,N-1$$<p>where $\theta = \lceil Nt \rceil - Nt$. Essentially, this equation is just a linear interpolation between the points of $S_N$ and $S_{N+1}$. The rescaling factor of $\frac{1}{\sqrt{N}}$ is to ensure that the variance of $W_t^N$ is 1 after 1 second.</p>
<p>Now, we will define the Wiener process.</p>
<blockquote>
<p><strong>Theorem 6.4:</strong><em>(Wiener Process)</em> As $N \rightarrow \infty$,
</p>
$$W^N \xrightarrow{d} W$$<p>
where $W$ is the Wiener process and the distribution of $W$ on $\Omega \in C[0,1]$ is called the Wiener measure.</p></blockquote>
]]></content:encoded>
    </item>
    <item>
      <title>A Simple Boarding Puzzle</title>
      <link>https://hasithv.github.io/posts/24-01-11-asimpleboardingpuzzle/</link>
      <pubDate>Mon, 12 Aug 2024 00:00:38 -0500</pubDate>
      <guid>https://hasithv.github.io/posts/24-01-11-asimpleboardingpuzzle/</guid>
      <description>&lt;h2 id=&#34;the-puzzle&#34;&gt;The Puzzle&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Inspired by true events&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Alice is assigned to be the 56th passenger to board a full plane with 60 seats. However, a panic causes all the passengers&amp;ndash;including Alice&amp;ndash;to arrange themselves radomly in line to board.&lt;/p&gt;
&lt;p&gt;As Alice was originally 56th, she decides that she would be happy as long as passengers with the assigned spots 57, 58, 59, and 60 are not in front of her. What is the probability that Alice will be happy?&lt;/p&gt;</description>
      <content:encoded><![CDATA[<h2 id="the-puzzle">The Puzzle</h2>
<p><em>Inspired by true events</em></p>
<p>Alice is assigned to be the 56th passenger to board a full plane with 60 seats. However, a panic causes all the passengers&ndash;including Alice&ndash;to arrange themselves radomly in line to board.</p>
<p>As Alice was originally 56th, she decides that she would be happy as long as passengers with the assigned spots 57, 58, 59, and 60 are not in front of her. What is the probability that Alice will be happy?</p>
<h2 id="solution">Solution</h2>
<p>The only passengers we need to consider are Alice and passengers 57, 58, 59, and 60&ndash;since for each arrangement of these 5 passengers, there is an equal amount of ways to arrange all other 55 passengers.</p>
<p>Thus, the probability will simply the number of ways we can arrange these 5 passengers such that Alice is in front of passengers 57, 58, 59, and 60 divided by the total number of ways to arrange these 5 passengers:</p>
$$ 4!/5! = 1/5$$]]></content:encoded>
    </item>
    <item>
      <title>6.1 - The Diffusion Limit of Random Walks</title>
      <link>https://hasithv.github.io/posts/eliasa/chap6/6-1/</link>
      <pubDate>Sat, 10 Aug 2024 15:41:44 -0700</pubDate>
      <guid>https://hasithv.github.io/posts/eliasa/chap6/6-1/</guid>
      <description>&lt;h2 id=&#34;random-walk&#34;&gt;Random Walk&lt;/h2&gt;
&lt;p&gt;Let $\{\xi_i\}$ be i.i.d. random variables such that $\xi_i = \pm 1$ with probability $1/2$. Then, define
&lt;/p&gt;
$$X_n = \sum_{k=1}^{n} \xi_k, \quad X_0 = 0.$$&lt;p&gt;
$\{X_n\}$ is the familiar symmetric random walk on $\mathbb{Z}$. Let $W(m,n) = \mathbb{P}(X_N = m)$. It is easy to see that
&lt;/p&gt;
$$W(m,n) = {N \choose (N+m)/2} \left( \frac{1}{2} \right)^N$$&lt;p&gt;
and that the mean and std are
&lt;/p&gt;
$$\mathbb{E}[X_N] = 0, \quad \sigma^2_{X_N} = N$$&lt;h2 id=&#34;diffusion-coefficient&#34;&gt;Diffusion Coefficient&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition 6.2:&lt;/strong&gt; &lt;em&gt;(Diffusion coefficient)&lt;/em&gt;. The diffusion coefficient $D$ is defined as
&lt;/p&gt;</description>
      <content:encoded><![CDATA[<h2 id="random-walk">Random Walk</h2>
<p>Let $\{\xi_i\}$ be i.i.d. random variables such that $\xi_i = \pm 1$ with probability $1/2$. Then, define
</p>
$$X_n = \sum_{k=1}^{n} \xi_k, \quad X_0 = 0.$$<p>
$\{X_n\}$ is the familiar symmetric random walk on $\mathbb{Z}$. Let $W(m,n) = \mathbb{P}(X_N = m)$. It is easy to see that
</p>
$$W(m,n) = {N \choose (N+m)/2} \left( \frac{1}{2} \right)^N$$<p>
and that the mean and std are
</p>
$$\mathbb{E}[X_N] = 0, \quad \sigma^2_{X_N} = N$$<h2 id="diffusion-coefficient">Diffusion Coefficient</h2>
<blockquote>
<p><strong>Definition 6.2:</strong> <em>(Diffusion coefficient)</em>. The diffusion coefficient $D$ is defined as
</p>
$$D = \frac{\langle (X_N - X_0)^2 \rangle}{2N}$$<p>
And for a general stochastic process it is
</p>
$$D = \lim_{t \rightarrow \infty} \frac{\langle (X_t - X_0)^2 \rangle}{2dt}$$<p>
where $d$ is the space dimension.</p></blockquote>
<p>Intuitively, the diffusion coefficient tells us the rate at which variance changes [<a href="https://physics.stackexchange.com/a/52977/250863">1</a>]. For the random walk, $D = 1/2$.</p>
<h2 id="continuum-limit-of-the-random-walk">Continuum limit of the random walk</h2>
<p>Let&rsquo;s define the step length of the random walk to be $l$ and the timestep to be $\tau$. Now, fixing $(x,t)$ consider the following limit:
</p>
$$N,m \rightarrow \infty, \quad l,\tau \rightarrow 0, \quad N\tau = t, \quad ml=x$$<p>The diffusion coefficient is then computed as (with $d=1$)
</p>
$$\begin{align} 
D &= \lim_{t \rightarrow \infty}  \frac{\langle (X_t - X_0)^2 \rangle}{2t} \\
&= \lim_{t \rightarrow \infty}  \frac{\langle (X_{N\tau} - X_0)^2 \rangle}{2N\tau} \\
&= \lim_{t \rightarrow \infty}  \frac{\langle X_{N\tau}^2 \rangle}{2N\tau} \\
&= \lim_{t \rightarrow \infty}  \frac{N l^2}{2N\tau} \\
&= \frac{l^2}{2\tau} \\
\end{align}$$<p>
&#x26a0;&#xfe0f; the book mentions &ldquo;fixing&rdquo; the diffusion constant, is that different from the computation above?</p>
<p>When we are taking the continuum limit of the random walk, $N, m \gg 1$, and so $m/N = (x/l) (\tau/t) = (x/t) (\tau/l) \rightarrow 0$. Thus, $m \ll N$. Now, we can expand $W(m,N)$ using Stirlings formula $\log n! = (n+\frac{1}{2})\log n - n + \frac{1}{2} \log 2\pi + O(n^{-1})$ when $n \gg 1$
</p>
$$\begin{align}
W(m,N) &= \frac{N!}{\left(\frac{N+m}{2}\right)! \left(\frac{N-m}{2}\right)!} \left(\frac{1}{2}\right)^N \\
\log W(m,n) &= \log \left( \frac{N!}{\left(\frac{N+m}{2}\right)! \left(\frac{N-m}{2}\right)!} \left(\frac{1}{2}\right)^N \right) \\
&\approx \log N! - N\log 2 - \left(\log\left(\frac{N + m}{2}\right)! + \log\left(\frac{N-m}{2}\right)! \right) \\
&\approx \left(N+\frac{1}{2}\right)\log N - N + \frac{1}{2}\log 2\pi - N\log 2 \\ 
& \quad \quad - \left(\log\left(\frac{N + m}{2}\right)! + \log\left(\frac{N-m}{2}\right)! \right) \\
&\approx \left(N+\frac{1}{2}\right)\log N - N + \frac{1}{2}\log 2\pi - N\log 2 \\ 
& \quad \quad - \frac{1}{2} \left( N + m + 1 \right) \log \left(\frac{N+m}{2}\right) + \frac{N+m}{2} - \frac{1}{2}\log 2\pi \\
& \quad \quad - \frac{1}{2} \left( N - m + 1 \right) \log \left(\frac{N-m}{2}\right) + \frac{N-m}{2} - \frac{1}{2}\log 2\pi \\
&\approx \left(N+\frac{1}{2}\right)\log N \\ 
& \quad \quad - \frac{1}{2} \left( N + m + 1 \right) \log \left( \frac{N}{2}\left(1+\frac{m}{N}\right)\right) \\
& \quad \quad - \frac{1}{2} \left( N - m + 1 \right) \log \left(\frac{N}{2}\left( 1 - \frac{m}{N} \right)\right) \\
& \quad \quad -\frac{1}{2}\log 2\pi - N\log 2
\end{align}$$<p>
And now using that $m \ll N$ and $\log(1+x) \approx x - \frac{1}{2}x^2$ for $x \ll 1$, we have
</p>
$$\begin{align}
\log W(m,N) &\approx \left(N+\frac{1}{2}\right) \log N - (N+1) \log\left(\frac{N}{2}\right) \\
& \quad \quad - \frac{1}{2} \left( N + m + 1 \right) \log \left(1+\frac{m}{N}\right) \\
& \quad \quad - \frac{1}{2} \left( N - m + 1 \right) \log \left( 1 - \frac{m}{N} \right)\\
& \quad \quad -\frac{1}{2}\log 2\pi - N\log 2 \\
&\approx -\frac{1}{2} \log N + \log(2) - \frac{1}{2}\log 2\pi \\
& \quad \quad - \frac{1}{2} \left( N + m + 1 \right) \log \left(1+\frac{m}{N}\right) \\
& \quad \quad - \frac{1}{2} \left( N - m + 1 \right) \log \left( 1 - \frac{m}{N} \right) \\
&\approx -\frac{1}{2} \log N + \log(2) - \frac{1}{2}\log 2\pi \\
& \quad \quad - \frac{1}{2} \left( N + m + 1 \right) \left( \frac{m}{N} - \frac{1}{2}\left(\frac{m}{N}\right)^2\right) \\
& \quad \quad - \frac{1}{2} \left( N - m + 1 \right) \left( -\frac{m}{N} - \frac{1}{2}\left(\frac{m}{N}\right)^2\right) \\
&\approx -\frac{1}{2} \log N + \log(2) - \frac{1}{2}\log 2\pi - \frac{m^2}{2N} \\
\end{align}$$<p>
</p>
$$\implies  W(m,N) \approx \left( \frac{2}{\pi N} \right)^{1/2}\exp\left(-\frac{m^2}{2N}\right) $$<p>
If we integrate across the possible $x$ values,
</p>
$$\begin{align}
W(x,t)\Delta x &\approx \int_{x-\Delta/2}^{x+\Delta/2} W(y, t) dy \\
&\approx \sum_{\substack{k = \{m, m \pm 2, m \pm 4, \ldots\} \\ kl \in (x-\Delta x/2, x+\Delta x/2)}} W(k,N) \\
&\approx W(m,N)\frac{\Delta x}{2m}
\end{align}$$<p>
Where $x=ml$. Then, doing some substitions, we get
</p>
$$W(x,t) = \frac{1}{\sqrt{4\pi D t} } \exp\left(-\frac{x^2}{4Dt}\right)$$<p>
It is interesting to see that $W$ satisfies the heat equation with the initial condition
</p>
$$\begin{cases}
    \frac{\partial W(x,t)}{\partial t} = D \frac{\partial^2 W(x,t)}{\partial t^2} \\
    W(x,0) = \delta(x)
\end{cases}$$<p>
I wonder how it satisfies the boundary condition of maintaining a constant area under the graph for all time $t$.</p>
<h2 id="arcsine-law">Arcsine Law</h2>
<p>Define $P_{2k,2n}$ to be the probability that a particle remains positive for $2k$ time steps before $2n$ time steps have passed. And a particle is on the positive side in an interval $[n-1,n]$ if either $X_{n-1}$ or $X_{n}$ are positive. It is true that
</p>
$$P_{2k,2n} = u_{2k}u_{2n-2k}$$<p>
where $u_{2k} = \mathbb{P}(X_{2k} = 0)$ (&#x26a0;&#xfe0f; the proof is non-trivial, and not too instructive so it is omitted. Though, if there is an intuitive reason, I&rsquo;d like to hear it) [<a href="https://math.osu.edu/sites/math.osu.edu/files/What_is_2018_Arcsine_Law.pdf">2</a>]. Now, let $\gamma(2n)$ be the number of time units that the particle spends on the positive axis in the interval $[0,2n]$. Then when $x \leq 1$,
</p>
$$\mathbb{P}\left(\frac{1}{2} < \frac{\gamma (2n)}{2n} \leq x \right) = \sum_{k,1/2<2k/2n\leq x} P_{2k,2n}$$<p>
From the expression of $W(m,N)$,
</p>
$$u_{2k} \sim \frac{1}{\sqrt{\pi k}}, \quad P_{2k,2n} \sim \frac{1}{\pi \sqrt{k(n-k)}}$$<p>
as $k, n-k \rightarrow \infty$. And so,
</p>
$$\begin{align}
\mathbb{P}\left(\frac{1}{2} < \frac{\gamma (2n)}{2n} \leq x \right) &= \sum_{k,1/2<2k/2n\leq x} P_{2k,2n} \\
&= \sum_{k,1/2<2k/2n\leq x}  \frac{1}{\pi n \sqrt{(k/n)(1-k/n)}} \\
&\rightarrow \frac{1}{\pi} \int_\frac{1}{2}^x \frac{dt}{\sqrt{t(1-t)}}
\end{align}$$<p>Which leads us to</p>
<blockquote>
<p><strong>Theorem 6.3:</strong> <em>(Arcsine law).</em> The probability that the fraction of time spenty by a particle ont he positive side is at most $x$ tends to $\frac{2}{\pi}\arcsin \sqrt{x}$:
</p>
$$ \mathbb{P}\left(\frac{\gamma (2n)}{2n} \leq x \right) = \frac{2}{\pi} \arcsin \sqrt{x} $$<p>
The consequence of this theorem is that it is most likely for a radnom walk to spend either almost all of its time on the positive side, or for it to spend almost no time on the positive side.</p>
<p>We can do this because there was no reason to limit the lower bound to $1/2$ rather than $0$ in the derivation</p></blockquote>
<h2 id="references">References</h2>
<ol>
<li>Helena (<a href="https://physics.stackexchange.com/users/12948/helena)">https://physics.stackexchange.com/users/12948/helena)</a>, What is the physical meaning of diffusion coefficient?, URL (version: 2014-12-03): <a href="https://physics.stackexchange.com/q/52977https://physics.stackexchange.com/a/52977/250863">https://physics.stackexchange.com/q/52977https://physics.stackexchange.com/a/52977/250863</a></li>
<li>Ackelsberg, Ethan (<a href="https://math.osu.edu/sites/math.osu.edu/files/What_is_2018_Arcsine_Law.pdf)">https://math.osu.edu/sites/math.osu.edu/files/What_is_2018_Arcsine_Law.pdf)</a>, What is the Arcsine Law?, URL (version: 2024-08-11): <a href="https://math.osu.edu/sites/math.osu.edu/files/What_is_2018_Arcsine_Law.pdf">https://math.osu.edu/sites/math.osu.edu/files/What_is_2018_Arcsine_Law.pdf</a></li>
</ol>
]]></content:encoded>
    </item>
    <item>
      <title>5.4 - Gaussian Processes</title>
      <link>https://hasithv.github.io/posts/eliasa/chap5/5-4/</link>
      <pubDate>Tue, 06 Aug 2024 21:17:49 -0700</pubDate>
      <guid>https://hasithv.github.io/posts/eliasa/chap5/5-4/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition 5.9:&lt;/strong&gt; A stochasitc process $\{X_t\}_{t \geq 0}$ is a &lt;em&gt;Gaussian Process&lt;/em&gt; if its finite dimensional distributions are consistent Gaussian measures for any $0 \leq t_1 &lt; t_2 &lt; \ldots &lt; t_k$.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Recall that a Gaussian random vector $\mathbf{X} = (X_1, X_2,\ldots,X_n)^T$ is completely characterized by its first and second moments
&lt;/p&gt;
$$\mathbf{m} = \mathbb{E}[\mathbf{X}], \quad \mathbf{K} = \mathbb{E}[(\mathbf{X} - \mathbf{m}) (\mathbf{X} - \mathbf{m})^T]$$&lt;p&gt;Meaning that the characteristic function is expressed only in terms of $\mathbf{m}$ and $\mathbf{K}$
&lt;/p&gt;</description>
      <content:encoded><![CDATA[<blockquote>
<p><strong>Definition 5.9:</strong> A stochasitc process $\{X_t\}_{t \geq 0}$ is a <em>Gaussian Process</em> if its finite dimensional distributions are consistent Gaussian measures for any $0 \leq t_1 < t_2 < \ldots < t_k$.</p></blockquote>
<p>Recall that a Gaussian random vector $\mathbf{X} = (X_1, X_2,\ldots,X_n)^T$ is completely characterized by its first and second moments
</p>
$$\mathbf{m} = \mathbb{E}[\mathbf{X}], \quad \mathbf{K} = \mathbb{E}[(\mathbf{X} - \mathbf{m}) (\mathbf{X} - \mathbf{m})^T]$$<p>Meaning that the characteristic function is expressed only in terms of $\mathbf{m}$ and $\mathbf{K}$
</p>
$$\mathbb{E}\left[e^{i \mathbf{\xi} \cdot \mathbf{X}}\right] = e^{i \mathbf{\xi} \cdot \mathbf{m} - \frac{1}{2}\mathbf{\xi}^T \mathbf{K} \mathbf{\xi}} $$<p>
This means that for any $0 \leq t_1 < t_2 < \ldots < t_k$, the measure $\mu_{t_1, t_2, \ldots, t_k}$ is uniquely determined by an $\mathbf{m} = (m(t_1), \ldots, m(t_k))$ and a covariance matrix $\mathbf{K}_{ij} = K(t_i, t_j)$. Because our $\mu$ satisifes the conditions for Kolomorov&rsquo;s extension theorem, we have a probability space and a stochastic process associated with $\mu$.</p>
<p>&#x26a0;&#xfe0f; Isn&rsquo;t one of the conditions of Kolmogorov&rsquo;s extension theoren that we need to be able to permute the $t_i$? How would this work if we require the $t_i$ to be increasing?</p>
<blockquote>
<p><strong>Theorem 5.10:</strong> Assuming that the stochastic process $\{X_t\}_{t\in[0,T]}$ satisfies
</p>
$$\mathbb{E} \left[ \int_0^T X_t^2 dt \right] < \infty$$<p>
then $m \in L^2_t$. Also, the operator
</p>
$$\mathcal{K} f(s) := \int_0^T K(s,t) f(t) dt$$<p>
is nonnegative, compact on $L^2_t$</p></blockquote>
<blockquote>
<p><strong>Proof:</strong> For the first statement,
</p>
$$\int_0^T \mathbb{E}[X]^2 dt \leq \int_0^T \mathbb{E}[X_t^2] dt < \infty$$<p>
For the second,
</p>
$$\begin{align}
\int_0^T \int_0^T K^2(s,t) ds dt &= \int_0^T \int_0^T \mathbb{E}[\left((X_t - m(t))(X_s - m(s))\right)^2]ds dt \\
&\leq \int_0^T \int_0^T \mathbb{E}[(X_t - m(t))^2]\mathbb{E}[(X_s - m(s))^2]ds dt \\
&\leq \left( \int_0^T \mathbb{E}[X_t] dt \right) \\
&\leq \infty 
\end{align}$$<p>
which lets us conclude that $K \in L^2([0,T] \times [0,T])$, which tells us that $\mathcal{K}$ is compact on $L^2_t$</p>
<p>&#x26a0;&#xfe0f; I can&rsquo;t properly find what theorem lets us say the last statement, but I can trust it for now.</p>
<p>Furthermore, noting that $K$ is symmetric which means that $\mathcal{K}$ is self adjoint, and we can say
</p>
$$(\mathcal{K}f, f) = \int_0^T \int_0^T \mathbb{E}[(X_t - m(t))]\mathbb{E}[(X_s - m(s))]f(t)f(s) ds dt \geq 0$$<p>
by symmetry of $s$ and $t$.</p></blockquote>
<p>If we want to extend the characteristic to $L_t$ rather than the finite dimensional version, we can write
</p>
$$\mathbb{E}[e^{i(\xi,X)}] = e^{i(\xi,m) - \frac{1}{2} (\xi, \mathcal{K} \xi)}, \quad \xi \in L^2_t$$<p>
With $(\xi,m) = \int_a^b \xi(t) m(t) dt$ and $\mathcal{K}\xi (t) = \int_a^b K(t,s) \xi(s) ds$. This is a fairly reasonable extrapolation from the finite dimensional case.</p>
<blockquote>
<p><strong>Theorem 5.13:</strong> <em>Karhunen-Loeve expansion</em>. Let $(X_t)_{t \in [0,1]}$ be a Gaussian process with mean 0 and covariance function $K(s,t)$, Assume that $K$ continuous and $\{\lambda_k\}$ be the set of eigenvalues for orthonormal eigenfunctions of $K$, $\{\phi_k\}$. Then, $X_t$ has the representation of
</p>
$$X_t = \sum_{k=1}^\infty \alpha_k \sqrt{\lambda_k} \phi_k(t)$$<p>
Where $\alpha_k$ is a standard normal random variable $\mathscr{N}(0,1)$</p>
<p>&#x26a0;&#xfe0f; I am omitting the proof because I feel the result is easy enough to intuitively grasp, and also it is a little theoretical, so maybe I should revisit it if I get more comfortable with proving covergence in probability.</p></blockquote>
]]></content:encoded>
    </item>
    <item>
      <title>5.3 - Markov Processes</title>
      <link>https://hasithv.github.io/posts/eliasa/chap5/5-3/</link>
      <pubDate>Sat, 03 Aug 2024 23:22:06 -0700</pubDate>
      <guid>https://hasithv.github.io/posts/eliasa/chap5/5-3/</guid>
      <description>&lt;h2 id=&#34;markov-processes-in-continuous-time-and-space&#34;&gt;Markov processes in continuous time and space&lt;/h2&gt;
&lt;p&gt;Given a probability space $(\Omega, \mathcal{F}, \mathbb{P})$ and the filtration $\mathbb{F} = (\mathcal{F}_t)_{t \geq 0}$, a stochastic process $X_t$ is called a Markov process wrt $\mathcal{F}_t$ if&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$X_t$ is $\mathcal{F}_t$-adapted&lt;/li&gt;
&lt;li&gt;For any $t \geq s$ and $B \in \mathcal{R}$, we have
$$\mathbb{P}(X_t \in B | \mathcal{F}_s) = \mathbb{P}(X_t \in B | X_s)$$
Essentially, this is saying that history doesn&amp;rsquo;t matter, only the current state matters. We can associate a family of probability measures $\{\mathbb{P}^x\}_{x\in\mathbb{R}}$ for the processes starting at $x$ by defining $\mu_0$ to be the point mass at $x$. Then, we still have
$$\mathbb{P}^x(X_t \in B | \mathcal{F}_s) = \mathbb{P}^x(X_t \in B | X_s), \quad t \geq s$$
and $\mathbb{E}[f(X_0)] = f(x)$ for any function $f \in C(\mathbb{R})$.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&amp;#x26a0;&amp;#xfe0f; I am not fully confident on what the above section is saying. Specifically, I am having trouble with understanding how we are defining $\mathbb{P}^x$. However, I can understand the strong markov property, so I think I should be okay moving forward.&lt;/p&gt;</description>
      <content:encoded><![CDATA[<h2 id="markov-processes-in-continuous-time-and-space">Markov processes in continuous time and space</h2>
<p>Given a probability space $(\Omega, \mathcal{F}, \mathbb{P})$ and the filtration $\mathbb{F} = (\mathcal{F}_t)_{t \geq 0}$, a stochastic process $X_t$ is called a Markov process wrt $\mathcal{F}_t$ if</p>
<ol>
<li>$X_t$ is $\mathcal{F}_t$-adapted</li>
<li>For any $t \geq s$ and $B \in \mathcal{R}$, we have
$$\mathbb{P}(X_t \in B | \mathcal{F}_s) = \mathbb{P}(X_t \in B | X_s)$$
Essentially, this is saying that history doesn&rsquo;t matter, only the current state matters. We can associate a family of probability measures $\{\mathbb{P}^x\}_{x\in\mathbb{R}}$ for the processes starting at $x$ by defining $\mu_0$ to be the point mass at $x$. Then, we still have
$$\mathbb{P}^x(X_t \in B | \mathcal{F}_s) = \mathbb{P}^x(X_t \in B | X_s), \quad t \geq s$$
and $\mathbb{E}[f(X_0)] = f(x)$ for any function $f \in C(\mathbb{R})$.</li>
</ol>
<p>&#x26a0;&#xfe0f; I am not fully confident on what the above section is saying. Specifically, I am having trouble with understanding how we are defining $\mathbb{P}^x$. However, I can understand the strong markov property, so I think I should be okay moving forward.</p>
<p>The <em>transition function</em> of a Markov process is defined as
</p>
$$p(B,t|x,s) = \mathbb{P}(X_t \in B | X_s = x)$$<p>
and it has the properties</p>
<ol>
<li>$p(.,t|x,s)$ is a probability measure on $\mathcal{R}$</li>
<li>$p(B,t|.,s)$ is a measurable function on $\mathbb{R}$</li>
<li>$p$ satisfies
$$p(B,t|y,s) = \int_\mathbb{R} p(B,t|y,u) p(dy,u|x,s), \quad s \leq u \leq t$$
The last property is the continuous analog of the <em>Chapman-Kolmogorov equation</em>, and it essentially lets us break the transition function into two the transition from $s$ to $u$ and from $u$ to $t$.</li>
</ol>
<p>Now, we can write the expenctation from $x$ as
</p>
$$ \begin{multline}
\mathbb{E}^x[f(X_{t_1}, X_{t_2}, \ldots, X_{t_n})] = \int_\mathbb{R} \ldots \int_\mathbb{R} f(x_1,x_2,\ldots,x_n) p(dx_n,t_n|x_{n-1},t_{n-1}) \\ \ldots p(dx_2, t_2 | x_1,t_1) p(dx_1, t_1|x,0)
\end{multline}
$$<p>when $t_n$ are strictly increasing.</p>
<p>$p(y,t|x,s)$ is a <em>transition density function</em> of $X$. &#x26a0;&#xfe0f; The book makes it seem like this is not always the case, but I fail to see when it isn&rsquo;t.</p>
<p>A stochastic process is <em>stationary</em> if the joint distributions are translation invariant in time. However, if the process only depends on the difference between time, then the process is <em>homogeneous</em>. The difference is that a stationary process has the same distribution at all times, while a homogeneous process has the same distribution for all time differences.</p>
<p>&#x26a0;&#xfe0f; at this point, the book dives into semigroup theory which I know nothing about, so I will skip this section for now.</p>
<h2 id="example-57---q-process">Example 5.7 - Q-process</h2>
<p>Recall the definition of a generator $\mathbf{Q}$ to be
</p>
$$\mathbf{Q} = \lim_{h \rightarrow 0+} \frac{1}{h} (\mathbf(P)(h) - \mathbf{I})$$<p>
Now, we will define an infinitesimal generator $\mathcal{A}$ on a sample space $S = \{1,2,\ldots,I\}$:
</p>
$$\mathcal{A}f = \lim_{t \rightarrow 0+} \frac{\mathbb{E}[f(X_t)] - f}{t}$$<p>
and
</p>
$$\begin{align}
\mathcal{A}f(i) &= \lim_{t \rightarrow 0+} \frac{\mathbb{E}^i[f(X_t)] - f(i)}{t} \\
&= \lim_{t \rightarrow 0+} \frac{1}{t} \left(\sum_{j \in S} (P_{ij} - \delta_{ij})f(j)\right)\\
&= \sum_{j \in S} q_{ij} f(j), \quad i \in S
\end{align}$$<p>
Thus, the generator $\mathbf{Q}$ is exactly the infinitesimal generator of $X_t$. This is important to digest especially because the $\mathcal{A}$ is new to me.</p>
<p>Extending the idea above, we get the backward Kolmogorov equation for $\mathbf{u} = (u_1, u_2, \ldots, u_I)^T$ and $u_i(t) = \mathbb{E}^i[f(X_t)]$:
</p>
$$\frac{d \mathbf{u}}{dt} = \mathbf{Qu} = \mathcal{A}\mathbf{u}$$<p>&#x26a0;&#xfe0f; This, too, is getting a little confusing. Let&rsquo;s delve into it a bit more.</p>
<blockquote>
<p>We are essentially dealing with a continous time markov chaic (CTMC) in the above case, because we have a finite number of states that have some associated probability of moving to another state at an infitesimal time step.</p>
<p><a href="https://en.wikipedia.org/wiki/Kolmogorov_equations#Continuous-time_Markov_chains">Wikipedia</a> says that for CTMC&rsquo;s, the Komogorov backward equations are, rather intuitively, that the time derviative of the probaility of transitioning from state $i$ at time $s$ to state $j$ at time $t$.
</p>
$$\frac{\partial P_{ij}}{\partial t}(s;t) = \sum_k P_{kj}(s;t)A_{ik}(s)$$<p>
Where $A$ is the <a href="https://en.wikipedia.org/wiki/Transition-rate_matrix">transition-rate matrix</a> in which an element $q_{ij}$ denotes the rate departing from $i$ and arriving in state $j$. Knowing this, I can understand why $\mathcal{A}$ is the generator $\mathbf{Q}$. Converting things back into our notation, we have
</p>
$$ \frac{d P_{ij}}{dt} = \sum_{k \in I} \mathcal{A}_{ik} P_{kj} $$<p>In that case, we look back at the expression for $\mathbb{E}^i[f(X_t)]$
</p>
$$\mathbb{E}^i[f(X_t)] = \sum_j P_{ij}f(j)$$<p>
So,
</p>
$$ \begin{align}
\implies \frac{d}{dt} \mathbb{E}^i[f(X_t)] &= \sum_j \frac{d P_{ij}}{d t} f(j) \\
&= \sum_j \left(\left[ \sum_k \mathcal{A_{ik}} P_{kj} \right] f(j) \right) \\
&= \sum_k \sum_j \mathcal{A}_{ik} P_{kj} f(j) \\
&= \sum_k \mathcal{A}_{ik} \mathbb{E}^k[f(X_t)]
\end{align}
$$<p>Now, it&rsquo;s clear that
</p>
$$\frac{d}{dt} \mathbf{u} = \mathcal{A} \mathbf{u}$$</blockquote>
<p>The backward kolmogrov equation is heavily linked to diffusion, so I will definitely explore that in the future.</p>
<p>On the other hand, if we have a distribution $\mathbf{\nu} = (\nu_1, \nu_2, \ldots, \nu_I)$ (which, by convention, is a row vector), then it satisfies the forward Kolomogrov equation
</p>
$$\frac{d\mathbf{\nu}}{dt} = \mathbf{\nu} \mathcal{A}$$<p>
Or using the adjoint
</p>
$$\frac{d\mathbf{\nu}^T}{dt} = \mathcal{A}^* \mathbf{\nu}^T$$<p>
where $\mathcal{A}^*$ is defined as
</p>
$$(A^* g, f) = (g, \mathcal{A} f) \quad \forall \; f \in \mathscr{B}, g \in \mathscr{B}$$<p>
If this is giving you trouble, refer to equation (3.19) in the book and think with bra-ket notation. Recall that if $\mathscr{B} = L^2$, then the dual space is also $L^2$, and so $\mathcal{A}^* = \mathcal{A}^T$.</p>
<p>&#x26a0;&#xfe0f; Is this last statement rigorous? Specifically, I am asking about stating that $\mathcal{A} = \mathbf{Q}$. The book seems to avoid saying both are directly equal, but it really looks like they are.</p>
<h2 id="example-58---poisson-process">Example 5.8 - Poisson process</h2>
<p>Consider the Poisson process $X_t$ on $\mathbb{N}$ with rate $\lambda$. Then,
</p>
$$\begin{align}
(\mathcal{A}f)(n) &= \lim_{t \rightarrow 0+} \frac{\mathbb{E}^n[f(X_t)] - f(n)}{t} \\
&= \lim_{t \rightarrow 0+} \frac{1}{t} \left( \sum_{k=n}^\infty \frac{(\lambda t)^{k-n}}{(k-n)!} e^{-\lambda t} f(k) - f(n)\right) \\
&= \lim_{t \rightarrow 0+} \frac{1}{t} \left( f(n)e^{-\lambda t} + f(n+1)\lambda t + \sum_{k=n+2}^\infty \frac{(\lambda t)^{k-n}}{(k-n)!} e^{-\lambda t} f(k) - f(n)\right) \\
&= \lim_{t \rightarrow 0+} \frac{1}{t} \left( f(n)(e^{-\lambda t}-1) + f(n+1)\lambda t e^{-\lambda t} + \sum_{k=n+2}^\infty \frac{(\lambda t)^{k-n}}{(k-n)!} e^{-\lambda t} f(k) \right) \\
&= \lambda(f(n+1) - f(n))
\end{align}$$<p>
The last step is justified with L&rsquo;Hopital&rsquo;s rule.</p>
<p>Then, the book says
</p>
$$\mathcal{A}^*f(n) = \lambda(f(n-1) - f(n))$$<blockquote>
<p>&#x26a0;&#xfe0f; Here is the best reasoning I can come up with:
</p>
$$(g, \mathcal{A}f) = (\mathcal{A}^* g, f), \quad \forall \; f\in \mathscr{B}, g \in \mathscr{B}^*$$<p>
And defining $f^\pm(n) := f(n \pm 1)$, then we require
</p>
$$(\mathcal{A}^*g, f) = \lambda(g,f^+) - \lambda(g,f)$$<p>
Then if we note that $(g,f^+) = (g^-,f)$ (this is the part I cannot justify), then it follows that $\mathcal{A}^*f(n) = \lambda(f(n-1) - f(n))$</p></blockquote>
<p>Again, lets compute the time derivative of $u(t,n) = \mathbb{E}^n[f(X_t)]$
</p>
$$\begin{align}
\frac{d u}{dt} &= \frac{d}{dt}\left(\sum_{k \geq n} f(k) \frac{(\lambda t)^{k-n}}{(k-n)!}e^{-\lambda t}\right) \\
&= \frac{d}{dt}\left( e^{-\lambda t} f(n) + \sum_{k > n} f(k) \frac{(\lambda t)^{k-n}}{(k-n)!}e^{-\lambda t} \right) \\
&=  -\lambda e^{-\lambda t} f(n) + \sum_{k > n} f(k) \left[ \left(-\lambda\frac{(\lambda t)^{k-n}}{(k-n)!}e^{-\lambda t}\right) + \left(\lambda\frac{(\lambda t)^{k-n-1}}{(k-n-1)!}e^{-\lambda t}\right) \right] \\
&= \lambda(u(t,n+1)-u(t,n)) \\
&= \mathcal{A}u(t,n)
\end{align}$$<p>And the time derivative of the distribution $\mathbf{\nu} = (\nu_0, \nu_1, \ldots)$ will be
</p>
$$\begin{align}
\frac{d \nu_n(t)}{dt} &= \frac{d}{dt}\left(\frac{(\lambda t)^{k-n}}{(k-n)!}e^{-\lambda t}\right) \\
&= -\lambda\frac{(\lambda t)^{k-n}}{(k-n)!}e^{-\lambda t} + \lambda \frac{(\lambda t)^{k-n-1}}{(k-n-1)!}e^{-\lambda t} \\
&= \lambda(\nu_{n-1} - \nu_n) \\
&= (\mathcal{A}^* \mathbf{\nu})_n
\end{align}$$<p>&#x26a0;&#xfe0f; I keep getting $\lambda(\nu_{n+1} - \nu_n)$, which disagrees with the book. Where did I go wrong?</p>
<p>Notice how both Markov processes satisfied the forward Kolmogrov equation for the distribution, and the backwards for the expected values. This is a general property of Markov processes (wow!) that will be revisited.</p>
]]></content:encoded>
    </item>
    <item>
      <title>5.2 - Filtration and Stopping Time</title>
      <link>https://hasithv.github.io/posts/eliasa/chap5/5-2/</link>
      <pubDate>Sat, 03 Aug 2024 18:57:02 -0700</pubDate>
      <guid>https://hasithv.github.io/posts/eliasa/chap5/5-2/</guid>
      <description>&lt;h2 id=&#34;filtration&#34;&gt;Filtration&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition 5.3:&lt;/strong&gt; (Filtration). Given a probability space, the filtration is a nondecreaseing family of $\sigma$-algebras $\{\mathcal{F}_t\}_{t \leq 0}$ such that $\mathcal{F}_s \subset \mathcal{F}_t \subset \mathcal{F}$ for all $0 \leq s &lt; t$.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Intuitively, the filtration is a sigma algebra of events that can be determined before time $t$ (we can&amp;rsquo;t lose information by foing forward in time). A stochastic process is called &lt;em&gt;$\mathcal{F}_t$-adapted&lt;/em&gt; if it is measurable with respect to $\mathcal{F}_t$; that is, for all $B \in \mathcal{R}$, $X_t^{-1}(B) \in \mathcal{F}_t$. We can always assume that the $\mathcal{F}_t$ contains $F_t^{X}$ and all sets of measure zero, where $F_t^{X} = \sigma(X_s, s \leq t)$ is the sigma algebra generated by the process $X$ up to time $t$.&lt;/p&gt;</description>
      <content:encoded><![CDATA[<h2 id="filtration">Filtration</h2>
<blockquote>
<p><strong>Definition 5.3:</strong> (Filtration). Given a probability space, the filtration is a nondecreaseing family of $\sigma$-algebras $\{\mathcal{F}_t\}_{t \leq 0}$ such that $\mathcal{F}_s \subset \mathcal{F}_t \subset \mathcal{F}$ for all $0 \leq s < t$.</p></blockquote>
<p>Intuitively, the filtration is a sigma algebra of events that can be determined before time $t$ (we can&rsquo;t lose information by foing forward in time). A stochastic process is called <em>$\mathcal{F}_t$-adapted</em> if it is measurable with respect to $\mathcal{F}_t$; that is, for all $B \in \mathcal{R}$, $X_t^{-1}(B) \in \mathcal{F}_t$. We can always assume that the $\mathcal{F}_t$ contains $F_t^{X}$ and all sets of measure zero, where $F_t^{X} = \sigma(X_s, s \leq t)$ is the sigma algebra generated by the process $X$ up to time $t$.</p>
<p>As an example, in a series of coin flips, when $n=0$
</p>
$$\mathcal{F}_0^X = \{\emptyset, \Omega\}$$<p>
and when $n=1$,
</p>
$$\mathcal{F}_1^X = \{\emptyset, \Omega, \{H\}, \{T\}\}$$<p>
when $n=2$,
</p>
$$\mathcal{F}_2^X = \sigma(\{\emptyset, \{HH\}, \{TT\}, \{HT\}, \{TH\} \})$$<p>
(I believe this last statement is equivalent to what the book has)</p>
<h2 id="stopping-time">Stopping Time</h2>
<blockquote>
<p><strong>Definition 5.4:</strong> (Stopping time for discrete time stochastic processes). A stopping time is a random variable $T$ taking values in $\{1,2,\ldots\}\cup \{\infty\}$ such that for any $n < \infty$,
</p>
$$\{T \leq n\} \in \mathcal{F}_n$$</blockquote>
<p>For the discrete case, it doesn&rsquo;t matter if we say $\{T \leq n\}$ or $\{T = n\}$ simply becase it has to be satisfied for all $n$.</p>
<blockquote>
<p><strong>Proposition 5.5:</strong> (Properties of stopping times). For the Markov process $\{X_n\}_{n \in \mathbb{N}}$, we have</p>
<ul>
<li>(1) if $T_1, T_2$ are stopping times, then $T_1 \wedge T_2, T_1 \vee T_2, T_1 + T_2$ are stopping times</li>
<li>(2) if $\{T_k\}_{k \geq 1}$ are stopping times then $\sup_k T_k, \inf_k T_k, \limsup_k T_k, \liminf_k T_k$ are stopping times</li>
</ul></blockquote>
<blockquote>
<p><strong>Definition 5.6:</strong> (Stopping time for continuous time stochastic processes). A stopping time is a random variable $T$ taking values in $[0,\infty]$ such that for any $t \in \mathbb{\bar{R}}^+$,
</p>
$$\{T \leq t\} \in \mathcal{F}_t$$</blockquote>
<p>Note that we cannot swap the inequality for an equals sign in the definition of a stopping time for continuous time processes. Furthermore, porposition 5.5 holds for conitnious time processes if the filtration is right continuous: $\mathcal{F}_t = \mathcal{F}_{t^+}= \bigcap_{s>t} \mathcal{F}_s$.</p>
]]></content:encoded>
    </item>
    <item>
      <title>5.1 - Axiomatic Construction of Stochastic Process</title>
      <link>https://hasithv.github.io/posts/eliasa/chap5/5-1/</link>
      <pubDate>Sat, 03 Aug 2024 16:45:46 -0700</pubDate>
      <guid>https://hasithv.github.io/posts/eliasa/chap5/5-1/</guid>
      <description>&lt;h2 id=&#34;definition-of-a-stochastic-process&#34;&gt;Definition of a stochastic process&lt;/h2&gt;
&lt;p&gt;A stochastic process is a parameterized random variable $\{X_t\}_{t\in\mathbf{T}}$ defined on a probability space $(\Omega, \mathcal{F}, \mathbb{P})$ taking on values in $\mathbb{R}$. $\mathbf{T}$ can seemingly be any subset of $\mathbb{R}$. For any fixed $t \in \mathbf{T}$, we can define the random variable&lt;/p&gt;
$$X_t: \Omega \rightarrow \mathbb{R}, \quad \omega \rightarrowtail X_t(\omega)$$&lt;p&gt;Thinking of a simple random walk, this means that $X_t$ is a random variable that takes in some subset of $\Omega = \{H,T\}^\mathbb{N}$ and outputs a real valued number (the sum of the first $t$ values in $\omega$): $\{\omega_1, \omega_2, \ldots \} \rightarrow \sum_{n \leq t} X(\omega_n)$&lt;/p&gt;</description>
      <content:encoded><![CDATA[<h2 id="definition-of-a-stochastic-process">Definition of a stochastic process</h2>
<p>A stochastic process is a parameterized random variable $\{X_t\}_{t\in\mathbf{T}}$ defined on a probability space $(\Omega, \mathcal{F}, \mathbb{P})$ taking on values in $\mathbb{R}$. $\mathbf{T}$ can seemingly be any subset of $\mathbb{R}$. For any fixed $t \in \mathbf{T}$, we can define the random variable</p>
$$X_t: \Omega \rightarrow \mathbb{R}, \quad \omega \rightarrowtail X_t(\omega)$$<p>Thinking of a simple random walk, this means that $X_t$ is a random variable that takes in some subset of $\Omega = \{H,T\}^\mathbb{N}$ and outputs a real valued number (the sum of the first $t$ values in $\omega$): $\{\omega_1, \omega_2, \ldots \} \rightarrow \sum_{n \leq t} X(\omega_n)$</p>
<p>On the other side of the coin, for a fixed $\omega \in \Omega$, we can define a real-valued measureable function on $\mathbf{T}$ called the trajectory of $X$</p>
$$X_.(\omega): \mathbf{T} \rightarrow \mathbb{R}, \quad t \rightarrowtail X_t(\omega)$$<p>Again, back to the random walk, this means that we can get a real valued output for any given $t$. To be even more compact, we can say taht a stochastic process is a measureable function from $\Omega \times \mathbf{T}$ to $\mathbb{R}$</p>
$$(\omega, t) \rightarrowtail X(\omega, t) := X_t(\omega)$$<p>The largest probability space that one can take is the infinite product space $\Omega = \mathbb{R}^\mathbf{T}$. Essentially, this is a space which can takeon any real value at any moment in time (&#x26a0;&#xfe0f; why are we restricting ourselves to $\mathbb{R}$? Why can&rsquo;t it be a vector valued function?)</p>
<p>For finite dimension distributions, we are interested in
</p>
$$\mu_{1,\ldots,t_k}(F_1 \times \ldots \times F_k) = \mathbb{P[X_{t_1}\in F_1, \ldots X_{t_k} \in F_k]}$$<blockquote>
<p><strong>Theorem 5.2:</strong> (Kolmogorov&rsquo;s extension theorem). Kolmogorov&rsquo;s extension theorem allows us to say, for any $\mu$ invariant under permuting the order of $t_k$ and $F_k$ and also adding additional time points with their associated $F$ being $\mathbb{R}$, that there exists a probability space and a stochastic prcess such that
</p>
$$\mu_{1,\ldots,t_k}(F_1 \times \ldots \times F_k) = \mathbb{P[X_{t_1}\in F_1, \ldots X_{t_k} \in F_k]}$$</blockquote>
<p>Kolmogorov&rsquo;s extension theorem is very general. In fact, so general that it does not give us a very good idea of what the process actually looks like. Usually, we start with this extremely general definition and then impose stricter conditions to prove that the measure can be defined on a smaller probability space rather than $\Omega$</p>
]]></content:encoded>
    </item>
    <item>
      <title>Applied Stochastic Analysis</title>
      <link>https://hasithv.github.io/posts/eliasa/eliasa/</link>
      <pubDate>Sat, 03 Aug 2024 16:43:39 -0700</pubDate>
      <guid>https://hasithv.github.io/posts/eliasa/eliasa/</guid>
      <description>&lt;p&gt;Here are my notes for E, Li, and Vanden-Eijnden&amp;rsquo;s &lt;a href=&#34;https://bookstore.ams.org/gsm-199/&#34;&gt;&lt;em&gt;Applied Stochastic Analysis&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Chapter 5 - Stochastic Processes
&lt;ul&gt;
&lt;li&gt;5.1 - &lt;a href=&#34;https://hasithv.github.io/posts/eliasa/chap5/5-1/&#34;&gt;Axiomatic Construction of Stochastic Process&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;5.2 - &lt;a href=&#34;https://hasithv.github.io/posts/eliasa/chap5/5-2/&#34;&gt;Filtration and Stopping Time&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;5.3 - &lt;a href=&#34;https://hasithv.github.io/posts/eliasa/chap5/5-3/&#34;&gt;Markov Processes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;5.4 - &lt;a href=&#34;https://hasithv.github.io/posts/eliasa/chap5/5-4/&#34;&gt;Gaussian Processes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Chapter 6 - Wiener Process
&lt;ul&gt;
&lt;li&gt;6.1 - &lt;a href=&#34;https://hasithv.github.io/posts/eliasa/chap6/6-1/&#34;&gt;The Diffusion Limit of Random Walks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;6.2 - &lt;a href=&#34;https://hasithv.github.io/posts/eliasa/chap6/6-2/&#34;&gt;The Invariance Principle&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;</description>
      <content:encoded><![CDATA[<p>Here are my notes for E, Li, and Vanden-Eijnden&rsquo;s <a href="https://bookstore.ams.org/gsm-199/"><em>Applied Stochastic Analysis</em></a></p>
<ul>
<li>Chapter 5 - Stochastic Processes
<ul>
<li>5.1 - <a href="/posts/eliasa/chap5/5-1/">Axiomatic Construction of Stochastic Process</a></li>
<li>5.2 - <a href="/posts/eliasa/chap5/5-2/">Filtration and Stopping Time</a></li>
<li>5.3 - <a href="/posts/eliasa/chap5/5-3/">Markov Processes</a></li>
<li>5.4 - <a href="/posts/eliasa/chap5/5-4/">Gaussian Processes</a></li>
</ul>
</li>
<li>Chapter 6 - Wiener Process
<ul>
<li>6.1 - <a href="/posts/eliasa/chap6/6-1/">The Diffusion Limit of Random Walks</a></li>
<li>6.2 - <a href="/posts/eliasa/chap6/6-2/">The Invariance Principle</a></li>
</ul>
</li>
</ul>
]]></content:encoded>
    </item>
    <item>
      <title>2.2 - Symmetric monoidal preorders</title>
      <link>https://hasithv.github.io/posts/fongspivakact/chap2/2-2/</link>
      <pubDate>Fri, 02 Aug 2024 01:26:30 -0700</pubDate>
      <guid>https://hasithv.github.io/posts/fongspivakact/chap2/2-2/</guid>
      <description>&lt;h2 id=&#34;221---definition-and-first-examples&#34;&gt;2.2.1 - Definition and first examples&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition 2.2:&lt;/strong&gt; A &lt;em&gt;symmetric monoidal structure&lt;/em&gt; on a preoirder $(X, \leq)$ consists of&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;(i) a &lt;em&gt;monoidal unit&lt;/em&gt;, $I \in X$&lt;/li&gt;
&lt;li&gt;(ii) a &lt;em&gt;monoidal product&lt;/em&gt; $\otimes: X \times X \rightarrow X$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;And the monoidal product $\otimes(x_1,x_2) = x_1 \otimes x_2$ must also satisfy the following properties (assume all elements are in $X$)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;(a) $x_1 \leq y_1$ and $x_2 \leq y_2 \implies x_1 \otimes x_2 \leq y_1 \otimes y_2$&lt;/li&gt;
&lt;li&gt;(b) $I \otimes x = x \otimes I = x$&lt;/li&gt;
&lt;li&gt;(c) associativity&lt;/li&gt;
&lt;li&gt;(d) commutivity/symmetry&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;(a) is called &lt;em&gt;monotnoicity&lt;/em&gt; and (b) is &lt;em&gt;unitality&lt;/em&gt;&lt;/p&gt;</description>
      <content:encoded><![CDATA[<h2 id="221---definition-and-first-examples">2.2.1 - Definition and first examples</h2>
<blockquote>
<p><strong>Definition 2.2:</strong> A <em>symmetric monoidal structure</em> on a preoirder $(X, \leq)$ consists of</p>
<ul>
<li>(i) a <em>monoidal unit</em>, $I \in X$</li>
<li>(ii) a <em>monoidal product</em> $\otimes: X \times X \rightarrow X$</li>
</ul>
<p>And the monoidal product $\otimes(x_1,x_2) = x_1 \otimes x_2$ must also satisfy the following properties (assume all elements are in $X$)</p>
<ul>
<li>(a) $x_1 \leq y_1$ and $x_2 \leq y_2 \implies x_1 \otimes x_2 \leq y_1 \otimes y_2$</li>
<li>(b) $I \otimes x = x \otimes I = x$</li>
<li>(c) associativity</li>
<li>(d) commutivity/symmetry</li>
</ul>
<p>(a) is called <em>monotnoicity</em> and (b) is <em>unitality</em></p></blockquote>
<p><strong>Remark 2.3:</strong> replacing $=$ with $\cong$ in definition 2.2 will give us a <em>weak monoidal structure</em>.</p>
<blockquote>
<p><strong>Exercise 2.5:</strong> The preorder structure $(\mathbb{R}, \leq)$ and the multiplication operation $\times$ will not give us a symmetric monoidal order because of the simple counter example of $-2 \times -2 \nleq 1 \times 1$.</p></blockquote>
<blockquote>
<p><strong>Example 2.6:</strong> A <em>monid</em> is similar to a symmetric monoidal preorder in that it consists of a set $M$, a function $*: M\times M \rightarrow M$, and an elment $e \in M$ called the <em>monid unit</em>, such that for every $m,n,p \in M$,</p>
<ul>
<li>$m * e = m$</li>
<li>$e * m = m$</li>
<li>associativity holds</li>
</ul>
<p>Further, if commutivity holds (which isn&rsquo;t not generally true),  then it is also called <em>commutative</em></p></blockquote>
<h2 id="222---introducing-wiring-diagrams">2.2.2 - Introducing wiring diagrams</h2>
<p>&#x26a0;&#xfe0f; I am seeing the wiring diagrams, but I fail to understand why they are any different from the Hasse diagrams we&rsquo;ve seen previously.</p>
<p>Essentially, wiring diagrams seem to be a way to encode information about symmetric monoidal structures. The basic rules built up so far are as follows:</p>
<ul>
<li>A wire without a label, or with the label of the monoidal unit, is equivalent to nothing</li>
<li>Otherwise, a wire labeled with an element represents that element (&#x26a0;&#xfe0f; this could be wrong)</li>
<li>Two parallel wires represent the monoidal product of those elements</li>
<li>Placing a $\leq$ block between two $x,y$ wires indicates that $x \leq y$</li>
</ul>
<p>Thinking back to the conditions for a symmetric monoidal structure, we find that</p>
<ol>
<li>
<p>Transitivity allows us to combine wiring diagrams left to right
<figure class="align-center ">
    <img loading="lazy" src="./images/transitivitypt1.png#center" width="400px"/> 
</figure>
</p>
</li>
<li>
<p>Monotonicity is represented as being able to combine wiring diagrams top to bottom
<figure class="align-center ">
    <img loading="lazy" src="./images/monotonicity.png#center" width="400px"/> 
</figure>
</p>
</li>
<li>
<p>A monoidal product with a monoidal unit and another element gives us the element again, because the monoidal unit is equivalent to nothing (reflexivity)</p>
</li>
<li>
<p>Associativity means we can &ldquo;wiggle&rdquo; around parallel wires
<figure class="align-center ">
    <img loading="lazy" src="./images/associativity.png#center" width="400px"/> 
</figure>
</p>
</li>
<li>
<p>Commutivity means we can cross wires
<figure class="align-center ">
    <img loading="lazy" src="./images/commutivity.png#center" width="400px"/> 
</figure>
</p>
</li>
</ol>
<p>It is intuitive to see how these wiring diagrams can be used to prove statements. In fact, the above images are trivial proofs. Take a look at the following exercise</p>
<blockquote>
<p><strong>Exercise 2.20:</strong>
Prove&ndash;given $t \leq v+w$, $w+u \leq x+z$, and  $v+x \leq y$&ndash;that $t+u \leq y+z$.</p>
<p>ALgebraically, we proceed like so:
</p>
$$\begin{align} 
    t + u &\leq (v+w) + u \\
    &\leq v + (w+u) \\
    &\leq v + (x+z) \\
    &\leq (v + x) + z \\
    &\leq y + z \\
\end{align}$$<p>
and the wiring diagram would look like
<figure class="align-center ">
    <img loading="lazy" src="./images/wiringex.png#center"
         alt="The squares are the $\leq$ blocks" width="400px"/> <figcaption>
            <p>The squares are the $\leq$ blocks</p>
        </figcaption>
</figure>
</p></blockquote>
<h2 id="223---applied-examples">2.2.3 - Applied examples</h2>
<p>While this section did solidify some concepts. It wasn&rsquo;t too important. Although, it did carry two useful examples: discarding and splitting.</p>
<p>With discarding, if a symmetric monoidal structure also satisfies $x \leq I$ for every $x \in X$, then it is possible to terminate any wire:
<figure class="align-center ">
    <img loading="lazy" src="./images/discard.png#center" width="300px"/> 
</figure>
</p>
<p>And if instead have a property like $x \leq x + x$, then we can split any wire:
<figure class="align-center ">
    <img loading="lazy" src="./images/splitting.png#center" width="300px"/> 
</figure>
</p>
<h2 id="224---abstract-examples">2.2.4 - Abstract examples</h2>
<p>Again, after a skim through, this section did not seem critical.</p>
<h2 id="225---monoidal-montone-maps">2.2.5 - Monoidal montone maps</h2>
<p>We begin with recalling that for any preorder $(X,\leq)$ we have an induced equivalence relation $\cong$ on $X$ where two elements $x \cong x' \iff x \leq x$ and $x' \leq x$</p>
<blockquote>
<p><strong>Definition 2.41:</strong> $\mathcal{P} = (P, \leq_P, I_P, \otimes_P)$ and $\mathcal{Q} = (Q, \leq_Q, I_Q, \otimes_Q)$ be monoidal preorders. A <strong>monoidal monotone</strong> from $\mathcal{P}$ to $\mathcal{Q}$ is a monotone map $f: (P, \leq_P) \rightarrow (Q, \leq_Q)$ which satisfies</p>
<ul>
<li>(a) $I_Q \leq_Q f(I_P)$</li>
<li>(b) $f(p_1) \otimes_Q f(p_1 \otimes_P p_2)$
for all $p_1,p_2 \in P$.</li>
</ul>
<p>Additionally, $f$ is a <em>strong monoidal monotone</em> if it satisfies</p>
<ul>
<li>(a&rsquo;) $I_Q \cong f(I_P)$</li>
<li>(b&rsquo;) $f(p_1) \otimes_Q f(p_1 \otimes_P p_2) \cong f(p_1) \otimes_Q f(p_2)$
And it is called a <em>strict monoidal monotone</em> if it satisfies</li>
<li>(a&rsquo;&rsquo;) $I_Q = f(I_P)$</li>
<li>(b&rsquo;&rsquo;) $f(p_1) \otimes_Q f(p_1 \otimes_P p_2) = f(p_1) \otimes_Q f(p_2)$</li>
</ul></blockquote>
<p>Monoidal monotones are said to be examples of <em>monoidal functors</em> in category theory.</p>
<p>The exercises for this section seem a little easy, so I will be skipping them for now, returning to them if I get confused on the defnitions of monoidal monotones.</p>
]]></content:encoded>
    </item>
    <item>
      <title>An Invitation to Appied Category Theory</title>
      <link>https://hasithv.github.io/posts/fongspivakact/fongspivakact/</link>
      <pubDate>Fri, 02 Aug 2024 00:47:08 -0700</pubDate>
      <guid>https://hasithv.github.io/posts/fongspivakact/fongspivakact/</guid>
      <description>&lt;p&gt;This is a collection of my notes for Brendan Fong and David Spivak&amp;rsquo;s &lt;a href=&#34;https://arxiv.org/pdf/1803.05316&#34;&gt;&lt;em&gt;An Invitation to Appied Category Theory&lt;/em&gt;&lt;/a&gt;. The first chapter was done through LaTeX, but the rest should be markdown with mathjax.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://hasithv.github.io/1.pdf&#34;&gt;Chapter 1 - Generative effects: Orders and adjunctions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Chapter 2 - Resource theories: Monoidal preorders and enrichment
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://hasithv.github.io/posts/fongspivakact/chap2/2-2/&#34;&gt;Section 2.2 - Symmetric monoidal preorders&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;</description>
      <content:encoded><![CDATA[<p>This is a collection of my notes for Brendan Fong and David Spivak&rsquo;s <a href="https://arxiv.org/pdf/1803.05316"><em>An Invitation to Appied Category Theory</em></a>. The first chapter was done through LaTeX, but the rest should be markdown with mathjax.</p>
<ul>
<li><a href="/1.pdf">Chapter 1 - Generative effects: Orders and adjunctions</a></li>
<li>Chapter 2 - Resource theories: Monoidal preorders and enrichment
<ul>
<li><a href="/posts/fongspivakact/chap2/2-2/">Section 2.2 - Symmetric monoidal preorders</a></li>
</ul>
</li>
</ul>
]]></content:encoded>
    </item>
    <item>
      <title>That&#39;s not how Probability Works!</title>
      <link>https://hasithv.github.io/posts/24-07-29-nothowprobabilityworks/</link>
      <pubDate>Tue, 30 Jul 2024 00:07:20 -0700</pubDate>
      <guid>https://hasithv.github.io/posts/24-07-29-nothowprobabilityworks/</guid>
      <description>&lt;p&gt;I was recently doing a probability puzzle that I can&amp;rsquo;t quite remember the context of, but I came across the answer that the probability would be
&lt;/p&gt;
$$\mathbb{P}(X) = n p^n \; \quad \forall \: n\in\mathbb{N}, p \in [0,1].$$&lt;p&gt;But this is obviously wrong! Plug in $p=.9, n=2$, and you get that $\mathbb{P}(X) = 1.62$. Thaat&amp;rsquo;s not how probability works! However, for $p=0.5$, $\mathbb{P}(X)$ will remain $\leq 1$ for all $n \in \mathbb{N}$. So, somewhere in the interval $(0.5,0.9)$, we reach a critical value where any $p$ greater than that will result in a probability greater than one, and any value less than it will be a bit more reasonable.&lt;/p&gt;</description>
      <content:encoded><![CDATA[<p>I was recently doing a probability puzzle that I can&rsquo;t quite remember the context of, but I came across the answer that the probability would be
</p>
$$\mathbb{P}(X) = n p^n \; \quad \forall \: n\in\mathbb{N}, p \in [0,1].$$<p>But this is obviously wrong! Plug in $p=.9, n=2$, and you get that $\mathbb{P}(X) = 1.62$. Thaat&rsquo;s not how probability works! However, for $p=0.5$, $\mathbb{P}(X)$ will remain $\leq 1$ for all $n \in \mathbb{N}$. So, somewhere in the interval $(0.5,0.9)$, we reach a critical value where any $p$ greater than that will result in a probability greater than one, and any value less than it will be a bit more reasonable.</p>
<p>So, what is this critical value that will help me save face?</p>
<p>Well, the question we are trying to answer, phrased a bit more formally, is:</p>
<blockquote>
<p>find the largest $p \in [0,1]$ such that $np^n \leq 1$ for all $n \in \mathbb{N}.$</p></blockquote>
<p>First, we rephrase the problem by stating
</p>
$$np^n \leq 1 \iff p^n \leq \frac{1}{n}.$$<p>Visually, this means that the exponential graph of $f_p(n) = p^n$ can never go above $g_p(n) = \frac{1}{n}$ for some fixed $p$. From this, we can deduce that the critical value of $p$, which we will denote as $p_0$, will satisfy the following relation:</p>
<blockquote>
<p>Given the parametrized forms of $f_p$ and $g_p$
</p>
$$F_{p}(t) = \begin{bmatrix}
t \\
f_p(t)
\end{bmatrix}, \; 
G_p(t) = \begin{bmatrix}
t \\
g_p(t)
\end{bmatrix},$$<p>
the critical $p_0$ value will be such that
</p>
$$F_{p_0}(t_0) = G_{p_0}(t_0),\text{ and } \dot{F}_{p_0}(t_0) = \lambda \dot{G}_{p_0}(t_0)$$<p>
for some $\lambda \in \mathbb{R}, t_0 \in \mathbb{R}^+$. In other words, their velocities will point in the same direction (and, perhaps more intuitively, the outward normals of each curve will be parallel, so the graphs &lsquo;kiss&rsquo; at some $t_0$ with the choice of $p_0$).</p></blockquote>
<p>Now, we have a fairly simple problem to solve. Because the $x$ component of $F$ and $G$ are always equal, we immediately find that $\lambda = 1$ for their time derivatives to be equal to each other. Now, that leads us to solve for a $p_0$ and $n$ such that
</p>
$$
\begin{aligned}
    f_{p_0}(t_0) &= g_{p_0}(t_0) \\
    \implies p_0^n &= \frac{1}{t_0}
\end{aligned}
$$<p>
and
</p>
$$
\begin{aligned}
     \dot{f}_{p_0}(t_0) &= \dot{g}_{p_0}(t_0) \\
    \implies -\ln(p_0)p_0^n &= \frac{1}{t_0^2}
\end{aligned}
$$<p>
So, rather unsatisfyingly, we boiled it down to a system of nonlinear equations
</p>
$$
\begin{cases}
    p_0^n = \frac{1}{t_0}, \\
    \ln(p_0)p_0^n = -\left(\frac{1}{t_0}\right)^2
\end{cases}
$$<p>
which I cannot solve, but Desmos tells me that $p_0 \approx 0.6922$ and $t_0 \approx 2.7181$.</p>
<p>Thus, my answer would have been reasonable in <em>some</em> convoluted scenario in which $p < 0.6922$.</p>
<p>(This answer, too, is not totally right! This is because there may be a larger $p$ value that satisfies $np^n \leq 1$ for $n \in \mathbb{N}$ but <em>not</em> for $n\in \mathbb{R}^+$. We solved for the $n \in \mathbb{R}^+$ case, which would technically give us a lower bound for $p_0$. Taking this into consideration, our $p_0$ value would really be $p_0 \approx 0.6934$)</p>
]]></content:encoded>
    </item>
    <item>
      <title>Introduction</title>
      <link>https://hasithv.github.io/posts/introduction/</link>
      <pubDate>Mon, 29 Jul 2024 00:07:20 -0700</pubDate>
      <guid>https://hasithv.github.io/posts/introduction/</guid>
      <description>&lt;p&gt;I have no idea what I am doing. Anyways, here&amp;rsquo;s a cool equation:&lt;/p&gt;
$$\frac{d}{dt}\left(\frac{\partial L}{\partial \dot{q}}\right) - \frac{\partial L}{\partial q} = 0$$</description>
      <content:encoded><![CDATA[<p>I have no idea what I am doing. Anyways, here&rsquo;s a cool equation:</p>
$$\frac{d}{dt}\left(\frac{\partial L}{\partial \dot{q}}\right) - \frac{\partial L}{\partial q} = 0$$]]></content:encoded>
    </item>
  </channel>
</rss>
