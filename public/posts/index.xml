<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Posts on HasithAlted</title>
    <link>https://hasithv.github.io/posts/</link>
    <description>Recent content in Posts on HasithAlted</description>
    <generator>Hugo -- 0.138.0</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 04 Nov 2024 14:44:14 -0600</lastBuildDate>
    <atom:link href="https://hasithv.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Review of &#34;Planting Undetectable Backdoors in Machine Learning Models&#34; paper by Goldwasser</title>
      <link>https://hasithv.github.io/posts/notes/reviews/24-11-04-plantingbackdoors/</link>
      <pubDate>Mon, 04 Nov 2024 14:44:14 -0600</pubDate>
      <guid>https://hasithv.github.io/posts/notes/reviews/24-11-04-plantingbackdoors/</guid>
      <description>&lt;p&gt;Notes on the paper &lt;a href=&#34;https://arxiv.org/abs/2204.06974&#34;&gt;&lt;em&gt;Planting Undetectable Backdoors in Machine Learning Models&lt;/em&gt;&lt;/a&gt; by Shafi Goldwasser, Michael P. Kim, Vinod Vaikuntanathan, and Or Zamir.&lt;/p&gt;
&lt;p&gt;This paper was recommended to me by Scott Aaronson if I wanted to better understand some earlier, more cryptographic/theoretical work in backdooring neural networks. I am also reading through Anthropic&amp;rsquo;s &lt;a href=&#34;https://arxiv.org/abs/2401.05566&#34;&gt;&lt;em&gt;Sleeper Agents&lt;/em&gt;&lt;/a&gt; paper, which is more recent and practical in its approach to backdooring current LLMs, those notes will be posted soon as well.&lt;/p&gt;</description>
      <content:encoded><![CDATA[<p>Notes on the paper <a href="https://arxiv.org/abs/2204.06974"><em>Planting Undetectable Backdoors in Machine Learning Models</em></a> by Shafi Goldwasser, Michael P. Kim, Vinod Vaikuntanathan, and Or Zamir.</p>
<p>This paper was recommended to me by Scott Aaronson if I wanted to better understand some earlier, more cryptographic/theoretical work in backdooring neural networks. I am also reading through Anthropic&rsquo;s <a href="https://arxiv.org/abs/2401.05566"><em>Sleeper Agents</em></a> paper, which is more recent and practical in its approach to backdooring current LLMs, those notes will be posted soon as well.</p>
<h2 id="quick-summary">Quick Summary</h2>
<ul>
<li>Formally defines a backdoor in a neural network and then defines what it means for a backdoor to be undetectable, non-replicable, and persistent.</li>
<li>Constructs a simple backdoor method that is easily detectable and replicable, and then presents a more sophisticated backdoor method that is non-replicable and undetectable.</li>
<li>Also presents a method for constructing a neural network that is persistent to gradient descent.</li>
</ul>
<h2 id="discussion">Discussion</h2>
<ul>
<li>I found the formal definitions of backdoors, undetectability, and non-replicability to be very useful for future approaches to backdooring neural networks. I thought that they were applicable to not only theoretical work but also practical work.</li>
<li>The definition for a persistent neural network, however, seemed to only be a purely theoretical exercise. Any practical use of such a persistent neural network would immediately raise eyebrows when the network simply cannot be further optimized with any loss function.</li>
<li>While the simple backdoored method could definitely be used in practice, as the paper points out, it is easily detectable and replicable.</li>
<li>The more persistent backdoor definitely seems like a strong method for backdooring neural networks, but it can only be used as a blackbox. This means that the adversary must have access to the model&rsquo;s predictions and not the model itself, since seeing the &lsquo;weights&rsquo; of the model would reveal the verification step of the backdoor&ndash;instantly giving away that some backdoor is present. (Unless there is some way to encode the verification step into the weights of the model, but from my naive understanding of crptography, this seems unlikely).</li>
</ul>
<hr>
<h2 id="full-summary">Full Summary</h2>
<hr>
<h2 id="3-preliminaries">3 Preliminaries</h2>
<p><strong>Notations</strong></p>
<ul>
<li>
<p>Let ${\mathcal{X} \rightarrow \mathcal{Y}}$ represent the set of all functions from $\mathcal{X}$ to $\mathcal{Y}$.</p>
</li>
<li>
<p>Probabilistic polynomial time is shortented to $\text{p.p.t.}$</p>
</li>
<li>
<p>A function $\text{negl}: \mathbb{N} \rightarrow \mathbb{R}^+$ is negligible when, for all polynomial functions $p(n)$, there exists an $n_0 \in \mathbb{N}$ such that for all $n > n_0, \; \text{negl}(n) < 1/p(n)$</p>
</li>
</ul>
<h3 id="31-supervised-learning">3.1 Supervised Learning</h3>
<p>A supervised learning task maps the input space $\mathcal{X} \subseteq \mathbb{R}^d$ to the label space $\mathcal{Y}$. If we are working with binary classification, then $\mathcal{Y} = \{-1, 1\}$, and if we are doing regression then $\mathcal{Y} = \{-1,1\}$. (Obviously, this is only an arbitrary constraint of the paper).</p>
<p>For a given data distribution $\mathcal{D}$ over $\mathcal{X} \times \mathcal{Y}$, the optimal predictor of the mean, $f^*: \mathcal{X} \rightarrow [-1,1]$, is
</p>
$$ f^*(x) = \underset{\mathcal{D}}{\mathbb{E}}[Y|X=x].$$<p>
And for classiciation tasks, the optimal predictor is
</p>
$$ \frac{f^*(x)+1}{2} = \underset{\mathcal{D}}{\mathbb{P}}[Y=1|X=x].$$<blockquote>
<p><strong>Definition 3.1</strong> <em>(Efficient Training Algorithm):</em> For a hypothesis class $\mathcal{H}$, an efficient training algorithm $\text{Train}^\mathcal{D}: \mathbb{N} \rightarrow \mathcal{H}$ is a probabilistic algorithm with sample access to $\mathcal{D}$ that, for any $n \in \mathbb{N}$, the algorithm runs in polynomial time in $n$ and outputs a hypothesis $h_n \in \mathcal{H}$
</p>
$$h_n \leftarrow \text{Train}^\mathcal{D}(1^n)$$</blockquote>
<p>Essentially, an efficient training algorithm is a polynomial time algorithm that outputs a hypothesis with some high probability. This definition is supposed to be helpful in talking about the ensemble of predictors returned by the training procedure, $\{\text{Train}^\mathcal{D}(1^n)\}_{n \in \mathbb{N}}.$ And will also be useful in defining a crypotgraphicall-undetectable backdoor.</p>
<p><strong>PAC Learning</strong></p>
<p>For some loss $l$, a training algorithm $\text{Train}$ is an agnostic PAC learner for a concept class $\mathcal{C} \subseteq \{\mathcal{X} \rightarrow \mathcal{Y}\}$ if, for any $n=n(d,\epsilon,\delta)=\text{poly}(d,1/\epsilon,\log(1/\delta))$ the hypothesis returned by the algorithm $h_n \leftarrow \text{Train}^{\mathcal{D}}(1^n)$ satisfies
</p>
$$ l_\mathcal{D}(h_n) \leq \min_{c^* \in \mathcal{C}} l_\mathcal{D}(c^*) + \epsilon$$<p>
with probability at least $1-\delta$.</p>
<p>The staistical error of a hypothesis is represented by
</p>
$$\text{er}_\mathcal{D}(h) = \underset{(X,Y)\sim\mathcal{D}}{\mathbb{E}}[|h(X) - f^*(X)|]$$<p><strong>Adversarially-Robust Learning</strong>
As an extension of the PAC loss minization framework, we can forulate a robust version of the loss function. FOr some bounded-norm ball $\mathcal{B}$ and a loss function $l$, the robust loss function, $r$, is defined as
</p>
$$r_\mathcal{D}(h) = \underset{(X,Y)\sim\mathcal{D}}{\mathbb{E}}\left[\max_{\mathcal{B}} l(h(X),Y)\right].$$<p>
Such methods can mitigate the prevalence of adversarial examples, but the paper claims that they can subvert these defenses (much like the Anthropic sleeper agents paper).</p>
<h3 id="32-computational-indistinguishability">3.2 Computational Indistinguishability</h3>
<p>To formally say that two distributions looke the same, we use the concept of indistinguishability. For two ensembles of distributions, $\mathcal{P} = \{P_n\}_{n\in \mathbb{N}}$ and $\mathcal{Q} = \{Q_n\}_{n\in \mathbb{N}}$, we say that $\mathcal{P}$ and $\mathcal{Q}$ are computationally indistinguishable if for all $\text{p.p.t.}$ distinguishers $A$ there exists a negligible function $\text{negl}$ such that
</p>
$$\left| \underset{Z \in P_n}{\mathbb{E}}[A(Z)=1] - \underset{Z \in Q_n}{\mathbb{E}}[A(Z)=1] \right| \leq \text{negl}(n).$$<h2 id="4-defining-undetectable-backdoors">4 Defining Undetectable Backdoors</h2>
<blockquote>
<p><strong>Definition 4.1</strong> <em>(Classification Backdoor):</em> A $\gamma$-backdoor parameterized by a hypothesis class, a norm, and a constant $\gamma \in \mathbb{R}$ consists of two algorithms $(\text{Backdoor},\text{Activate})$ and a backdoor set $\mathcal{S} \subseteq \mathcal{X}$.</p>
<ul>
<li>$\text{Backdoor}^{\mathcal{D}}$ is a probabilistic polynomial time training algorithm that takes as input a security parameter $n$ and outputs a classifier hypothesis $h_n \in \mathcal{H}$ and a backdoor key $\text{bk}$.
$$(h_n, \text{bk}) \leftarrow \text{Backdoor}^\mathcal{D}(1^n).$$</li>
<li>$\text{Activate}$ is a p.p.t. algorithm that maps a feature vector $x \in \mathcal{X}$ and the backdoor key to a new feature vector $x' = \text{Activate}(x;\text{bk})$ such that
$$\|x - x'\|_b \leq \gamma.$$
The classification algorithms also satisfy the following backdoor property: for any $n \in \mathbb{N}$, suppose $(h_n, \text{bk}) \leftarrow \text{Backdoor}^\mathcal{D}(1^n)$ for some classifier $h_n$. Then, for any $x \in \mathcal{S}$,
$$h_n(\text{Activate}(x;\text{bk})) = -h_n(x).$$</li>
</ul>
</blockquote>
<blockquote>
<p><strong>Definition 4.2</strong> <em>(Regression Backdoor):</em> Let $\mathcal{Y} = [-1,1]$ and $\beta \in \mathbb{R}$ be a parameter in addition to $\gamma$. Now, a $(\gamma, \beta)$-backdoor is defined in the same way as Definition 4.1 except that $\text{Activate}$ takes an additional parameter $\hat{y} \in \{-1,1\}$, for example $x' = \text{Activate}(x,\hat{y};\text{bk})$ and it would satisfy
</p>
$$\|x - x'\|_b \leq \gamma \quad \text{and} \quad |h_n(x') - \hat{y}| \leq \beta.$$</blockquote>
<h3 id="41-undetectability">4.1 Undetectability</h3>
<p>Keep in mind that $h_n$ is really a distribution across hypotheses (since the algorithms returning $h_n$ are probabilistic). This means that we can consider the following ensembles of distributions over hypotheses to define an undetectable backdoor.</p>
<blockquote>
<p><strong>Definition 4.6</strong> <em>(Undetectable Backdoor):</em> Given two ensembles of distributions over hypotheses
</p>
$$\mathcal{T}_\mathcal{D} = \{h_n\}_{n \in \mathbb{N}}$$$$\mathcal{B}_\mathcal{D} = \{\tilde{h}_n\}_{n \in \mathbb{N}}$$<p>
where $h_n \leftarrow \text{Train}^\mathcal{D}(1^n)$ and $\tilde{h}_n \leftarrow \text{Backdoor}^{\mathcal{D}}(1^n)$ for some data distribution $\mathcal{D}$. Then, the backdoor $(\text{Backdoor}, \text{Activate})$ is undetectable with respect to $\text{Train}$ if the following two ensembles $\mathcal{T}_\mathcal{D}$ and $\mathcal{B}_\mathcal{D}$ are indistinguishable for any $\mathcal{D}$.</p>
</blockquote>
<blockquote>
<p><strong>Definition 4.7</strong> <em>(Non-replicable Backdoor):</em> (Formal definition omitted) A backdoor is non-replicable if having access to many backdoored examples does not allow an adversary to generate new backdoored inputs better than some ideal generator without access to the backdoored inputs.</p>
</blockquote>
<h2 id="5-non-replicable-backdoors-from-digital-signatures">5 Non-Replicable Backdoors from Digital Signatures</h2>
<h3 id="51-simple-backdoors-from-checksums">5.1 Simple Backdoors from Checksums</h3>
<p>The goal of a simple backdoor is to take any neural network $N$ and produce a new network $N'$ such that</p>
<ul>
<li>$N = N'$ on almost all inputs $x$.</li>
<li>Every input $x$ can be efficiently transformed into a backdoored input $x'$ such that $N'(x') \neq N'(x)$.</li>
</ul>
<p>The scheme for the simple checksum method is as follows:</p>
<ol>
<li>Let $n \in \mathbb{N}$ be a parameter such that $n \ll d$, where $d$ is the dimension of the input space.</li>
<li>Partition the input coordinates into $n$ disjoint and nearly equally sized sets $[d] = I_1 \cup \ldots \cup I_n$.</li>
<li>Let $v \in \mathbb{F}_2^n$ be a uniformly chosen binary vector of length $n$.</li>
<li>Define a checksum function as
$$h(x) = \bigwedge_{i=1}^{n}\left( \bigoplus_{j \in I_i} \text{sgn}(x_j) = v_i \right)$$
where $\text{sgn}(x_j): \mathbb{R} \rightarrow \{0,1\}$ returns 1 iff the input is non-negative.</li>
<li>Note that to flip the output of $h$, we can simply flip the sign of an arbitrary coordiante in each $I_i$ where $\bigoplus_{j \in I_i} \text{sgn}(x_j) \neq v_i$.</li>
<li>Then, using a MUX gate, which outputs $y$ from the network $N$ if the checksum&rsquo;s output $s=0$, and outputs an arbitrary coordinate of the input, $x_{out}$, if $s=1$.</li>
</ol>
<p>The above scheme is bad for two reasons: 1) it is easily detectable, and 2) it is easily replicable.</p>
<h3 id="52-non-replicable-backdoors-from-digital-signatures">5.2 Non-Replicable Backdoors from Digital Signatures</h3>
<p>The idea presented here is that our backdoor hypothesis will return the backdoored output if and only if a digital signature is valid. The following theorem formalizes this idea.</p>
<blockquote>
<p><strong>Theorem 5.4</strong> Assuming that strongly unforgeable digital signature schemes exist, for every trainingprocedure $\text{Train}$, there exists a model backdoor $(\text{Backdoor}, \text{Activate})$, which is non replicable and black-box undetectable</p>
</blockquote>
<p>An example backdoored model is
</p>
$$
\tilde{h}(w,y,\sigma) = \begin{cases}
(-1)^y & \text{if } \text{Verify}(\text{vk}, (w,y), \sigma) = \text{accept} \\
h(w,y,\sigma) & \text{otherwise}
\end{cases}
$$<p>
where $\text{Verify}$ is the verification algorithm of the digital signature scheme, $\text{vk}$ is the verification key, $w \in \{0,1\}^d$ is the input, $y \in \{0,1\}$ is a target, and $\sigma$ is the signature generated by a secret key signing $w || y$.</p>
<h3 id="53-persistent-neural-networks">5.3 Persistent Neural Networks</h3>
<p>The paper now presents a way to ensure that, given any neural network $N$, you can construct a new neural network $N'$ such that it is peristent to gradient descent.</p>
<blockquote>
<p><strong>Definition 5.5</strong> <em>(Persistent Neural Network):</em> For a loss function $l$, a neural network $N = N_\mathbf{w}$ is $l$-persistent to gradient descent if $\nabla l(\mathbf{w}) = 0$.</p>
</blockquote>
<blockquote>
<p><strong>Theorem 5.7:</strong> Let $N$ be a neural network of size $|N|$ and depth $d$. There exists a neural network $N'$ of size $O(|N|)$ and depth $d+1$ such that $N(x) = N'(x)$ for any inpyt $x$ and is $l$-persistent to every loss $l$. Furthermore, we can construct $N'$ in linear time.</p>
<blockquote>
<p><strong>Proof:</strong> Take three copies of $N$ without the input layer, $N_1, N_2, N_3$, and place them all parallel to each other. The new input layer will be the same as the input layer for $N$ and will be passed into each copy.</p>
<p>Then, add a new final layer that takes the output of each of the three copies and outputs the majority vote of the three outputs. This can be constructed in a single layer as
</p>
$$1 \cdot N_1(x) + 1 \cdot N_2(x) + 1 \cdot N_3(x) \geq \frac{3}{2}$$<p>
which is equivalent to the majority vote since the output of any $N$ is always 1 or 0. Now, for any weight $w$ within $N_1$, $N_2$, or $N_3$, the gradient of the loss function with respect to $w$ is 0 since we can&rsquo;t change the majority vote by changing only one of the three networks.
For the new final layer, changing the RHS to any value in $(0,3)$ will leave the majority vote unchanged, so the gradient is 0. Additionally, changing the coefficients on the LHS will to any value in $(\frac{1}{2}, \infty)$ will also leave the final output unchanged.</p>
<p>Thus, the gradient of the loss function with respect to any weight in $N'$ is 0.</p>
</blockquote>
</blockquote>
<hr>
]]></content:encoded>
    </item>
    <item>
      <title>Is Basketball a Random Walk?</title>
      <link>https://hasithv.github.io/posts/projects/24-08-17-basketballrandomwalk/</link>
      <pubDate>Sat, 17 Aug 2024 20:36:30 -0500</pubDate>
      <guid>https://hasithv.github.io/posts/projects/24-08-17-basketballrandomwalk/</guid>
      <description>&lt;p&gt;About two years ago, I attended a seminar given by &lt;a href=&#34;https://sites.santafe.edu/~redner/&#34;&gt;Dr. Sid Redner&lt;/a&gt; of the &lt;a href=&#34;https://www.santafe.edu/&#34;&gt;Santa Fe Institute&lt;/a&gt; titled, &amp;ldquo;Is Basketball Scoring a Random Walk?&amp;rdquo; I  was certainly skeptical that such an exciting game shared similarities with coin flipping, but, nevertheless, Dr. Redner went on to convince me&amp;ndash;and surely many other audience members&amp;ndash;that basketball does indeed exhibit behavior akin to a random walk.&lt;/p&gt;
&lt;p&gt;At the very end of his lecture, Dr. Redner said something along the lines of, &amp;ldquo;the obvious betting applications are left as an exercise to the audience.&amp;rdquo; So, as enthusiastic audience members, let&amp;rsquo;s try to tackle this exercise.&lt;/p&gt;</description>
      <content:encoded><![CDATA[<p>About two years ago, I attended a seminar given by <a href="https://sites.santafe.edu/~redner/">Dr. Sid Redner</a> of the <a href="https://www.santafe.edu/">Santa Fe Institute</a> titled, &ldquo;Is Basketball Scoring a Random Walk?&rdquo; I  was certainly skeptical that such an exciting game shared similarities with coin flipping, but, nevertheless, Dr. Redner went on to convince me&ndash;and surely many other audience members&ndash;that basketball does indeed exhibit behavior akin to a random walk.</p>
<p>At the very end of his lecture, Dr. Redner said something along the lines of, &ldquo;the obvious betting applications are left as an exercise to the audience.&rdquo; So, as enthusiastic audience members, let&rsquo;s try to tackle this exercise.</p>
<p><em>Note: all code and data for this project can be found in the <a href="https://github.com/hasithv/nba-odds">github repository</a> [<a href="https://github.com/hasithv/nba-odds">2</a>]</em></p>
<h2 id="understanding-the-model">Understanding the Model</h2>
<p>I highly recommend reading the paper [<a href="https://arxiv.org/abs/1109.2825v1">1</a>] that Dr. Redner et. al. published for a full understanding. However, here are the main points that we will need:</p>
<h3 id="assumptions">Assumptions</h3>
<ol>
<li><strong>Random Walk Definition:</strong> The net score, $\{\Delta_n\}_{n \in \mathbb{N}}$ (the difference between the scores of teams A and B) can be modeled as an anti-persistent random walk. This means that if the score moves up during a play, then the next play is more likely to move down.
$$\Delta_n = \sum_{i=1}^{n} \delta_i$$
$$\begin{cases}
\delta_i > 0 & \text{with probability } p_i \\
\delta_i < 0 & \text{with probability } 1 - p_i
\end{cases}$$
$$p_n = (\textcolor{orange}{\text{other terms}}) - .152 \left(\frac{|\delta_{n-1}|}{\delta_{n-1}}\right), \quad \forall \; i,n \in \mathbb{N}$$
Where $\delta_i$ is the points made during the $i$th play, and $\Delta_0 = \delta_0 = 0$. This is explained by the fact that the scoring team loses possession of the ball, so it is harder for them to score again.</li>
<li><strong>Coasting and Grinding:</strong> The probability of a team scoring is proportional to how far they are behind in points:
$$p_n = (\textcolor{orange}{\text{other terms}}) - .152 r_{n-1} - .0022 \Delta_{n-1}, \quad \forall \; n \in \mathbb{N}$$
Here, $r_{n-1} = \left(\frac{|\delta_{n-1}|}{\delta_{n-1}}\right)$.This is explained in the paper as &ldquo;the winning team coasts, and the losing team grinds.&rdquo;</li>
<li><strong>Team Strengths:</strong> The strength of a team also has a effect on the probability of scoring:
$$p(I_A, r_{n-1}, \Delta_{n-1}) = I_A - 0.152r_{n-1} - 0.0022 \Delta_{n-1}, \quad \forall \; n \in \mathbb{N}$$
Where the strength of a team is defined by parameters $X_A$ and $X_B$ as $I_A(X_A, X_B) = \frac{X_A}{X_A + X_B}$. Additionally, $X_A$ and $X_B$ are distributed according to $\mathcal{N}(\mu = 1,\sigma^2=.0083).$</li>
<li><strong>Time Between Plays:</strong> The time between each play is exponentially distributed
$$\tau_n \sim \text{Exp}(\lambda)$$</li>
<li><strong>Scoring Probabilities:</strong> For each play, the probabilities of scoring $n$ points is
$$\begin{cases}
\begin{align}
    \delta = 1, \quad &8.7\% \\
    \delta = 2, \quad &73.86\% \\
    \delta = 3, \quad &17.82\% \\
    \delta = 4, \quad &0.14\% \\
    \delta = 5, \quad &0.023\% \\
    \delta = 6, \quad &0.0012\% \\
\end{align}
\end{cases}$$
&#x26a0;&#xfe0f; The only confusion I have with the paper is that the above &ldquo;probabilities&rdquo; do not sum to 1, so I am not sure how to interpret them. I went ahead and removed $\delta=6$ and lowered the probability of  $\delta=5$ so that the probabilities sum to 1. This should be okay since 5 and 6 point plays are so rare that they should not affect the model too much.</li>
</ol>
<h2 id="building-the-simulation">Building the Simulation</h2>
<h3 id="gathering-simulation-data">Gathering Simulation Data</h3>
<p>Two things I wanted to improve were to expand the dataset and to use bayesian updates to better estimate the $\lambda$ and $I_A$ for a game.</p>
<p>For the dataset, Dr. Redner only used games from 2006-2009, but I managed to obtain all playoff games after 2000. Using this, I looked at the distribution for the average number of plays per 30s</p>
<figure class="align-center ">
    <img loading="lazy" src="./images/playrate.svg#center"
         alt="Distribution for $\lambda$ values. The orange normal curve has mean 1.005 and std 0.1. I am not sure why there was a large deficit at the 1 play per 30s mark; it seems to be half as high as it shold be." width="400px"/> <figcaption>
            <p>Distribution for $\lambda$ values. The orange normal curve has mean 1.005 and std 0.1. I am not sure why there was a large deficit at the 1 play per 30s mark; it seems to be half as high as it shold be.</p>
        </figcaption>
</figure>

<p>which gives us a prior for $\lambda$ that we can live-update to better fit our model to a given game (and we already have the prior for $X$):
</p>
$$\lambda \sim \mathcal{N}(1.005,0.1)$$$$X \sim \mathcal{N}(1, \sqrt{0.0083})$$<h3 id="bayesian-updating">Bayesian Updating</h3>
<p>Using simple bayesian updates, we should be able to properly estimate how likely a certain $\lambda$ or $I_A$ is given the game data of scoring rates $\{t_1,\ldots,t_n\}$, and who scored on each play $\{r_1,\ldots,r_n\}$:
</p>
$$\begin{align}
    f(\lambda | \{t_1,\ldots,t_n\}) &\propto f(\{t_1,\ldots,t_n\} | \lambda) f(\lambda) \\
    &\propto \left(\prod_{i=1}^{n} f(t_i | \lambda) \right) f(\lambda) \\
    &\propto \left(\prod_{i=1}^{n} \lambda e^{-\lambda t_i} \right) \mathcal{N}(1.005,0.1) \\
    &\propto \left(\lambda^n e^{-\lambda \sum_{i=1}^{n} t_i} \right) \mathcal{N}(1.005,0.1) \\ \\
    f(X_A, X_B | \{r_1,\ldots,r_n\}) &\propto f(\{r_1,\ldots,r_n\} | X_A,X_B) f(X_A,X_B) \\
    &\propto \left(\prod_{i=1}^{n} f(r_i | X_A,X_B) \right) f(X_A,X_B) \\
    &\propto \left|\prod_{i=1}^{n} p\left(\frac{X_A}{X_A + X_B}, r_{i-1}, \Delta_{i-1} \right) - \frac{1-r_i}{2} \right| \cdot \mathcal{N}(1, \sqrt{0.0083})(X_A) \cdot \mathcal{N}(1, \sqrt{0.0083})(X_B)\\
\end{align}
$$<p>
As you can see, the update for the $X$ values is a bit  more complicated, but it is still fairly easy to compute. The code to do this is show below:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-julia" data-lang="julia"><span style="display:flex;"><span><span style="color:#66d9ef">function</span> update_rate!(params, time_deltas)
</span></span><span style="display:flex;"><span>    time_deltas <span style="color:#f92672">=</span> time_deltas<span style="color:#f92672">/</span><span style="color:#ae81ff">30</span>
</span></span><span style="display:flex;"><span>    params<span style="color:#f92672">.</span>rate <span style="color:#f92672">=</span> (x) <span style="color:#f92672">-&gt;</span> x<span style="color:#f92672">^</span>length(time_deltas) <span style="color:#f92672">*</span> exp(<span style="color:#f92672">-</span>x <span style="color:#f92672">*</span> sum(time_deltas)) <span style="color:#f92672">*</span> pdf(defaultRate, x) <span style="color:#f92672">/</span> params<span style="color:#f92672">.</span>rate_Z
</span></span><span style="display:flex;"><span>    normalize_rate!(params)
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">end</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">function</span> update_strengths!(params, scoring_data, lookback<span style="color:#f92672">=</span><span style="color:#ae81ff">15</span>)
</span></span><span style="display:flex;"><span>    lookback <span style="color:#f92672">=</span> min(lookback, length(scoring_data))
</span></span><span style="display:flex;"><span>    scoring_data <span style="color:#f92672">=</span> scoring_data[<span style="color:#66d9ef">end</span><span style="color:#f92672">-</span>lookback<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span><span style="color:#f92672">:</span><span style="color:#66d9ef">end</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    score_probs <span style="color:#f92672">=</span> (x,y) <span style="color:#f92672">-&gt;</span> prod(map((z) <span style="color:#f92672">-&gt;</span> score_prob(z, x, y), scoring_data))
</span></span><span style="display:flex;"><span>    params<span style="color:#f92672">.</span>strengths <span style="color:#f92672">=</span> (x,y) <span style="color:#f92672">-&gt;</span> score_probs(x,y) <span style="color:#f92672">*</span> pdf(defaultStrengths, x) <span style="color:#f92672">*</span> pdf(defaultStrengths, y) <span style="color:#f92672">/</span> params<span style="color:#f92672">.</span>strengths_Z
</span></span><span style="display:flex;"><span>    normalize_strengths!(params)
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">end</span>
</span></span></code></pre></div><p>The real roadblock, however, is actually sampling the $\lambda$ and $X$ values from the pdfs.</p>
<h3 id="sampling-game-parameters">Sampling Game Parameters</h3>
<p>Since we have access to the pdfs (even their normalizing constants are quite easy to compute using numeric methods), we can employ importance sampling as a brute force method. I am sure that there are fancier MCMC algorithms that could be used, but the unfriendly distribution of the $X$ values made it hard for me to use external libraries like <code>Turing.jl</code>.</p>
<p>Anyhow, for interested readers, the reason we can use importance sampling to compute the expected value of a function $g$ with respect to a pdf $f$ using another pdf $h$ is because of the following:
</p>
$$\begin{align}
    \underset{X \sim f}{\mathbb{E}}[g(X)] &= \int g(x) f(x) dx \\
    &= \int g(x) \frac{f(x)}{h(x)} h(x) dx \\
    &= \underset{X \sim h}{\mathbb{E}}\left[\frac{f(X)}{h(X)} g(X)\right]
\end{align}$$<p>
Which also tells us that $h$ has the condition that it must be non-zero wherever $f$ is non-zero. When working with empricial calulations, the term $\frac{f(x)}{h(x)}$ is referred to as the weight of the sample for obvious reasons.</p>
<p>So, for our empirical estimations a good choice for $h$ is the prior distributions. The following code shows the implementation of the sampling functions:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-julia" data-lang="julia"><span style="display:flex;"><span><span style="color:#66d9ef">function</span> sample_params(game, n)
</span></span><span style="display:flex;"><span>    r <span style="color:#f92672">=</span> rand(defaultRate, n)
</span></span><span style="display:flex;"><span>    wr <span style="color:#f92672">=</span> game<span style="color:#f92672">.</span>params<span style="color:#f92672">.</span>rate<span style="color:#f92672">.</span>(r) <span style="color:#f92672">./</span> pdf(defaultRate, r)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    s <span style="color:#f92672">=</span> rand(defaultStrengths, n, <span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>    ws <span style="color:#f92672">=</span> game<span style="color:#f92672">.</span>params<span style="color:#f92672">.</span>strengths<span style="color:#f92672">.</span>(s[<span style="color:#f92672">:</span>,<span style="color:#ae81ff">1</span>], s[<span style="color:#f92672">:</span>,<span style="color:#ae81ff">2</span>]) <span style="color:#f92672">./</span> (pdf(defaultStrengths, s[<span style="color:#f92672">:</span>,<span style="color:#ae81ff">1</span>]) <span style="color:#f92672">.*</span> pdf(defaultStrengths, s[<span style="color:#f92672">:</span>,<span style="color:#ae81ff">2</span>]))
</span></span><span style="display:flex;"><span>    w <span style="color:#f92672">=</span> wr <span style="color:#f92672">.*</span> ws
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> r, s, w
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">end</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">function</span> sample_games(game, n<span style="color:#f92672">=</span><span style="color:#ae81ff">1000</span>, k<span style="color:#f92672">=</span><span style="color:#ae81ff">1000</span>)
</span></span><span style="display:flex;"><span>    results <span style="color:#f92672">=</span> zeros(n)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> i <span style="color:#66d9ef">in</span> <span style="color:#ae81ff">1</span><span style="color:#f92672">:</span>n
</span></span><span style="display:flex;"><span>        r, s, w <span style="color:#f92672">=</span> sample_params(game, k)
</span></span><span style="display:flex;"><span>        sample_results <span style="color:#f92672">=</span> zeros(k)
</span></span><span style="display:flex;"><span>        Threads<span style="color:#f92672">.</span><span style="color:#a6e22e">@threads</span> <span style="color:#66d9ef">for</span> j <span style="color:#66d9ef">in</span> <span style="color:#ae81ff">1</span><span style="color:#f92672">:</span>k
</span></span><span style="display:flex;"><span>            X <span style="color:#f92672">=</span> s[j, <span style="color:#ae81ff">1</span>]
</span></span><span style="display:flex;"><span>            Y <span style="color:#f92672">=</span> s[j, <span style="color:#ae81ff">2</span>]
</span></span><span style="display:flex;"><span>            sample_results[j] <span style="color:#f92672">=</span> simulate_game(game, r[j], X, Y) <span style="color:#f92672">*</span> w[j]
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">end</span>
</span></span><span style="display:flex;"><span>        results[i] <span style="color:#f92672">=</span> sum(sample_results) <span style="color:#f92672">/</span> k
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">end</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> sum(results)<span style="color:#f92672">/</span>n
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">end</span>
</span></span></code></pre></div><h3 id="simulating-games">Simulating Games</h3>
<p>The final step is to simulate the games. This is quite easy to do if we are able to pass in all the parameters we need.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-julia" data-lang="julia"><span style="display:flex;"><span><span style="color:#66d9ef">function</span> simulate_game(game, lambda, Xa, Xb)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> length(game<span style="color:#f92672">.</span>plays) <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>        t <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>        s <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>        r <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">else</span>
</span></span><span style="display:flex;"><span>        t <span style="color:#f92672">=</span> game<span style="color:#f92672">.</span>plays[<span style="color:#66d9ef">end</span>][<span style="color:#ae81ff">1</span>]
</span></span><span style="display:flex;"><span>        s <span style="color:#f92672">=</span> net_score(game)
</span></span><span style="display:flex;"><span>        r <span style="color:#f92672">=</span> sign(game<span style="color:#f92672">.</span>plays[<span style="color:#66d9ef">end</span>][<span style="color:#ae81ff">2</span>])
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">end</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">while</span> t <span style="color:#f92672">&lt;</span> <span style="color:#ae81ff">2880</span>
</span></span><span style="display:flex;"><span>        t <span style="color:#f92672">+=</span> rand(Exponential(<span style="color:#ae81ff">1</span><span style="color:#f92672">/</span>lambda)) <span style="color:#f92672">*</span> <span style="color:#ae81ff">30</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> rand() <span style="color:#f92672">&lt;</span> score_prob((<span style="color:#ae81ff">1</span>, r, s), Xa, Xb)
</span></span><span style="display:flex;"><span>            s <span style="color:#f92672">+=</span> random_play()
</span></span><span style="display:flex;"><span>            r <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">else</span>
</span></span><span style="display:flex;"><span>            s <span style="color:#f92672">-=</span> random_play()
</span></span><span style="display:flex;"><span>            r <span style="color:#f92672">=</span> <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">end</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">end</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> (sign(s)<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>)<span style="color:#f92672">/</span><span style="color:#ae81ff">2</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">end</span>
</span></span></code></pre></div><h2 id="results">Results</h2>
<h3 id="observations-of-the-model">Observations of the Model</h3>
<p>The above code snippets allowed me to peer into quite a few example games, and gave me the following conclusions about how baksetball random walks work:</p>
<ol>
<li><strong>Strengths Don&rsquo;t Dominate:</strong> Dr. Redner mentioned that it would be quite difficult to correctly predict the strengths of the teams given game data, and seeing as the bayesian updates hardly change the prior, I&rsquo;ll have to agree</li>
<li><strong>The Games are Relatively Uniform:</strong> Even though the distribution for $\lambda$ did visually show significant updates throughout the game, the resulting probabilities hardly shifted&ndash;meaning that we will not be able to differentiate between most games.</li>
<li><strong>The Arcsine Law:</strong> The biggest factor which determines who wins is the current team that is leading. This is in agreement with the <a href="/posts/notes/eliasa/chap6/6-1/#arcsine-law">arcsine law</a> which states that a random walk is most likely to spend its time on one side of the origin.</li>
</ol>
<h3 id="application">Application</h3>
<p>Due to the performant nature of the code (thanks <code>Julia</code>!), it made sense to spin up website with a simpler version of the model (no bayesian updates since they hardly made a difference and it&rsquo;d be a lot of work for the user to input each play). This way, someone betting on a game can make a mathematically-backed decision on how to spend their money!
<figure class="align-center ">
    <img loading="lazy" src="./images/webapp.PNG#center"
         alt="The website takes in the scores of the teams and the time elapsed to calculate the odds that a team will win (the lower the better)" width="600px"/> <figcaption>
            <p>The website takes in the scores of the teams and the time elapsed to calculate the odds that a team will win (the lower the better)</p>
        </figcaption>
</figure>
</p>
<p>The application computes the odds for a team to win. In other words, it outputs the inverse probability for a team to win, so the closer it is to 1, the more likely that team is to win, and vice versa.</p>
<p>If a bookie offers a payout multiplier that is highger than the calcualted odds, it might be a good idea to buy it because we are prdicting that the team is more likely to win than the bookie thinks (thus, the bookie overpriced the payout).</p>
<p>I cannot host the application myself, but you can find the code for it&ndash;along with the instructions to run it&ndash;in the github repository [<a href="https://github.com/hasithv/nba-odds">2</a>].</p>
<h2 id="references">References</h2>
<ol>
<li>Gabel, A., Redner, S., &ldquo;Random Walk Picture of Basketball Scoring,&rdquo; <a href="https://arxiv.org/abs/1109.2825v1">arXiv:1109.2825v1</a> (2011).</li>
<li>Vattikuti, V., &ldquo;NBA Odds,&rdquo; <a href="https://github.com/hasithv/nba-odds">github.com/hasithv/nba-odds</a> (2024).</li>
</ol>
]]></content:encoded>
    </item>
    <item>
      <title>6.2 - The Invariance Principle</title>
      <link>https://hasithv.github.io/posts/notes/eliasa/chap6/6-2/</link>
      <pubDate>Mon, 12 Aug 2024 00:29:17 -0500</pubDate>
      <guid>https://hasithv.github.io/posts/notes/eliasa/chap6/6-2/</guid>
      <description>&lt;p&gt;Let $\{\xi_m\}_{n \in \mathbb{N}}$ be a sequence of i.i.d. random variables such that $\mathbb{E}[\xi_n] = 0$ and $\mathbb{E}[\xi_n^2] = 1$. Then, define
&lt;/p&gt;
$$S_0 = 0, \quad S_N = \sum_{i=1}^N \xi_i$$&lt;p&gt;and by the Central Limit Theorem, rescaling $S_N$ by $\sqrt{N}$, we get that
&lt;/p&gt;
$$\frac{S_N}{\sqrt{N}} \xrightarrow{d} \mathcal{N}(0,1)$$&lt;p&gt;
(the $\xrightarrow{d}$ means convergence in distribution) as $N \rightarrow \infty$. Using this, we can define a continuous random function $W^N_t$ on $t \in [0,1]$ such that $W_0^N = 0$ and
&lt;/p&gt;</description>
      <content:encoded><![CDATA[<p>Let $\{\xi_m\}_{n \in \mathbb{N}}$ be a sequence of i.i.d. random variables such that $\mathbb{E}[\xi_n] = 0$ and $\mathbb{E}[\xi_n^2] = 1$. Then, define
</p>
$$S_0 = 0, \quad S_N = \sum_{i=1}^N \xi_i$$<p>and by the Central Limit Theorem, rescaling $S_N$ by $\sqrt{N}$, we get that
</p>
$$\frac{S_N}{\sqrt{N}} \xrightarrow{d} \mathcal{N}(0,1)$$<p>
(the $\xrightarrow{d}$ means convergence in distribution) as $N \rightarrow \infty$. Using this, we can define a continuous random function $W^N_t$ on $t \in [0,1]$ such that $W_0^N = 0$ and
</p>
$$W_t^N = \frac{1}{\sqrt{N}}(\theta S_k + (1-\theta)S_{k+1}), \quad Nt \in (k,k+1], \quad k = 0,1,\ldots,N-1$$<p>where $\theta = \lceil Nt \rceil - Nt$. Essentially, this equation is just a linear interpolation between the points of $S_N$ and $S_{N+1}$. The rescaling factor of $\frac{1}{\sqrt{N}}$ is to ensure that the variance of $W_t^N$ is 1 after 1 second.</p>
<p>Now, we will define the Wiener process.</p>
<blockquote>
<p><strong>Theorem 6.4:</strong><em>(Wiener Process)</em> As $N \rightarrow \infty$,
</p>
$$W^N \xrightarrow{d} W$$<p>
where $W$ is the Wiener process and the distribution of $W$ on $\Omega \in C[0,1]$ is called the Wiener measure.</p>
</blockquote>
]]></content:encoded>
    </item>
    <item>
      <title>A Simple Boarding Puzzle</title>
      <link>https://hasithv.github.io/posts/24-01-11-asimpleboardingpuzzle/</link>
      <pubDate>Mon, 12 Aug 2024 00:00:38 -0500</pubDate>
      <guid>https://hasithv.github.io/posts/24-01-11-asimpleboardingpuzzle/</guid>
      <description>&lt;h2 id=&#34;the-puzzle&#34;&gt;The Puzzle&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Inspired by true events&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Alice is assigned to be the 56th passenger to board a full plane with 60 seats. However, a panic causes all the passengers&amp;ndash;including Alice&amp;ndash;to arrange themselves radomly in line to board.&lt;/p&gt;
&lt;p&gt;As Alice was originally 56th, she decides that she would be happy as long as passengers with the assigned spots 57, 58, 59, and 60 are not in front of her. What is the probability that Alice will be happy?&lt;/p&gt;</description>
      <content:encoded><![CDATA[<h2 id="the-puzzle">The Puzzle</h2>
<p><em>Inspired by true events</em></p>
<p>Alice is assigned to be the 56th passenger to board a full plane with 60 seats. However, a panic causes all the passengers&ndash;including Alice&ndash;to arrange themselves radomly in line to board.</p>
<p>As Alice was originally 56th, she decides that she would be happy as long as passengers with the assigned spots 57, 58, 59, and 60 are not in front of her. What is the probability that Alice will be happy?</p>
<h2 id="solution">Solution</h2>
<p>The only passengers we need to consider are Alice and passengers 57, 58, 59, and 60&ndash;since for each arrangement of these 5 passengers, there is an equal amount of ways to arrange all other 55 passengers.</p>
<p>Thus, the probability will simply the number of ways we can arrange these 5 passengers such that Alice is in front of passengers 57, 58, 59, and 60 divided by the total number of ways to arrange these 5 passengers:</p>
$$ 4!/5! = 1/5$$]]></content:encoded>
    </item>
    <item>
      <title>6.1 - The Diffusion Limit of Random Walks</title>
      <link>https://hasithv.github.io/posts/notes/eliasa/chap6/6-1/</link>
      <pubDate>Sat, 10 Aug 2024 15:41:44 -0700</pubDate>
      <guid>https://hasithv.github.io/posts/notes/eliasa/chap6/6-1/</guid>
      <description>&lt;h2 id=&#34;random-walk&#34;&gt;Random Walk&lt;/h2&gt;
&lt;p&gt;Let $\{\xi_i\}$ be i.i.d. random variables such that $\xi_i = \pm 1$ with probability $1/2$. Then, define
&lt;/p&gt;
$$X_n = \sum_{k=1}^{n} \xi_k, \quad X_0 = 0.$$&lt;p&gt;
$\{X_n\}$ is the familiar symmetric random walk on $\mathbb{Z}$. Let $W(m,n) = \mathbb{P}(X_N = m)$. It is easy to see that
&lt;/p&gt;
$$W(m,n) = {N \choose (N+m)/2} \left( \frac{1}{2} \right)^N$$&lt;p&gt;
and that the mean and std are
&lt;/p&gt;
$$\mathbb{E}[X_N] = 0, \quad \sigma^2_{X_N} = N$$&lt;h2 id=&#34;diffusion-coefficient&#34;&gt;Diffusion Coefficient&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition 6.2:&lt;/strong&gt; &lt;em&gt;(Diffusion coefficient)&lt;/em&gt;. The diffusion coefficient $D$ is defined as
&lt;/p&gt;</description>
      <content:encoded><![CDATA[<h2 id="random-walk">Random Walk</h2>
<p>Let $\{\xi_i\}$ be i.i.d. random variables such that $\xi_i = \pm 1$ with probability $1/2$. Then, define
</p>
$$X_n = \sum_{k=1}^{n} \xi_k, \quad X_0 = 0.$$<p>
$\{X_n\}$ is the familiar symmetric random walk on $\mathbb{Z}$. Let $W(m,n) = \mathbb{P}(X_N = m)$. It is easy to see that
</p>
$$W(m,n) = {N \choose (N+m)/2} \left( \frac{1}{2} \right)^N$$<p>
and that the mean and std are
</p>
$$\mathbb{E}[X_N] = 0, \quad \sigma^2_{X_N} = N$$<h2 id="diffusion-coefficient">Diffusion Coefficient</h2>
<blockquote>
<p><strong>Definition 6.2:</strong> <em>(Diffusion coefficient)</em>. The diffusion coefficient $D$ is defined as
</p>
$$D = \frac{\langle (X_N - X_0)^2 \rangle}{2N}$$<p>
And for a general stochastic process it is
</p>
$$D = \lim_{t \rightarrow \infty} \frac{\langle (X_t - X_0)^2 \rangle}{2dt}$$<p>
where $d$ is the space dimension.</p>
</blockquote>
<p>Intuitively, the diffusion coefficient tells us the rate at which variance changes [<a href="https://physics.stackexchange.com/a/52977/250863">1</a>]. For the random walk, $D = 1/2$.</p>
<h2 id="continuum-limit-of-the-random-walk">Continuum limit of the random walk</h2>
<p>Let&rsquo;s define the step length of the random walk to be $l$ and the timestep to be $\tau$. Now, fixing $(x,t)$ consider the following limit:
</p>
$$N,m \rightarrow \infty, \quad l,\tau \rightarrow 0, \quad N\tau = t, \quad ml=x$$<p>The diffusion coefficient is then computed as (with $d=1$)
</p>
$$\begin{align} 
D &= \lim_{t \rightarrow \infty}  \frac{\langle (X_t - X_0)^2 \rangle}{2t} \\
&= \lim_{t \rightarrow \infty}  \frac{\langle (X_{N\tau} - X_0)^2 \rangle}{2N\tau} \\
&= \lim_{t \rightarrow \infty}  \frac{\langle X_{N\tau}^2 \rangle}{2N\tau} \\
&= \lim_{t \rightarrow \infty}  \frac{N l^2}{2N\tau} \\
&= \frac{l^2}{2\tau} \\
\end{align}$$<p>
&#x26a0;&#xfe0f; the book mentions &ldquo;fixing&rdquo; the diffusion constant, is that different from the computation above?</p>
<p>When we are taking the continuum limit of the random walk, $N, m \gg 1$, and so $m/N = (x/l) (\tau/t) = (x/t) (\tau/l) \rightarrow 0$. Thus, $m \ll N$. Now, we can expand $W(m,N)$ using Stirlings formula $\log n! = (n+\frac{1}{2})\log n - n + \frac{1}{2} \log 2\pi + O(n^{-1})$ when $n \gg 1$
</p>
$$\begin{align}
W(m,N) &= \frac{N!}{\left(\frac{N+m}{2}\right)! \left(\frac{N-m}{2}\right)!} \left(\frac{1}{2}\right)^N \\
\log W(m,n) &= \log \left( \frac{N!}{\left(\frac{N+m}{2}\right)! \left(\frac{N-m}{2}\right)!} \left(\frac{1}{2}\right)^N \right) \\
&\approx \log N! - N\log 2 - \left(\log\left(\frac{N + m}{2}\right)! + \log\left(\frac{N-m}{2}\right)! \right) \\
&\approx \left(N+\frac{1}{2}\right)\log N - N + \frac{1}{2}\log 2\pi - N\log 2 \\ 
& \quad \quad - \left(\log\left(\frac{N + m}{2}\right)! + \log\left(\frac{N-m}{2}\right)! \right) \\
&\approx \left(N+\frac{1}{2}\right)\log N - N + \frac{1}{2}\log 2\pi - N\log 2 \\ 
& \quad \quad - \frac{1}{2} \left( N + m + 1 \right) \log \left(\frac{N+m}{2}\right) + \frac{N+m}{2} - \frac{1}{2}\log 2\pi \\
& \quad \quad - \frac{1}{2} \left( N - m + 1 \right) \log \left(\frac{N-m}{2}\right) + \frac{N-m}{2} - \frac{1}{2}\log 2\pi \\
&\approx \left(N+\frac{1}{2}\right)\log N \\ 
& \quad \quad - \frac{1}{2} \left( N + m + 1 \right) \log \left( \frac{N}{2}\left(1+\frac{m}{N}\right)\right) \\
& \quad \quad - \frac{1}{2} \left( N - m + 1 \right) \log \left(\frac{N}{2}\left( 1 - \frac{m}{N} \right)\right) \\
& \quad \quad -\frac{1}{2}\log 2\pi - N\log 2
\end{align}$$<p>
And now using that $m \ll N$ and $\log(1+x) \approx x - \frac{1}{2}x^2$ for $x \ll 1$, we have
</p>
$$\begin{align}
\log W(m,N) &\approx \left(N+\frac{1}{2}\right) \log N - (N+1) \log\left(\frac{N}{2}\right) \\
& \quad \quad - \frac{1}{2} \left( N + m + 1 \right) \log \left(1+\frac{m}{N}\right) \\
& \quad \quad - \frac{1}{2} \left( N - m + 1 \right) \log \left( 1 - \frac{m}{N} \right)\\
& \quad \quad -\frac{1}{2}\log 2\pi - N\log 2 \\
&\approx -\frac{1}{2} \log N + \log(2) - \frac{1}{2}\log 2\pi \\
& \quad \quad - \frac{1}{2} \left( N + m + 1 \right) \log \left(1+\frac{m}{N}\right) \\
& \quad \quad - \frac{1}{2} \left( N - m + 1 \right) \log \left( 1 - \frac{m}{N} \right) \\
&\approx -\frac{1}{2} \log N + \log(2) - \frac{1}{2}\log 2\pi \\
& \quad \quad - \frac{1}{2} \left( N + m + 1 \right) \left( \frac{m}{N} - \frac{1}{2}\left(\frac{m}{N}\right)^2\right) \\
& \quad \quad - \frac{1}{2} \left( N - m + 1 \right) \left( -\frac{m}{N} - \frac{1}{2}\left(\frac{m}{N}\right)^2\right) \\
&\approx -\frac{1}{2} \log N + \log(2) - \frac{1}{2}\log 2\pi - \frac{m^2}{2N} \\
\end{align}$$$$\implies  W(m,N) \approx \left( \frac{2}{\pi N} \right)^{1/2}\exp\left(-\frac{m^2}{2N}\right) $$<p>
If we integrate across the possible $x$ values,
</p>
$$\begin{align}
W(x,t)\Delta x &\approx \int_{x-\Delta/2}^{x+\Delta/2} W(y, t) dy \\
&\approx \sum_{\substack{k = \{m, m \pm 2, m \pm 4, \ldots\} \\ kl \in (x-\Delta x/2, x+\Delta x/2)}} W(k,N) \\
&\approx W(m,N)\frac{\Delta x}{2m}
\end{align}$$<p>
Where $x=ml$. Then, doing some substitions, we get
</p>
$$W(x,t) = \frac{1}{\sqrt{4\pi D t} } \exp\left(-\frac{x^2}{4Dt}\right)$$<p>
It is interesting to see that $W$ satisfies the heat equation with the initial condition
</p>
$$\begin{cases}
    \frac{\partial W(x,t)}{\partial t} = D \frac{\partial^2 W(x,t)}{\partial t^2} \\
    W(x,0) = \delta(x)
\end{cases}$$<p>
I wonder how it satisfies the boundary condition of maintaining a constant area under the graph for all time $t$.</p>
<h2 id="arcsine-law">Arcsine Law</h2>
<p>Define $P_{2k,2n}$ to be the probability that a particle remains positive for $2k$ time steps before $2n$ time steps have passed. And a particle is on the positive side in an interval $[n-1,n]$ if either $X_{n-1}$ or $X_{n}$ are positive. It is true that
</p>
$$P_{2k,2n} = u_{2k}u_{2n-2k}$$<p>
where $u_{2k} = \mathbb{P}(X_{2k} = 0)$ (&#x26a0;&#xfe0f; the proof is non-trivial, and not too instructive so it is omitted. Though, if there is an intuitive reason, I&rsquo;d like to hear it) [<a href="https://math.osu.edu/sites/math.osu.edu/files/What_is_2018_Arcsine_Law.pdf">2</a>]. Now, let $\gamma(2n)$ be the number of time units that the particle spends on the positive axis in the interval $[0,2n]$. Then when $x \leq 1$,
</p>
$$\mathbb{P}\left(\frac{1}{2} < \frac{\gamma (2n)}{2n} \leq x \right) = \sum_{k,1/2<2k/2n\leq x} P_{2k,2n}$$<p>
From the expression of $W(m,N)$,
</p>
$$u_{2k} \sim \frac{1}{\sqrt{\pi k}}, \quad P_{2k,2n} \sim \frac{1}{\pi \sqrt{k(n-k)}}$$<p>
as $k, n-k \rightarrow \infty$. And so,
</p>
$$\begin{align}
\mathbb{P}\left(\frac{1}{2} < \frac{\gamma (2n)}{2n} \leq x \right) &= \sum_{k,1/2<2k/2n\leq x} P_{2k,2n} \\
&= \sum_{k,1/2<2k/2n\leq x}  \frac{1}{\pi n \sqrt{(k/n)(1-k/n)}} \\
&\rightarrow \frac{1}{\pi} \int_\frac{1}{2}^x \frac{dt}{\sqrt{t(1-t)}}
\end{align}$$<p>Which leads us to</p>
<blockquote>
<p><strong>Theorem 6.3:</strong> <em>(Arcsine law).</em> The probability that the fraction of time spenty by a particle ont he positive side is at most $x$ tends to $\frac{2}{\pi}\arcsin \sqrt{x}$:
</p>
$$ \mathbb{P}\left(\frac{\gamma (2n)}{2n} \leq x \right) = \frac{2}{\pi} \arcsin \sqrt{x} $$<p>
The consequence of this theorem is that it is most likely for a radnom walk to spend either almost all of its time on the positive side, or for it to spend almost no time on the positive side.</p>
<p>We can do this because there was no reason to limit the lower bound to $1/2$ rather than $0$ in the derivation</p>
</blockquote>
<h2 id="references">References</h2>
<ol>
<li>Helena (<a href="https://physics.stackexchange.com/users/12948/helena)">https://physics.stackexchange.com/users/12948/helena)</a>, What is the physical meaning of diffusion coefficient?, URL (version: 2014-12-03): <a href="https://physics.stackexchange.com/q/52977https://physics.stackexchange.com/a/52977/250863">https://physics.stackexchange.com/q/52977https://physics.stackexchange.com/a/52977/250863</a></li>
<li>Ackelsberg, Ethan (<a href="https://math.osu.edu/sites/math.osu.edu/files/What_is_2018_Arcsine_Law.pdf)">https://math.osu.edu/sites/math.osu.edu/files/What_is_2018_Arcsine_Law.pdf)</a>, What is the Arcsine Law?, URL (version: 2024-08-11): <a href="https://math.osu.edu/sites/math.osu.edu/files/What_is_2018_Arcsine_Law.pdf">https://math.osu.edu/sites/math.osu.edu/files/What_is_2018_Arcsine_Law.pdf</a></li>
</ol>
]]></content:encoded>
    </item>
    <item>
      <title>5.4 - Gaussian Processes</title>
      <link>https://hasithv.github.io/posts/notes/eliasa/chap5/5-4/</link>
      <pubDate>Tue, 06 Aug 2024 21:17:49 -0700</pubDate>
      <guid>https://hasithv.github.io/posts/notes/eliasa/chap5/5-4/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition 5.9:&lt;/strong&gt; A stochasitc process $\{X_t\}_{t \geq 0}$ is a &lt;em&gt;Gaussian Process&lt;/em&gt; if its finite dimensional distributions are consistent Gaussian measures for any $0 \leq t_1 &lt; t_2 &lt; \ldots &lt; t_k$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Recall that a Gaussian random vector $\mathbf{X} = (X_1, X_2,\ldots,X_n)^T$ is completely characterized by its first and second moments
&lt;/p&gt;
$$\mathbf{m} = \mathbb{E}[\mathbf{X}], \quad \mathbf{K} = \mathbb{E}[(\mathbf{X} - \mathbf{m}) (\mathbf{X} - \mathbf{m})^T]$$&lt;p&gt;Meaning that the characteristic function is expressed only in terms of $\mathbf{m}$ and $\mathbf{K}$
&lt;/p&gt;</description>
      <content:encoded><![CDATA[<blockquote>
<p><strong>Definition 5.9:</strong> A stochasitc process $\{X_t\}_{t \geq 0}$ is a <em>Gaussian Process</em> if its finite dimensional distributions are consistent Gaussian measures for any $0 \leq t_1 < t_2 < \ldots < t_k$.</p>
</blockquote>
<p>Recall that a Gaussian random vector $\mathbf{X} = (X_1, X_2,\ldots,X_n)^T$ is completely characterized by its first and second moments
</p>
$$\mathbf{m} = \mathbb{E}[\mathbf{X}], \quad \mathbf{K} = \mathbb{E}[(\mathbf{X} - \mathbf{m}) (\mathbf{X} - \mathbf{m})^T]$$<p>Meaning that the characteristic function is expressed only in terms of $\mathbf{m}$ and $\mathbf{K}$
</p>
$$\mathbb{E}\left[e^{i \mathbf{\xi} \cdot \mathbf{X}}\right] = e^{i \mathbf{\xi} \cdot \mathbf{m} - \frac{1}{2}\mathbf{\xi}^T \mathbf{K} \mathbf{\xi}} $$<p>
This means that for any $0 \leq t_1 < t_2 < \ldots < t_k$, the measure $\mu_{t_1, t_2, \ldots, t_k}$ is uniquely determined by an $\mathbf{m} = (m(t_1), \ldots, m(t_k))$ and a covariance matrix $\mathbf{K}_{ij} = K(t_i, t_j)$. Because our $\mu$ satisifes the conditions for Kolomorov&rsquo;s extension theorem, we have a probability space and a stochastic process associated with $\mu$.</p>
<p>&#x26a0;&#xfe0f; Isn&rsquo;t one of the conditions of Kolmogorov&rsquo;s extension theoren that we need to be able to permute the $t_i$? How would this work if we require the $t_i$ to be increasing?</p>
<blockquote>
<p><strong>Theorem 5.10:</strong> Assuming that the stochastic process $\{X_t\}_{t\in[0,T]}$ satisfies
</p>
$$\mathbb{E} \left[ \int_0^T X_t^2 dt \right] < \infty$$<p>
then $m \in L^2_t$. Also, the operator
</p>
$$\mathcal{K} f(s) := \int_0^T K(s,t) f(t) dt$$<p>
is nonnegative, compact on $L^2_t$</p>
</blockquote>
<blockquote>
<p><strong>Proof:</strong> For the first statement,
</p>
$$\int_0^T \mathbb{E}[X]^2 dt \leq \int_0^T \mathbb{E}[X_t^2] dt < \infty$$<p>
For the second,
</p>
$$\begin{align}
\int_0^T \int_0^T K^2(s,t) ds dt &= \int_0^T \int_0^T \mathbb{E}[\left((X_t - m(t))(X_s - m(s))\right)^2]ds dt \\
&\leq \int_0^T \int_0^T \mathbb{E}[(X_t - m(t))^2]\mathbb{E}[(X_s - m(s))^2]ds dt \\
&\leq \left( \int_0^T \mathbb{E}[X_t] dt \right) \\
&\leq \infty 
\end{align}$$<p>
which lets us conclude that $K \in L^2([0,T] \times [0,T])$, which tells us that $\mathcal{K}$ is compact on $L^2_t$</p>
<p>&#x26a0;&#xfe0f; I can&rsquo;t properly find what theorem lets us say the last statement, but I can trust it for now.</p>
<p>Furthermore, noting that $K$ is symmetric which means that $\mathcal{K}$ is self adjoint, and we can say
</p>
$$(\mathcal{K}f, f) = \int_0^T \int_0^T \mathbb{E}[(X_t - m(t))]\mathbb{E}[(X_s - m(s))]f(t)f(s) ds dt \geq 0$$<p>
by symmetry of $s$ and $t$.</p>
</blockquote>
<p>If we want to extend the characteristic to $L_t$ rather than the finite dimensional version, we can write
</p>
$$\mathbb{E}[e^{i(\xi,X)}] = e^{i(\xi,m) - \frac{1}{2} (\xi, \mathcal{K} \xi)}, \quad \xi \in L^2_t$$<p>
With $(\xi,m) = \int_a^b \xi(t) m(t) dt$ and $\mathcal{K}\xi (t) = \int_a^b K(t,s) \xi(s) ds$. This is a fairly reasonable extrapolation from the finite dimensional case.</p>
<blockquote>
<p><strong>Theorem 5.13:</strong> <em>Karhunen-Loeve expansion</em>. Let $(X_t)_{t \in [0,1]}$ be a Gaussian process with mean 0 and covariance function $K(s,t)$, Assume that $K$ continuous and $\{\lambda_k\}$ be the set of eigenvalues for orthonormal eigenfunctions of $K$, $\{\phi_k\}$. Then, $X_t$ has the representation of
</p>
$$X_t = \sum_{k=1}^\infty \alpha_k \sqrt{\lambda_k} \phi_k(t)$$<p>
Where $\alpha_k$ is a standard normal random variable $\mathscr{N}(0,1)$</p>
<p>&#x26a0;&#xfe0f; I am omitting the proof because I feel the result is easy enough to intuitively grasp, and also it is a little theoretical, so maybe I should revisit it if I get more comfortable with proving covergence in probability.</p>
</blockquote>
]]></content:encoded>
    </item>
    <item>
      <title>5.3 - Markov Processes</title>
      <link>https://hasithv.github.io/posts/notes/eliasa/chap5/5-3/</link>
      <pubDate>Sat, 03 Aug 2024 23:22:06 -0700</pubDate>
      <guid>https://hasithv.github.io/posts/notes/eliasa/chap5/5-3/</guid>
      <description>&lt;h2 id=&#34;markov-processes-in-continuous-time-and-space&#34;&gt;Markov processes in continuous time and space&lt;/h2&gt;
&lt;p&gt;Given a probability space $(\Omega, \mathcal{F}, \mathbb{P})$ and the filtration $\mathbb{F} = (\mathcal{F}_t)_{t \geq 0}$, a stochastic process $X_t$ is called a Markov process wrt $\mathcal{F}_t$ if&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$X_t$ is $\mathcal{F}_t$-adapted&lt;/li&gt;
&lt;li&gt;For any $t \geq s$ and $B \in \mathcal{R}$, we have
$$\mathbb{P}(X_t \in B | \mathcal{F}_s) = \mathbb{P}(X_t \in B | X_s)$$
Essentially, this is saying that history doesn&amp;rsquo;t matter, only the current state matters. We can associate a family of probability measures $\{\mathbb{P}^x\}_{x\in\mathbb{R}}$ for the processes starting at $x$ by defining $\mu_0$ to be the point mass at $x$. Then, we still have
$$\mathbb{P}^x(X_t \in B | \mathcal{F}_s) = \mathbb{P}^x(X_t \in B | X_s), \quad t \geq s$$
and $\mathbb{E}[f(X_0)] = f(x)$ for any function $f \in C(\mathbb{R})$.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&amp;#x26a0;&amp;#xfe0f; I am not fully confident on what the above section is saying. Specifically, I am having trouble with understanding how we are defining $\mathbb{P}^x$. However, I can understand the strong markov property, so I think I should be okay moving forward.&lt;/p&gt;</description>
      <content:encoded><![CDATA[<h2 id="markov-processes-in-continuous-time-and-space">Markov processes in continuous time and space</h2>
<p>Given a probability space $(\Omega, \mathcal{F}, \mathbb{P})$ and the filtration $\mathbb{F} = (\mathcal{F}_t)_{t \geq 0}$, a stochastic process $X_t$ is called a Markov process wrt $\mathcal{F}_t$ if</p>
<ol>
<li>$X_t$ is $\mathcal{F}_t$-adapted</li>
<li>For any $t \geq s$ and $B \in \mathcal{R}$, we have
$$\mathbb{P}(X_t \in B | \mathcal{F}_s) = \mathbb{P}(X_t \in B | X_s)$$
Essentially, this is saying that history doesn&rsquo;t matter, only the current state matters. We can associate a family of probability measures $\{\mathbb{P}^x\}_{x\in\mathbb{R}}$ for the processes starting at $x$ by defining $\mu_0$ to be the point mass at $x$. Then, we still have
$$\mathbb{P}^x(X_t \in B | \mathcal{F}_s) = \mathbb{P}^x(X_t \in B | X_s), \quad t \geq s$$
and $\mathbb{E}[f(X_0)] = f(x)$ for any function $f \in C(\mathbb{R})$.</li>
</ol>
<p>&#x26a0;&#xfe0f; I am not fully confident on what the above section is saying. Specifically, I am having trouble with understanding how we are defining $\mathbb{P}^x$. However, I can understand the strong markov property, so I think I should be okay moving forward.</p>
<p>The <em>transition function</em> of a Markov process is defined as
</p>
$$p(B,t|x,s) = \mathbb{P}(X_t \in B | X_s = x)$$<p>
and it has the properties</p>
<ol>
<li>$p(.,t|x,s)$ is a probability measure on $\mathcal{R}$</li>
<li>$p(B,t|.,s)$ is a measurable function on $\mathbb{R}$</li>
<li>$p$ satisfies
$$p(B,t|y,s) = \int_\mathbb{R} p(B,t|y,u) p(dy,u|x,s), \quad s \leq u \leq t$$
The last property is the continuous analog of the <em>Chapman-Kolmogorov equation</em>, and it essentially lets us break the transition function into two the transition from $s$ to $u$ and from $u$ to $t$.</li>
</ol>
<p>Now, we can write the expenctation from $x$ as
</p>
$$ \begin{multline}
\mathbb{E}^x[f(X_{t_1}, X_{t_2}, \ldots, X_{t_n})] = \int_\mathbb{R} \ldots \int_\mathbb{R} f(x_1,x_2,\ldots,x_n) p(dx_n,t_n|x_{n-1},t_{n-1}) \\ \ldots p(dx_2, t_2 | x_1,t_1) p(dx_1, t_1|x,0)
\end{multline}
$$<p>when $t_n$ are strictly increasing.</p>
<p>$p(y,t|x,s)$ is a <em>transition density function</em> of $X$. &#x26a0;&#xfe0f; The book makes it seem like this is not always the case, but I fail to see when it isn&rsquo;t.</p>
<p>A stochastic process is <em>stationary</em> if the joint distributions are translation invariant in time. However, if the process only depends on the difference between time, then the process is <em>homogeneous</em>. The difference is that a stationary process has the same distribution at all times, while a homogeneous process has the same distribution for all time differences.</p>
<p>&#x26a0;&#xfe0f; at this point, the book dives into semigroup theory which I know nothing about, so I will skip this section for now.</p>
<h2 id="example-57---q-process">Example 5.7 - Q-process</h2>
<p>Recall the definition of a generator $\mathbf{Q}$ to be
</p>
$$\mathbf{Q} = \lim_{h \rightarrow 0+} \frac{1}{h} (\mathbf(P)(h) - \mathbf{I})$$<p>
Now, we will define an infinitesimal generator $\mathcal{A}$ on a sample space $S = \{1,2,\ldots,I\}$:
</p>
$$\mathcal{A}f = \lim_{t \rightarrow 0+} \frac{\mathbb{E}[f(X_t)] - f}{t}$$<p>
and
</p>
$$\begin{align}
\mathcal{A}f(i) &= \lim_{t \rightarrow 0+} \frac{\mathbb{E}^i[f(X_t)] - f(i)}{t} \\
&= \lim_{t \rightarrow 0+} \frac{1}{t} \left(\sum_{j \in S} (P_{ij} - \delta_{ij})f(j)\right)\\
&= \sum_{j \in S} q_{ij} f(j), \quad i \in S
\end{align}$$<p>
Thus, the generator $\mathbf{Q}$ is exactly the infinitesimal generator of $X_t$. This is important to digest especially because the $\mathcal{A}$ is new to me.</p>
<p>Extending the idea above, we get the backward Kolmogorov equation for $\mathbf{u} = (u_1, u_2, \ldots, u_I)^T$ and $u_i(t) = \mathbb{E}^i[f(X_t)]$:
</p>
$$\frac{d \mathbf{u}}{dt} = \mathbf{Qu} = \mathcal{A}\mathbf{u}$$<p>&#x26a0;&#xfe0f; This, too, is getting a little confusing. Let&rsquo;s delve into it a bit more.</p>
<blockquote>
<p>We are essentially dealing with a continous time markov chaic (CTMC) in the above case, because we have a finite number of states that have some associated probability of moving to another state at an infitesimal time step.</p>
<p><a href="https://en.wikipedia.org/wiki/Kolmogorov_equations#Continuous-time_Markov_chains">Wikipedia</a> says that for CTMC&rsquo;s, the Komogorov backward equations are, rather intuitively, that the time derviative of the probaility of transitioning from state $i$ at time $s$ to state $j$ at time $t$.
</p>
$$\frac{\partial P_{ij}}{\partial t}(s;t) = \sum_k P_{kj}(s;t)A_{ik}(s)$$<p>
Where $A$ is the <a href="https://en.wikipedia.org/wiki/Transition-rate_matrix">transition-rate matrix</a> in which an element $q_{ij}$ denotes the rate departing from $i$ and arriving in state $j$. Knowing this, I can understand why $\mathcal{A}$ is the generator $\mathbf{Q}$. Converting things back into our notation, we have
</p>
$$ \frac{d P_{ij}}{dt} = \sum_{k \in I} \mathcal{A}_{ik} P_{kj} $$<p>In that case, we look back at the expression for $\mathbb{E}^i[f(X_t)]$
</p>
$$\mathbb{E}^i[f(X_t)] = \sum_j P_{ij}f(j)$$<p>
So,
</p>
$$ \begin{align}
\implies \frac{d}{dt} \mathbb{E}^i[f(X_t)] &= \sum_j \frac{d P_{ij}}{d t} f(j) \\
&= \sum_j \left(\left[ \sum_k \mathcal{A_{ik}} P_{kj} \right] f(j) \right) \\
&= \sum_k \sum_j \mathcal{A}_{ik} P_{kj} f(j) \\
&= \sum_k \mathcal{A}_{ik} \mathbb{E}^k[f(X_t)]
\end{align}
$$<p>Now, it&rsquo;s clear that
</p>
$$\frac{d}{dt} \mathbf{u} = \mathcal{A} \mathbf{u}$$</blockquote>
<p>The backward kolmogrov equation is heavily linked to diffusion, so I will definitely explore that in the future.</p>
<p>On the other hand, if we have a distribution $\mathbf{\nu} = (\nu_1, \nu_2, \ldots, \nu_I)$ (which, by convention, is a row vector), then it satisfies the forward Kolomogrov equation
</p>
$$\frac{d\mathbf{\nu}}{dt} = \mathbf{\nu} \mathcal{A}$$<p>
Or using the adjoint
</p>
$$\frac{d\mathbf{\nu}^T}{dt} = \mathcal{A}^* \mathbf{\nu}^T$$<p>
where $\mathcal{A}^*$ is defined as
</p>
$$(A^* g, f) = (g, \mathcal{A} f) \quad \forall \; f \in \mathscr{B}, g \in \mathscr{B}$$<p>
If this is giving you trouble, refer to equation (3.19) in the book and think with bra-ket notation. Recall that if $\mathscr{B} = L^2$, then the dual space is also $L^2$, and so $\mathcal{A}^* = \mathcal{A}^T$.</p>
<p>&#x26a0;&#xfe0f; Is this last statement rigorous? Specifically, I am asking about stating that $\mathcal{A} = \mathbf{Q}$. The book seems to avoid saying both are directly equal, but it really looks like they are.</p>
<h2 id="example-58---poisson-process">Example 5.8 - Poisson process</h2>
<p>Consider the Poisson process $X_t$ on $\mathbb{N}$ with rate $\lambda$. Then,
</p>
$$\begin{align}
(\mathcal{A}f)(n) &= \lim_{t \rightarrow 0+} \frac{\mathbb{E}^n[f(X_t)] - f(n)}{t} \\
&= \lim_{t \rightarrow 0+} \frac{1}{t} \left( \sum_{k=n}^\infty \frac{(\lambda t)^{k-n}}{(k-n)!} e^{-\lambda t} f(k) - f(n)\right) \\
&= \lim_{t \rightarrow 0+} \frac{1}{t} \left( f(n)e^{-\lambda t} + f(n+1)\lambda t + \sum_{k=n+2}^\infty \frac{(\lambda t)^{k-n}}{(k-n)!} e^{-\lambda t} f(k) - f(n)\right) \\
&= \lim_{t \rightarrow 0+} \frac{1}{t} \left( f(n)(e^{-\lambda t}-1) + f(n+1)\lambda t e^{-\lambda t} + \sum_{k=n+2}^\infty \frac{(\lambda t)^{k-n}}{(k-n)!} e^{-\lambda t} f(k) \right) \\
&= \lambda(f(n+1) - f(n))
\end{align}$$<p>
The last step is justified with L&rsquo;Hopital&rsquo;s rule.</p>
<p>Then, the book says
</p>
$$\mathcal{A}^*f(n) = \lambda(f(n-1) - f(n))$$<blockquote>
<p>&#x26a0;&#xfe0f; Here is the best reasoning I can come up with:
</p>
$$(g, \mathcal{A}f) = (\mathcal{A}^* g, f), \quad \forall \; f\in \mathscr{B}, g \in \mathscr{B}^*$$<p>
And defining $f^\pm(n) := f(n \pm 1)$, then we require
</p>
$$(\mathcal{A}^*g, f) = \lambda(g,f^+) - \lambda(g,f)$$<p>
Then if we note that $(g,f^+) = (g^-,f)$ (this is the part I cannot justify), then it follows that $\mathcal{A}^*f(n) = \lambda(f(n-1) - f(n))$</p>
</blockquote>
<p>Again, lets compute the time derivative of $u(t,n) = \mathbb{E}^n[f(X_t)]$
</p>
$$\begin{align}
\frac{d u}{dt} &= \frac{d}{dt}\left(\sum_{k \geq n} f(k) \frac{(\lambda t)^{k-n}}{(k-n)!}e^{-\lambda t}\right) \\
&= \frac{d}{dt}\left( e^{-\lambda t} f(n) + \sum_{k > n} f(k) \frac{(\lambda t)^{k-n}}{(k-n)!}e^{-\lambda t} \right) \\
&=  -\lambda e^{-\lambda t} f(n) + \sum_{k > n} f(k) \left[ \left(-\lambda\frac{(\lambda t)^{k-n}}{(k-n)!}e^{-\lambda t}\right) + \left(\lambda\frac{(\lambda t)^{k-n-1}}{(k-n-1)!}e^{-\lambda t}\right) \right] \\
&= \lambda(u(t,n+1)-u(t,n)) \\
&= \mathcal{A}u(t,n)
\end{align}$$<p>And the time derivative of the distribution $\mathbf{\nu} = (\nu_0, \nu_1, \ldots)$ will be
</p>
$$\begin{align}
\frac{d \nu_n(t)}{dt} &= \frac{d}{dt}\left(\frac{(\lambda t)^{k-n}}{(k-n)!}e^{-\lambda t}\right) \\
&= -\lambda\frac{(\lambda t)^{k-n}}{(k-n)!}e^{-\lambda t} + \lambda \frac{(\lambda t)^{k-n-1}}{(k-n-1)!}e^{-\lambda t} \\
&= \lambda(\nu_{n-1} - \nu_n) \\
&= (\mathcal{A}^* \mathbf{\nu})_n
\end{align}$$<p>&#x26a0;&#xfe0f; I keep getting $\lambda(\nu_{n+1} - \nu_n)$, which disagrees with the book. Where did I go wrong?</p>
<p>Notice how both Markov processes satisfied the forward Kolmogrov equation for the distribution, and the backwards for the expected values. This is a general property of Markov processes (wow!) that will be revisited.</p>
]]></content:encoded>
    </item>
    <item>
      <title>5.2 - Filtration and Stopping Time</title>
      <link>https://hasithv.github.io/posts/notes/eliasa/chap5/5-2/</link>
      <pubDate>Sat, 03 Aug 2024 18:57:02 -0700</pubDate>
      <guid>https://hasithv.github.io/posts/notes/eliasa/chap5/5-2/</guid>
      <description>&lt;h2 id=&#34;filtration&#34;&gt;Filtration&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition 5.3:&lt;/strong&gt; (Filtration). Given a probability space, the filtration is a nondecreaseing family of $\sigma$-algebras $\{\mathcal{F}_t\}_{t \leq 0}$ such that $\mathcal{F}_s \subset \mathcal{F}_t \subset \mathcal{F}$ for all $0 \leq s &lt; t$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Intuitively, the filtration is a sigma algebra of events that can be determined before time $t$ (we can&amp;rsquo;t lose information by foing forward in time). A stochastic process is called &lt;em&gt;$\mathcal{F}_t$-adapted&lt;/em&gt; if it is measurable with respect to $\mathcal{F}_t$; that is, for all $B \in \mathcal{R}$, $X_t^{-1}(B) \in \mathcal{F}_t$. We can always assume that the $\mathcal{F}_t$ contains $F_t^{X}$ and all sets of measure zero, where $F_t^{X} = \sigma(X_s, s \leq t)$ is the sigma algebra generated by the process $X$ up to time $t$.&lt;/p&gt;</description>
      <content:encoded><![CDATA[<h2 id="filtration">Filtration</h2>
<blockquote>
<p><strong>Definition 5.3:</strong> (Filtration). Given a probability space, the filtration is a nondecreaseing family of $\sigma$-algebras $\{\mathcal{F}_t\}_{t \leq 0}$ such that $\mathcal{F}_s \subset \mathcal{F}_t \subset \mathcal{F}$ for all $0 \leq s < t$.</p>
</blockquote>
<p>Intuitively, the filtration is a sigma algebra of events that can be determined before time $t$ (we can&rsquo;t lose information by foing forward in time). A stochastic process is called <em>$\mathcal{F}_t$-adapted</em> if it is measurable with respect to $\mathcal{F}_t$; that is, for all $B \in \mathcal{R}$, $X_t^{-1}(B) \in \mathcal{F}_t$. We can always assume that the $\mathcal{F}_t$ contains $F_t^{X}$ and all sets of measure zero, where $F_t^{X} = \sigma(X_s, s \leq t)$ is the sigma algebra generated by the process $X$ up to time $t$.</p>
<p>As an example, in a series of coin flips, when $n=0$
</p>
$$\mathcal{F}_0^X = \{\emptyset, \Omega\}$$<p>
and when $n=1$,
</p>
$$\mathcal{F}_1^X = \{\emptyset, \Omega, \{H\}, \{T\}\}$$<p>
when $n=2$,
</p>
$$\mathcal{F}_2^X = \sigma(\{\emptyset, \{HH\}, \{TT\}, \{HT\}, \{TH\} \})$$<p>
(I believe this last statement is equivalent to what the book has)</p>
<h2 id="stopping-time">Stopping Time</h2>
<blockquote>
<p><strong>Definition 5.4:</strong> (Stopping time for discrete time stochastic processes). A stopping time is a random variable $T$ taking values in $\{1,2,\ldots\}\cup \{\infty\}$ such that for any $n < \infty$,
</p>
$$\{T \leq n\} \in \mathcal{F}_n$$</blockquote>
<p>For the discrete case, it doesn&rsquo;t matter if we say $\{T \leq n\}$ or $\{T = n\}$ simply becase it has to be satisfied for all $n$.</p>
<blockquote>
<p><strong>Proposition 5.5:</strong> (Properties of stopping times). For the Markov process $\{X_n\}_{n \in \mathbb{N}}$, we have</p>
<ul>
<li>(1) if $T_1, T_2$ are stopping times, then $T_1 \wedge T_2, T_1 \vee T_2, T_1 + T_2$ are stopping times</li>
<li>(2) if $\{T_k\}_{k \geq 1}$ are stopping times then $\sup_k T_k, \inf_k T_k, \limsup_k T_k, \liminf_k T_k$ are stopping times</li>
</ul>
</blockquote>
<blockquote>
<p><strong>Definition 5.6:</strong> (Stopping time for continuous time stochastic processes). A stopping time is a random variable $T$ taking values in $[0,\infty]$ such that for any $t \in \mathbb{\bar{R}}^+$,
</p>
$$\{T \leq t\} \in \mathcal{F}_t$$</blockquote>
<p>Note that we cannot swap the inequality for an equals sign in the definition of a stopping time for continuous time processes. Furthermore, porposition 5.5 holds for conitnious time processes if the filtration is right continuous: $\mathcal{F}_t = \mathcal{F}_{t^+}= \bigcap_{s>t} \mathcal{F}_s$.</p>
]]></content:encoded>
    </item>
    <item>
      <title>5.1 - Axiomatic Construction of Stochastic Process</title>
      <link>https://hasithv.github.io/posts/notes/eliasa/chap5/5-1/</link>
      <pubDate>Sat, 03 Aug 2024 16:45:46 -0700</pubDate>
      <guid>https://hasithv.github.io/posts/notes/eliasa/chap5/5-1/</guid>
      <description>&lt;h2 id=&#34;definition-of-a-stochastic-process&#34;&gt;Definition of a stochastic process&lt;/h2&gt;
&lt;p&gt;A stochastic process is a parameterized random variable $\{X_t\}_{t\in\mathbf{T}}$ defined on a probability space $(\Omega, \mathcal{F}, \mathbb{P})$ taking on values in $\mathbb{R}$. $\mathbf{T}$ can seemingly be any subset of $\mathbb{R}$. For any fixed $t \in \mathbf{T}$, we can define the random variable&lt;/p&gt;
$$X_t: \Omega \rightarrow \mathbb{R}, \quad \omega \rightarrowtail X_t(\omega)$$&lt;p&gt;Thinking of a simple random walk, this means that $X_t$ is a random variable that takes in some subset of $\Omega = \{H,T\}^\mathbb{N}$ and outputs a real valued number (the sum of the first $t$ values in $\omega$): $\{\omega_1, \omega_2, \ldots \} \rightarrow \sum_{n \leq t} X(\omega_n)$&lt;/p&gt;</description>
      <content:encoded><![CDATA[<h2 id="definition-of-a-stochastic-process">Definition of a stochastic process</h2>
<p>A stochastic process is a parameterized random variable $\{X_t\}_{t\in\mathbf{T}}$ defined on a probability space $(\Omega, \mathcal{F}, \mathbb{P})$ taking on values in $\mathbb{R}$. $\mathbf{T}$ can seemingly be any subset of $\mathbb{R}$. For any fixed $t \in \mathbf{T}$, we can define the random variable</p>
$$X_t: \Omega \rightarrow \mathbb{R}, \quad \omega \rightarrowtail X_t(\omega)$$<p>Thinking of a simple random walk, this means that $X_t$ is a random variable that takes in some subset of $\Omega = \{H,T\}^\mathbb{N}$ and outputs a real valued number (the sum of the first $t$ values in $\omega$): $\{\omega_1, \omega_2, \ldots \} \rightarrow \sum_{n \leq t} X(\omega_n)$</p>
<p>On the other side of the coin, for a fixed $\omega \in \Omega$, we can define a real-valued measureable function on $\mathbf{T}$ called the trajectory of $X$</p>
$$X_.(\omega): \mathbf{T} \rightarrow \mathbb{R}, \quad t \rightarrowtail X_t(\omega)$$<p>Again, back to the random walk, this means that we can get a real valued output for any given $t$. To be even more compact, we can say taht a stochastic process is a measureable function from $\Omega \times \mathbf{T}$ to $\mathbb{R}$</p>
$$(\omega, t) \rightarrowtail X(\omega, t) := X_t(\omega)$$<p>The largest probability space that one can take is the infinite product space $\Omega = \mathbb{R}^\mathbf{T}$. Essentially, this is a space which can takeon any real value at any moment in time (&#x26a0;&#xfe0f; why are we restricting ourselves to $\mathbb{R}$? Why can&rsquo;t it be a vector valued function?)</p>
<p>For finite dimension distributions, we are interested in
</p>
$$\mu_{1,\ldots,t_k}(F_1 \times \ldots \times F_k) = \mathbb{P[X_{t_1}\in F_1, \ldots X_{t_k} \in F_k]}$$<blockquote>
<p><strong>Theorem 5.2:</strong> (Kolmogorov&rsquo;s extension theorem). Kolmogorov&rsquo;s extension theorem allows us to say, for any $\mu$ invariant under permuting the order of $t_k$ and $F_k$ and also adding additional time points with their associated $F$ being $\mathbb{R}$, that there exists a probability space and a stochastic prcess such that
</p>
$$\mu_{1,\ldots,t_k}(F_1 \times \ldots \times F_k) = \mathbb{P[X_{t_1}\in F_1, \ldots X_{t_k} \in F_k]}$$</blockquote>
<p>Kolmogorov&rsquo;s extension theorem is very general. In fact, so general that it does not give us a very good idea of what the process actually looks like. Usually, we start with this extremely general definition and then impose stricter conditions to prove that the measure can be defined on a smaller probability space rather than $\Omega$</p>
]]></content:encoded>
    </item>
    <item>
      <title>Applied Stochastic Analysis</title>
      <link>https://hasithv.github.io/posts/notes/eliasa/eliasa/</link>
      <pubDate>Sat, 03 Aug 2024 16:43:39 -0700</pubDate>
      <guid>https://hasithv.github.io/posts/notes/eliasa/eliasa/</guid>
      <description>&lt;p&gt;Here are my notes for E, Li, and Vanden-Eijnden&amp;rsquo;s &lt;a href=&#34;https://bookstore.ams.org/gsm-199/&#34;&gt;&lt;em&gt;Applied Stochastic Analysis&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Chapter 5 - Stochastic Processes
&lt;ul&gt;
&lt;li&gt;5.1 - &lt;a href=&#34;https://hasithv.github.io/posts/notes/eliasa/chap5/5-1/&#34;&gt;Axiomatic Construction of Stochastic Process&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;5.2 - &lt;a href=&#34;https://hasithv.github.io/posts/notes/eliasa/chap5/5-2/&#34;&gt;Filtration and Stopping Time&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;5.3 - &lt;a href=&#34;https://hasithv.github.io/posts/notes/eliasa/chap5/5-3/&#34;&gt;Markov Processes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;5.4 - &lt;a href=&#34;https://hasithv.github.io/posts/notes/eliasa/chap5/5-4/&#34;&gt;Gaussian Processes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Chapter 6 - Wiener Process
&lt;ul&gt;
&lt;li&gt;6.1 - &lt;a href=&#34;https://hasithv.github.io/posts/notes/eliasa/chap6/6-1/&#34;&gt;The Diffusion Limit of Random Walks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;6.2 - &lt;a href=&#34;https://hasithv.github.io/posts/notes/eliasa/chap6/6-2/&#34;&gt;The Invariance Principle&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;</description>
      <content:encoded><![CDATA[<p>Here are my notes for E, Li, and Vanden-Eijnden&rsquo;s <a href="https://bookstore.ams.org/gsm-199/"><em>Applied Stochastic Analysis</em></a></p>
<ul>
<li>Chapter 5 - Stochastic Processes
<ul>
<li>5.1 - <a href="/posts/notes/eliasa/chap5/5-1/">Axiomatic Construction of Stochastic Process</a></li>
<li>5.2 - <a href="/posts/notes/eliasa/chap5/5-2/">Filtration and Stopping Time</a></li>
<li>5.3 - <a href="/posts/notes/eliasa/chap5/5-3/">Markov Processes</a></li>
<li>5.4 - <a href="/posts/notes/eliasa/chap5/5-4/">Gaussian Processes</a></li>
</ul>
</li>
<li>Chapter 6 - Wiener Process
<ul>
<li>6.1 - <a href="/posts/notes/eliasa/chap6/6-1/">The Diffusion Limit of Random Walks</a></li>
<li>6.2 - <a href="/posts/notes/eliasa/chap6/6-2/">The Invariance Principle</a></li>
</ul>
</li>
</ul>
]]></content:encoded>
    </item>
    <item>
      <title>2.2 - Symmetric monoidal preorders</title>
      <link>https://hasithv.github.io/posts/notes/fongspivakact/chap2/2-2/</link>
      <pubDate>Fri, 02 Aug 2024 01:26:30 -0700</pubDate>
      <guid>https://hasithv.github.io/posts/notes/fongspivakact/chap2/2-2/</guid>
      <description>&lt;h2 id=&#34;221---definition-and-first-examples&#34;&gt;2.2.1 - Definition and first examples&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition 2.2:&lt;/strong&gt; A &lt;em&gt;symmetric monoidal structure&lt;/em&gt; on a preoirder $(X, \leq)$ consists of&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;(i) a &lt;em&gt;monoidal unit&lt;/em&gt;, $I \in X$&lt;/li&gt;
&lt;li&gt;(ii) a &lt;em&gt;monoidal product&lt;/em&gt; $\otimes: X \times X \rightarrow X$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;And the monoidal product $\otimes(x_1,x_2) = x_1 \otimes x_2$ must also satisfy the following properties (assume all elements are in $X$)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;(a) $x_1 \leq y_1$ and $x_2 \leq y_2 \implies x_1 \otimes x_2 \leq y_1 \otimes y_2$&lt;/li&gt;
&lt;li&gt;(b) $I \otimes x = x \otimes I = x$&lt;/li&gt;
&lt;li&gt;(c) associativity&lt;/li&gt;
&lt;li&gt;(d) commutivity/symmetry&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;(a) is called &lt;em&gt;monotnoicity&lt;/em&gt; and (b) is &lt;em&gt;unitality&lt;/em&gt;&lt;/p&gt;</description>
      <content:encoded><![CDATA[<h2 id="221---definition-and-first-examples">2.2.1 - Definition and first examples</h2>
<blockquote>
<p><strong>Definition 2.2:</strong> A <em>symmetric monoidal structure</em> on a preoirder $(X, \leq)$ consists of</p>
<ul>
<li>(i) a <em>monoidal unit</em>, $I \in X$</li>
<li>(ii) a <em>monoidal product</em> $\otimes: X \times X \rightarrow X$</li>
</ul>
<p>And the monoidal product $\otimes(x_1,x_2) = x_1 \otimes x_2$ must also satisfy the following properties (assume all elements are in $X$)</p>
<ul>
<li>(a) $x_1 \leq y_1$ and $x_2 \leq y_2 \implies x_1 \otimes x_2 \leq y_1 \otimes y_2$</li>
<li>(b) $I \otimes x = x \otimes I = x$</li>
<li>(c) associativity</li>
<li>(d) commutivity/symmetry</li>
</ul>
<p>(a) is called <em>monotnoicity</em> and (b) is <em>unitality</em></p>
</blockquote>
<p><strong>Remark 2.3:</strong> replacing $=$ with $\cong$ in definition 2.2 will give us a <em>weak monoidal structure</em>.</p>
<blockquote>
<p><strong>Exercise 2.5:</strong> The preorder structure $(\mathbb{R}, \leq)$ and the multiplication operation $\times$ will not give us a symmetric monoidal order because of the simple counter example of $-2 \times -2 \nleq 1 \times 1$.</p>
</blockquote>
<blockquote>
<p><strong>Example 2.6:</strong> A <em>monid</em> is similar to a symmetric monoidal preorder in that it consists of a set $M$, a function $*: M\times M \rightarrow M$, and an elment $e \in M$ called the <em>monid unit</em>, such that for every $m,n,p \in M$,</p>
<ul>
<li>$m * e = m$</li>
<li>$e * m = m$</li>
<li>associativity holds</li>
</ul>
<p>Further, if commutivity holds (which isn&rsquo;t not generally true),  then it is also called <em>commutative</em></p>
</blockquote>
<h2 id="222---introducing-wiring-diagrams">2.2.2 - Introducing wiring diagrams</h2>
<p>&#x26a0;&#xfe0f; I am seeing the wiring diagrams, but I fail to understand why they are any different from the Hasse diagrams we&rsquo;ve seen previously.</p>
<p>Essentially, wiring diagrams seem to be a way to encode information about symmetric monoidal structures. The basic rules built up so far are as follows:</p>
<ul>
<li>A wire without a label, or with the label of the monoidal unit, is equivalent to nothing</li>
<li>Otherwise, a wire labeled with an element represents that element (&#x26a0;&#xfe0f; this could be wrong)</li>
<li>Two parallel wires represent the monoidal product of those elements</li>
<li>Placing a $\leq$ block between two $x,y$ wires indicates that $x \leq y$</li>
</ul>
<p>Thinking back to the conditions for a symmetric monoidal structure, we find that</p>
<ol>
<li>
<p>Transitivity allows us to combine wiring diagrams left to right
<figure class="align-center ">
    <img loading="lazy" src="./images/transitivitypt1.png#center" width="400px"/> 
</figure>
</p>
</li>
<li>
<p>Monotonicity is represented as being able to combine wiring diagrams top to bottom
<figure class="align-center ">
    <img loading="lazy" src="./images/monotonicity.png#center" width="400px"/> 
</figure>
</p>
</li>
<li>
<p>A monoidal product with a monoidal unit and another element gives us the element again, because the monoidal unit is equivalent to nothing (reflexivity)</p>
</li>
<li>
<p>Associativity means we can &ldquo;wiggle&rdquo; around parallel wires
<figure class="align-center ">
    <img loading="lazy" src="./images/associativity.png#center" width="400px"/> 
</figure>
</p>
</li>
<li>
<p>Commutivity means we can cross wires
<figure class="align-center ">
    <img loading="lazy" src="./images/commutivity.png#center" width="400px"/> 
</figure>
</p>
</li>
</ol>
<p>It is intuitive to see how these wiring diagrams can be used to prove statements. In fact, the above images are trivial proofs. Take a look at the following exercise</p>
<blockquote>
<p><strong>Exercise 2.20:</strong>
Prove&ndash;given $t \leq v+w$, $w+u \leq x+z$, and  $v+x \leq y$&ndash;that $t+u \leq y+z$.</p>
<p>ALgebraically, we proceed like so:
</p>
$$\begin{align} 
    t + u &\leq (v+w) + u \\
    &\leq v + (w+u) \\
    &\leq v + (x+z) \\
    &\leq (v + x) + z \\
    &\leq y + z \\
\end{align}$$<p>
and the wiring diagram would look like
<figure class="align-center ">
    <img loading="lazy" src="./images/wiringex.png#center"
         alt="The squares are the $\leq$ blocks" width="400px"/> <figcaption>
            <p>The squares are the $\leq$ blocks</p>
        </figcaption>
</figure>
</p>
</blockquote>
<h2 id="223---applied-examples">2.2.3 - Applied examples</h2>
<p>While this section did solidify some concepts. It wasn&rsquo;t too important. Although, it did carry two useful examples: discarding and splitting.</p>
<p>With discarding, if a symmetric monoidal structure also satisfies $x \leq I$ for every $x \in X$, then it is possible to terminate any wire:
<figure class="align-center ">
    <img loading="lazy" src="./images/discard.png#center" width="300px"/> 
</figure>
</p>
<p>And if instead have a property like $x \leq x + x$, then we can split any wire:
<figure class="align-center ">
    <img loading="lazy" src="./images/splitting.png#center" width="300px"/> 
</figure>
</p>
<h2 id="224---abstract-examples">2.2.4 - Abstract examples</h2>
<p>Again, after a skim through, this section did not seem critical.</p>
<h2 id="225---monoidal-montone-maps">2.2.5 - Monoidal montone maps</h2>
<p>We begin with recalling that for any preorder $(X,\leq)$ we have an induced equivalence relation $\cong$ on $X$ where two elements $x \cong x' \iff x \leq x$ and $x' \leq x$</p>
<blockquote>
<p><strong>Definition 2.41:</strong> $\mathcal{P} = (P, \leq_P, I_P, \otimes_P)$ and $\mathcal{Q} = (Q, \leq_Q, I_Q, \otimes_Q)$ be monoidal preorders. A <strong>monoidal monotone</strong> from $\mathcal{P}$ to $\mathcal{Q}$ is a monotone map $f: (P, \leq_P) \rightarrow (Q, \leq_Q)$ which satisfies</p>
<ul>
<li>(a) $I_Q \leq_Q f(I_P)$</li>
<li>(b) $f(p_1) \otimes_Q f(p_1 \otimes_P p_2)$
for all $p_1,p_2 \in P$.</li>
</ul>
<p>Additionally, $f$ is a <em>strong monoidal monotone</em> if it satisfies</p>
<ul>
<li>(a&rsquo;) $I_Q \cong f(I_P)$</li>
<li>(b&rsquo;) $f(p_1) \otimes_Q f(p_1 \otimes_P p_2) \cong f(p_1) \otimes_Q f(p_2)$
And it is called a <em>strict monoidal monotone</em> if it satisfies</li>
<li>(a&rsquo;&rsquo;) $I_Q = f(I_P)$</li>
<li>(b&rsquo;&rsquo;) $f(p_1) \otimes_Q f(p_1 \otimes_P p_2) = f(p_1) \otimes_Q f(p_2)$</li>
</ul>
</blockquote>
<p>Monoidal monotones are said to be examples of <em>monoidal functors</em> in category theory.</p>
<p>The exercises for this section seem a little easy, so I will be skipping them for now, returning to them if I get confused on the defnitions of monoidal monotones.</p>
]]></content:encoded>
    </item>
    <item>
      <title>An Invitation to Appied Category Theory</title>
      <link>https://hasithv.github.io/posts/notes/fongspivakact/fongspivakact/</link>
      <pubDate>Fri, 02 Aug 2024 00:47:08 -0700</pubDate>
      <guid>https://hasithv.github.io/posts/notes/fongspivakact/fongspivakact/</guid>
      <description>&lt;p&gt;This is a collection of my notes for Brendan Fong and David Spivak&amp;rsquo;s &lt;a href=&#34;https://arxiv.org/pdf/1803.05316&#34;&gt;&lt;em&gt;An Invitation to Appied Category Theory&lt;/em&gt;&lt;/a&gt;. The first chapter was done through LaTeX, but the rest should be markdown with mathjax.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://hasithv.github.io/1.pdf&#34;&gt;Chapter 1 - Generative effects: Orders and adjunctions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Chapter 2 - Resource theories: Monoidal preorders and enrichment
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://hasithv.github.io/posts/notes/fongspivakact/chap2/2-2/&#34;&gt;Section 2.2 - Symmetric monoidal preorders&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;</description>
      <content:encoded><![CDATA[<p>This is a collection of my notes for Brendan Fong and David Spivak&rsquo;s <a href="https://arxiv.org/pdf/1803.05316"><em>An Invitation to Appied Category Theory</em></a>. The first chapter was done through LaTeX, but the rest should be markdown with mathjax.</p>
<ul>
<li><a href="/1.pdf">Chapter 1 - Generative effects: Orders and adjunctions</a></li>
<li>Chapter 2 - Resource theories: Monoidal preorders and enrichment
<ul>
<li><a href="/posts/notes/fongspivakact/chap2/2-2/">Section 2.2 - Symmetric monoidal preorders</a></li>
</ul>
</li>
</ul>
]]></content:encoded>
    </item>
    <item>
      <title>That&#39;s not how Probability Works!</title>
      <link>https://hasithv.github.io/posts/24-07-29-nothowprobabilityworks/</link>
      <pubDate>Tue, 30 Jul 2024 00:07:20 -0700</pubDate>
      <guid>https://hasithv.github.io/posts/24-07-29-nothowprobabilityworks/</guid>
      <description>&lt;p&gt;I was recently doing a probability puzzle that I can&amp;rsquo;t quite remember the context of, but I came across the answer that the probability would be
&lt;/p&gt;
$$\mathbb{P}(X) = n p^n \; \quad \forall \: n\in\mathbb{N}, p \in [0,1].$$&lt;p&gt;But this is obviously wrong! Plug in $p=.9, n=2$, and you get that $\mathbb{P}(X) = 1.62$. Thaat&amp;rsquo;s not how probability works! However, for $p=0.5$, $\mathbb{P}(X)$ will remain $\leq 1$ for all $n \in \mathbb{N}$. So, somewhere in the interval $(0.5,0.9)$, we reach a critical value where any $p$ greater than that will result in a probability greater than one, and any value less than it will be a bit more reasonable.&lt;/p&gt;</description>
      <content:encoded><![CDATA[<p>I was recently doing a probability puzzle that I can&rsquo;t quite remember the context of, but I came across the answer that the probability would be
</p>
$$\mathbb{P}(X) = n p^n \; \quad \forall \: n\in\mathbb{N}, p \in [0,1].$$<p>But this is obviously wrong! Plug in $p=.9, n=2$, and you get that $\mathbb{P}(X) = 1.62$. Thaat&rsquo;s not how probability works! However, for $p=0.5$, $\mathbb{P}(X)$ will remain $\leq 1$ for all $n \in \mathbb{N}$. So, somewhere in the interval $(0.5,0.9)$, we reach a critical value where any $p$ greater than that will result in a probability greater than one, and any value less than it will be a bit more reasonable.</p>
<p>So, what is this critical value that will help me save face?</p>
<p>Well, the question we are trying to answer, phrased a bit more formally, is:</p>
<blockquote>
<p>find the largest $p \in [0,1]$ such that $np^n \leq 1$ for all $n \in \mathbb{N}.$</p>
</blockquote>
<p>First, we rephrase the problem by stating
</p>
$$np^n \leq 1 \iff p^n \leq \frac{1}{n}.$$<p>Visually, this means that the exponential graph of $f_p(n) = p^n$ can never go above $g_p(n) = \frac{1}{n}$ for some fixed $p$. From this, we can deduce that the critical value of $p$, which we will denote as $p_0$, will satisfy the following relation:</p>
<blockquote>
<p>Given the parametrized forms of $f_p$ and $g_p$
</p>
$$F_{p}(t) = \begin{bmatrix}
t \\
f_p(t)
\end{bmatrix}, \; 
G_p(t) = \begin{bmatrix}
t \\
g_p(t)
\end{bmatrix},$$<p>
the critical $p_0$ value will be such that
</p>
$$F_{p_0}(t_0) = G_{p_0}(t_0),\text{ and } \dot{F}_{p_0}(t_0) = \lambda \dot{G}_{p_0}(t_0)$$<p>
for some $\lambda \in \mathbb{R}, t_0 \in \mathbb{R}^+$. In other words, their velocities will point in the same direction (and, perhaps more intuitively, the outward normals of each curve will be parallel, so the graphs &lsquo;kiss&rsquo; at some $t_0$ with the choice of $p_0$).</p>
</blockquote>
<p>Now, we have a fairly simple problem to solve. Because the $x$ component of $F$ and $G$ are always equal, we immediately find that $\lambda = 1$ for their time derivatives to be equal to each other. Now, that leads us to solve for a $p_0$ and $n$ such that
</p>
$$
\begin{aligned}
    f_{p_0}(t_0) &= g_{p_0}(t_0) \\
    \implies p_0^n &= \frac{1}{t_0}
\end{aligned}
$$<p>
and
</p>
$$
\begin{aligned}
     \dot{f}_{p_0}(t_0) &= \dot{g}_{p_0}(t_0) \\
    \implies -\ln(p_0)p_0^n &= \frac{1}{t_0^2}
\end{aligned}
$$<p>
So, rather unsatisfyingly, we boiled it down to a system of nonlinear equations
</p>
$$
\begin{cases}
    p_0^n = \frac{1}{t_0}, \\
    \ln(p_0)p_0^n = -\left(\frac{1}{t_0}\right)^2
\end{cases}
$$<p>
which I cannot solve, but Desmos tells me that $p_0 \approx 0.6922$ and $t_0 \approx 2.7181$.</p>
<p>Thus, my answer would have been reasonable in <em>some</em> convoluted scenario in which $p < 0.6922$.</p>
<p>(This answer, too, is not totally right! This is because there may be a larger $p$ value that satisfies $np^n \leq 1$ for $n \in \mathbb{N}$ but <em>not</em> for $n\in \mathbb{R}^+$. We solved for the $n \in \mathbb{R}^+$ case, which would technically give us a lower bound for $p_0$. Taking this into consideration, our $p_0$ value would really be $p_0 \approx 0.6934$)</p>
]]></content:encoded>
    </item>
    <item>
      <title>Introduction</title>
      <link>https://hasithv.github.io/posts/introduction/</link>
      <pubDate>Mon, 29 Jul 2024 00:07:20 -0700</pubDate>
      <guid>https://hasithv.github.io/posts/introduction/</guid>
      <description>&lt;p&gt;I have no idea what I am doing. Anyways, here&amp;rsquo;s a cool equation:&lt;/p&gt;
$$\frac{d}{dt}\left(\frac{\partial L}{\partial \dot{q}}\right) - \frac{\partial L}{\partial q} = 0$$</description>
      <content:encoded><![CDATA[<p>I have no idea what I am doing. Anyways, here&rsquo;s a cool equation:</p>
$$\frac{d}{dt}\left(\frac{\partial L}{\partial \dot{q}}\right) - \frac{\partial L}{\partial q} = 0$$]]></content:encoded>
    </item>
  </channel>
</rss>
