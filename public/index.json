[{"content":"This project was done as our final project for William Gilpin\u0026rsquo;s Graduate Computational Physics Course. Our complete GitHub repository, with instructions on how to replicate our results, can be found here.\nIntroduction The goal of this project is to simulate the behavior of a bike-sharing system in a network of stations and destinations, and then optimize the positions of the stations. We approach the simulation of the bike-sharing system with Agent Based Modeling (ABM).\nUltimately, we aim to find the optimal locations for metrobike stations around the University of Texas at Austin campus and the surrounding West Campus area.\nThe Model The model consists of a number of stations where bikes can be picked up and dropped off, and a number of destinations that agents (commuters) want to visit. This is reprsented by a graph that agents can traverse.\nFor example, consider the following graph:\nExample graph of destinations (in blue) and stations (in red). The numbers on the nodes are the index of the node.\nIn this graph, the blue nodes represent destinations and the red nodes represent stations. Every node is connected to every other node by an edge, whose weight represents the time it takes to travel between them on a bike (to get the time between two nodes by walking, multiply the edge weight by 3 since walking is about 3 times slower than biking). The fact that the graph is fully connected means that, in principle, an agent can travel between any two nodes by walking.\nAgent Decision Making If an agent wants to bike, they must find a station with an available bike and another station where they can return the bike (ideally, the agent would want to find a station close to their destination to drop off the bike). Based on the fact that stations can be full or empty, the agent must decide whether to walk or bike to their destination. The full logic of the agent\u0026rsquo;s pathfinding is quite tedious to explain, but trust that the agent will approximate your decision making to the first order. Interested readers can refer to the pathfinding.py file and the step method in the Commuter class in commuter.py.\nAt each timestep, the agents will:\nMove towards their destination The stations update their bike counts based on the agents\u0026rsquo; decisions Agents who arrived at their destination pick a new destination and start moving towards it The way agents choose a particular destination is based on a probability distribution that we can set. Using the previous graph as an example, we can set the probability distribution to be uniform for every destination, or we can set it so that agents favor certain destinations over others. This is done with the weights attribute in the MyModel class in model.py.\nOptimization The goal of the optimization is to find the best positions for the stations in the network. We can use two algorithms to do this: particle swarm optimization (PSO) and a genetic algorithm (GA). The optimization algorithms are implemented in the optimize.py file.\n$$L = -\\frac{T}{N}$$ where $T$ is the total number of trips completed by all agents and $N$ is the number of agents in the model. The reason we use the negative of the average trips completed is because the optimization algorithms are designed to minimize the fitness function, and we want to maximize the number of trips completed.\nPSO Our implementation of PSO has the following hyperparameters:\nn_particles: The number of particles in the swarm. n_iterations: The number of iterations the algorithm will run for. c1: The cognitive parameter. c2: The social parameter. w: The inertia parameter. $$v_{i+1} = wv_i + c_1r_1(p_{\\text{best}, i} - x_i) + c_2r_2(g_\\text{best} - x_i)$$$$x_{i+1} = x_i + v_{i+1}$$ where $v_i$ is the velocity of particle $i$, $x_i$ is the position of particle $i$, $p_{\\text{best}, i}$ is the best position of particle $i$ so far, $gbest$ is the best position of the swarm, $r_1$ and $r_2$ are random numbers between 0 and 1, and $w$, $c_1$, and $c_2$ are the inertia, cognitive, and social parameters, respectively.\nThe PSO algorithm will do this for n_iterations iterations, and at the end, it will return the best position of the swarm found so far.\nGenetic Algorithm Our implementation of the genetic algorithm has the following hyperparameters:\npopulation_size: The number of individuals in the population. n_generations: The number of generations the algorithm will run for. mutation_rate, $p$: The probability that a gene will mutate. alpha: The strength of the mutation. Genetic algorithms works by initializing a population of individuals with random genes. At each generation, the individuals are evaluated based on their fitness, and the best individuals are selected to reproduce. The reproduction process involves selecting two parents and creating a child by combining their genes. The child\u0026rsquo;s genes are then mutated with a certain probability. The best individuals from the previous generation are carried over to the next generation. The genetic algorithm will do this for n_generations generations, and at the end, it will return the best individual found so far.\n$$c_i = \\left[\\begin{cases} a_i \u0026 \\text{with probability } 0.5 \\\\ b_i \u0026 \\text{with probability } 0.5 \\end{cases}\\right] + \\begin{cases} \\alpha B \u0026 \\text{with probability } p \\\\ 0 \u0026 \\text{with probability } 1 - p \\end{cases} $$ where $B$ is the length of the maxiumum dimension of the search space, $p$ is the mutation rate, and $\\alpha$ is the strength of the mutation.\nResults Everything needed to reproduce the results can be found in the metrobike.ipynb notebook. The notebook will guide you through the process of running the simulations and visualizing the results.\nSimple 2 station, 2 destination case This is the simplest case we can consider. Here, the analytical solution is quite simple to find if we assume uniform weights for the destinations. The optimal positions for the station are to place them directly on top of the destinations, and both PSO and GA are able to find this solution quite easily:\nOptimized map of two destinations and two stations using the PSO algorithm.\nOptimized map of two destinations and two stations using the GA algorithm.\n4 station, 2 destination case For this case, we placed four destinations in a diamond shape. And destinations 2 and 1 are about twice as far away from each other than destination 3 and destination 4.\nMap of four destinations arranged in a diamond shape. The aspect ratio of this graph is misleading, Destination 2 and Destination 1 are about twice as far away from each other than Destination 4 and Destination 3\nThus, with only two stations to place, the optimal solution is to place one station near destination 2 and the other near destination 1. Both the PSO and the genetic algorithm are quite sensitive to this problem, it seems as if they only converge to the optimal solution about half the time. For the PSO, we show a failed case, and for the GA we show a successful case where the optimal solution was found:\nOptimized map of four destinations and two stations using the PSO algorithm. Every destination is assumed to be equally popular.\nOptimized map of four destinations and two stations using the GA algorithm. Every destination is assumed to be equally popular\nAdditionally, we can also consider the case where the weights are not uniform. For example, we can set the weights to be $(0.7, 0.1, 0.1, 0.1)$. In this case, the optimal solution is to place one station near destination 2 and the other near destination 1, since destination 1 will be the most popular. In this case, both PSO and GA are able to find the optimal solution:\nOptimized map of four destinations and two stations using the PSO algorithm. Station 1 was set to be the most popular with a probability of 0.7 while all other stations had a probability of 0.1.\nOptimized map of four destinations and two stations using the GA algorithm. Station 1 was set to be the most popular with a probability of 0.7 while all other stations had a probability of 0.1.\n4 station, 4 destination case For this case, we placed four destinations in a square shape. The optimal solution is to place one station near each destination. However, both algorithms fail to find the optimal solution nearly every time and give us something like the following result:\nOptimized map of four destinations and two stations using the GA algorithm. All stations are equally popular.\nThis is likely due to the fact that for this case, the optimal solution is hidden behind many local minima, and the algorithms are not able to escape them. It could be possible that more aggresive methods to jump out of local minima could help for this particular case of destinations=stations.\nInvariance to initial bike distribution As one would expect, the initial distribution of bikes at the stations does not affect the final distribution of bikes at the stations. This is shown in the following histograms, where we start with all 10 bikes at station 1, but the distribution of bikes at the stations after 10,000 steps is equal (distribution collected for a model with 4 stations and 4 destinations, each with uniform weights):\nDistribution of bikes held for each station. The map used was the example network shown in The Model section with 4 destinations and 4 stations along rectangular vertices. Each destination was set to be equally popular.\nWe can also alter the weights of the destinations to be $(0.7, 0.1, 0.1, 0.1)$ and observe how the invariant distribution is altered:\nDistribution of bikes held for each station. The map used was the example network shown in The Model. Destination 1 was set to be the most pipular with a probability of 0.7 and the other destinations had a probability of 0.1.\nEven without seeing the actual map we used (we used the basic example graph shown in the very beginning), one can already guess that station 0 was placed closes to the most popular destination since it has the heaviest tail towards the right. From there, the agents seemed to prefer to bike to station 2 more often than station 3, and hardly any bikes ever reached station 1. So, despite station 1, 2, and 3 being just as popular of a destination, station 1 could had much less bikes than stations 2 or 3.\nThus, we can see that the popularity of a destination is not the only factor that determines the distribution of bikes at nearby stations\u0026ndash;we also need to consider the practicality riding a bike towards that destination from other nearby, popular destinations.\nApplication to Real-World Data Using metrobike data, we were able to estimate how popular certain areas in Austin were, and using GIS data of the distances between select locations of around campus and west campus, we were able to create a graph to represent UT Austin.\nMap of select destinations around UT Austin and West Campus. The corresponding location to each node index and its estimated popularity is given in the applications section.\nThe select locations and their respective weights we used were (the list number corresponds to the destination number of the graph):\n26th West - 0.078 McCombs - 0.25 Target - 0.086 Union Building - 0.086 PMA - 0.14 Union on 24th - 0.071 Welch - 0.14 Rise - 0.021 Axis West - 0.077 Rec - 0.056 Then, we were able to use our optimization algorithms to find the optimal positions of the stations. We optimized for 6 stations around campus and west campus, and the results can be seen in the following images:\nResult of optimization of 6 station locations around UT and West Campus using GA optimization. Only four stations are visible since the GA failed to converge and placed two stations outside the plot bounds.\nResult of optimization of 6 station locations around UT and West Campus using PSO. The PSO algorithm converged to a fairly reasonable solution.\nWe can see that the PSO algorithm was able to place all 6 stations within the boundaries we set, while the genetic algorithm placed 2 stations outside of the boundaries. In fact, the PSO solution seems very reasonable, with stations placed near the most popular destinations!\nConclusion For small systems, the optimization algorithms are able to find the optimal solution quite easily and quickly. For larger systems, however, we begin to see convergence issues. Nevertheless, given enough different initial conditions, the algorithms are able to find the optimal solution eventually\u0026ndash;still faster than trying to brute force the solution as we allowed for a continious search of a $2n$ dimensional space, where $n$ is the number of stations whose locations we want to optimize.\nWe were able to find some interesting relationships between the popularity of a destination and the distribution of bikes at nearby stations. We also found that the initial distribution of bikes at the stations does not affect the final distribution of bikes at the stations, so the model has an invariant distribution of bikes that is reached rather quickly.\nFinally, we were able to apply our model to real-world data and find the optimal positions of stations around campus and west campus. The results were quite reasonable, with stations placed near the most popular destinations.\nFuture Work Some directions we would really like to explore are\nImplementing a diurnal function to change the weights of destinations over time Implement a more agressive method to escape local minimas, which could help us find more optimal solutions for larger systems Visualizing the paths agents take to reach their destinations Do a grid search to find the best hyperparameters for the optimization algorithms, maybe this could also help stabilize convergence for larger systems ","permalink":"https://hasithv.github.io/posts/projects/24-12-10-metrobike/","summary":"\u003cp\u003eThis project was done as our final project for \u003ca href=\"https://www.wgilpin.com/\"\u003eWilliam Gilpin\u0026rsquo;s\u003c/a\u003e Graduate \u003ca href=\"https://www.wgilpin.com/cphy/?utm_source=en_us_srepgw\"\u003eComputational Physics Course\u003c/a\u003e. Our complete GitHub repository, with instructions on how to replicate our results, can be found \u003ca href=\"https://github.com/devddesai/metrobike\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003ch1 id=\"introduction\"\u003eIntroduction\u003c/h1\u003e\n\u003cp\u003eThe goal of this project is to simulate the behavior of a bike-sharing system in a network of stations and destinations, and then optimize the positions of the stations. We approach the simulation of the bike-sharing system with Agent Based Modeling (ABM).\u003c/p\u003e","title":"Metrobike Optimization Around UT Austin"},{"content":"Notes on the paper Planting Undetectable Backdoors in Machine Learning Models by Shafi Goldwasser, Michael P. Kim, Vinod Vaikuntanathan, and Or Zamir.\nThis paper was recommended to me by Scott Aaronson if I wanted to better understand some earlier, more cryptographic/theoretical work in backdooring neural networks. I am also reading through Anthropic\u0026rsquo;s Sleeper Agents paper, which is more recent and practical in its approach to backdooring current LLMs, those notes will be posted soon as well.\nQuick Summary Formally defines a backdoor in a neural network and then defines what it means for a backdoor to be undetectable, non-replicable, and persistent. Constructs a simple backdoor method that is easily detectable and replicable, and then presents a more sophisticated backdoor method that is non-replicable and undetectable. Also presents a method for constructing a neural network that is persistent to gradient descent. Discussion I found the formal definitions of backdoors, undetectability, and non-replicability to be very useful for future approaches to backdooring neural networks. I thought that they were applicable to not only theoretical work but also practical work. The definition for a persistent neural network, however, seemed to only be a purely theoretical exercise. Any practical use of such a persistent neural network would immediately raise eyebrows when the network simply cannot be further optimized with any loss function. While the simple backdoored method could definitely be used in practice, as the paper points out, it is easily detectable and replicable. The more persistent backdoor definitely seems like a strong method for backdooring neural networks, but it can only be used as a blackbox. This means that the adversary must have access to the model\u0026rsquo;s predictions and not the model itself, since seeing the \u0026lsquo;weights\u0026rsquo; of the model would reveal the verification step of the backdoor\u0026ndash;instantly giving away that some backdoor is present. (Unless there is some way to encode the verification step into the weights of the model, but from my naive understanding of crptography, this seems unlikely). Full Summary 3 Preliminaries Notations\nLet ${\\mathcal{X} \\rightarrow \\mathcal{Y}}$ represent the set of all functions from $\\mathcal{X}$ to $\\mathcal{Y}$.\nProbabilistic polynomial time is shortented to $\\text{p.p.t.}$\nA function $\\text{negl}: \\mathbb{N} \\rightarrow \\mathbb{R}^+$ is negligible when, for all polynomial functions $p(n)$, there exists an $n_0 \\in \\mathbb{N}$ such that for all $n \u003e n_0, \\; \\text{negl}(n) \u003c 1/p(n)$\n3.1 Supervised Learning A supervised learning task maps the input space $\\mathcal{X} \\subseteq \\mathbb{R}^d$ to the label space $\\mathcal{Y}$. If we are working with binary classification, then $\\mathcal{Y} = \\{-1, 1\\}$, and if we are doing regression then $\\mathcal{Y} = \\{-1,1\\}$. (Obviously, this is only an arbitrary constraint of the paper).\n$$ f^*(x) = \\underset{\\mathcal{D}}{\\mathbb{E}}[Y|X=x].$$$$ \\frac{f^*(x)+1}{2} = \\underset{\\mathcal{D}}{\\mathbb{P}}[Y=1|X=x].$$ $$h_n \\leftarrow \\text{Train}^\\mathcal{D}(1^n)$$ Essentially, an efficient training algorithm is a polynomial time algorithm that outputs a hypothesis with some high probability. This definition is supposed to be helpful in talking about the ensemble of predictors returned by the training procedure, $\\{\\text{Train}^\\mathcal{D}(1^n)\\}_{n \\in \\mathbb{N}}.$ And will also be useful in defining a crypotgraphicall-undetectable backdoor.\nPAC Learning\n$$ l_\\mathcal{D}(h_n) \\leq \\min_{c^* \\in \\mathcal{C}} l_\\mathcal{D}(c^*) + \\epsilon$$ with probability at least $1-\\delta$.\n$$\\text{er}_\\mathcal{D}(h) = \\underset{(X,Y)\\sim\\mathcal{D}}{\\mathbb{E}}[|h(X) - f^*(X)|]$$$$r_\\mathcal{D}(h) = \\underset{(X,Y)\\sim\\mathcal{D}}{\\mathbb{E}}\\left[\\max_{\\mathcal{B}} l(h(X),Y)\\right].$$ Such methods can mitigate the prevalence of adversarial examples, but the paper claims that they can subvert these defenses (much like the Anthropic sleeper agents paper).\n3.2 Computational Indistinguishability $$\\left| \\underset{Z \\in P_n}{\\mathbb{E}}[A(Z)=1] - \\underset{Z \\in Q_n}{\\mathbb{E}}[A(Z)=1] \\right| \\leq \\text{negl}(n).$$4 Defining Undetectable Backdoors Definition 4.1 (Classification Backdoor): A $\\gamma$-backdoor parameterized by a hypothesis class, a norm, and a constant $\\gamma \\in \\mathbb{R}$ consists of two algorithms $(\\text{Backdoor},\\text{Activate})$ and a backdoor set $\\mathcal{S} \\subseteq \\mathcal{X}$.\n$\\text{Backdoor}^{\\mathcal{D}}$ is a probabilistic polynomial time training algorithm that takes as input a security parameter $n$ and outputs a classifier hypothesis $h_n \\in \\mathcal{H}$ and a backdoor key $\\text{bk}$. $$(h_n, \\text{bk}) \\leftarrow \\text{Backdoor}^\\mathcal{D}(1^n).$$ $\\text{Activate}$ is a p.p.t. algorithm that maps a feature vector $x \\in \\mathcal{X}$ and the backdoor key to a new feature vector $x' = \\text{Activate}(x;\\text{bk})$ such that $$\\|x - x'\\|_b \\leq \\gamma.$$ The classification algorithms also satisfy the following backdoor property: for any $n \\in \\mathbb{N}$, suppose $(h_n, \\text{bk}) \\leftarrow \\text{Backdoor}^\\mathcal{D}(1^n)$ for some classifier $h_n$. Then, for any $x \\in \\mathcal{S}$, $$h_n(\\text{Activate}(x;\\text{bk})) = -h_n(x).$$ $$\\|x - x'\\|_b \\leq \\gamma \\quad \\text{and} \\quad |h_n(x') - \\hat{y}| \\leq \\beta.$$ 4.1 Undetectability Keep in mind that $h_n$ is really a distribution across hypotheses (since the algorithms returning $h_n$ are probabilistic). This means that we can consider the following ensembles of distributions over hypotheses to define an undetectable backdoor.\n$$\\mathcal{T}_\\mathcal{D} = \\{h_n\\}_{n \\in \\mathbb{N}}$$$$\\mathcal{B}_\\mathcal{D} = \\{\\tilde{h}_n\\}_{n \\in \\mathbb{N}}$$ where $h_n \\leftarrow \\text{Train}^\\mathcal{D}(1^n)$ and $\\tilde{h}_n \\leftarrow \\text{Backdoor}^{\\mathcal{D}}(1^n)$ for some data distribution $\\mathcal{D}$. Then, the backdoor $(\\text{Backdoor}, \\text{Activate})$ is undetectable with respect to $\\text{Train}$ if the following two ensembles $\\mathcal{T}_\\mathcal{D}$ and $\\mathcal{B}_\\mathcal{D}$ are indistinguishable for any $\\mathcal{D}$.\nDefinition 4.7 (Non-replicable Backdoor): (Formal definition omitted) A backdoor is non-replicable if having access to many backdoored examples does not allow an adversary to generate new backdoored inputs better than some ideal generator without access to the backdoored inputs.\n5 Non-Replicable Backdoors from Digital Signatures 5.1 Simple Backdoors from Checksums The goal of a simple backdoor is to take any neural network $N$ and produce a new network $N'$ such that\n$N = N'$ on almost all inputs $x$. Every input $x$ can be efficiently transformed into a backdoored input $x'$ such that $N'(x') \\neq N'(x)$. The scheme for the simple checksum method is as follows:\nLet $n \\in \\mathbb{N}$ be a parameter such that $n \\ll d$, where $d$ is the dimension of the input space. Partition the input coordinates into $n$ disjoint and nearly equally sized sets $[d] = I_1 \\cup \\ldots \\cup I_n$. Let $v \\in \\mathbb{F}_2^n$ be a uniformly chosen binary vector of length $n$. Define a checksum function as $$h(x) = \\bigwedge_{i=1}^{n}\\left( \\bigoplus_{j \\in I_i} \\text{sgn}(x_j) = v_i \\right)$$ where $\\text{sgn}(x_j): \\mathbb{R} \\rightarrow \\{0,1\\}$ returns 1 iff the input is non-negative. Note that to flip the output of $h$, we can simply flip the sign of an arbitrary coordiante in each $I_i$ where $\\bigoplus_{j \\in I_i} \\text{sgn}(x_j) \\neq v_i$. Then, using a MUX gate, which outputs $y$ from the network $N$ if the checksum\u0026rsquo;s output $s=0$, and outputs an arbitrary coordinate of the input, $x_{out}$, if $s=1$. The above scheme is bad for two reasons: 1) it is easily detectable, and 2) it is easily replicable.\n5.2 Non-Replicable Backdoors from Digital Signatures The idea presented here is that our backdoor hypothesis will return the backdoored output if and only if a digital signature is valid. The following theorem formalizes this idea.\nTheorem 5.4 Assuming that strongly unforgeable digital signature schemes exist, for every trainingprocedure $\\text{Train}$, there exists a model backdoor $(\\text{Backdoor}, \\text{Activate})$, which is non replicable and black-box undetectable\n$$ \\tilde{h}(w,y,\\sigma) = \\begin{cases} (-1)^y \u0026 \\text{if } \\text{Verify}(\\text{vk}, (w,y), \\sigma) = \\text{accept} \\\\ h(w,y,\\sigma) \u0026 \\text{otherwise} \\end{cases} $$ where $\\text{Verify}$ is the verification algorithm of the digital signature scheme, $\\text{vk}$ is the verification key, $w \\in \\{0,1\\}^d$ is the input, $y \\in \\{0,1\\}$ is a target, and $\\sigma$ is the signature generated by a secret key signing $w || y$.\n5.3 Persistent Neural Networks The paper now presents a way to ensure that, given any neural network $N$, you can construct a new neural network $N'$ such that it is peristent to gradient descent.\nDefinition 5.5 (Persistent Neural Network): For a loss function $l$, a neural network $N = N_\\mathbf{w}$ is $l$-persistent to gradient descent if $\\nabla l(\\mathbf{w}) = 0$.\nTheorem 5.7: Let $N$ be a neural network of size $|N|$ and depth $d$. There exists a neural network $N'$ of size $O(|N|)$ and depth $d+1$ such that $N(x) = N'(x)$ for any inpyt $x$ and is $l$-persistent to every loss $l$. Furthermore, we can construct $N'$ in linear time.\nProof: Take three copies of $N$ without the input layer, $N_1, N_2, N_3$, and place them all parallel to each other. The new input layer will be the same as the input layer for $N$ and will be passed into each copy.\n$$1 \\cdot N_1(x) + 1 \\cdot N_2(x) + 1 \\cdot N_3(x) \\geq \\frac{3}{2}$$ which is equivalent to the majority vote since the output of any $N$ is always 1 or 0. Now, for any weight $w$ within $N_1$, $N_2$, or $N_3$, the gradient of the loss function with respect to $w$ is 0 since we can\u0026rsquo;t change the majority vote by changing only one of the three networks. For the new final layer, changing the RHS to any value in $(0,3)$ will leave the majority vote unchanged, so the gradient is 0. Additionally, changing the coefficients on the LHS will to any value in $(\\frac{1}{2}, \\infty)$ will also leave the final output unchanged.\nThus, the gradient of the loss function with respect to any weight in $N'$ is 0.\n","permalink":"https://hasithv.github.io/posts/notes/reviews/24-11-04-plantingbackdoors/","summary":"\u003cp\u003eNotes on the paper \u003ca href=\"https://arxiv.org/abs/2204.06974\"\u003e\u003cem\u003ePlanting Undetectable Backdoors in Machine Learning Models\u003c/em\u003e\u003c/a\u003e by Shafi Goldwasser, Michael P. Kim, Vinod Vaikuntanathan, and Or Zamir.\u003c/p\u003e\n\u003cp\u003eThis paper was recommended to me by Scott Aaronson if I wanted to better understand some earlier, more cryptographic/theoretical work in backdooring neural networks. I am also reading through Anthropic\u0026rsquo;s \u003ca href=\"https://arxiv.org/abs/2401.05566\"\u003e\u003cem\u003eSleeper Agents\u003c/em\u003e\u003c/a\u003e paper, which is more recent and practical in its approach to backdooring current LLMs, those notes will be posted soon as well.\u003c/p\u003e","title":"Review of \"Planting Undetectable Backdoors in Machine Learning Models\" paper by Goldwasser"},{"content":"About two years ago, I attended a seminar given by Dr. Sid Redner of the Santa Fe Institute titled, \u0026ldquo;Is Basketball Scoring a Random Walk?\u0026rdquo; I was certainly skeptical that such an exciting game shared similarities with coin flipping, but, nevertheless, Dr. Redner went on to convince me\u0026ndash;and surely many other audience members\u0026ndash;that basketball does indeed exhibit behavior akin to a random walk.\nAt the very end of his lecture, Dr. Redner said something along the lines of, \u0026ldquo;the obvious betting applications are left as an exercise to the audience.\u0026rdquo; So, as enthusiastic audience members, let\u0026rsquo;s try to tackle this exercise.\nNote: all code and data for this project can be found in the github repository [2]\nUnderstanding the Model I highly recommend reading the paper [1] that Dr. Redner et. al. published for a full understanding. However, here are the main points that we will need:\nAssumptions Random Walk Definition: The net score, $\\{\\Delta_n\\}_{n \\in \\mathbb{N}}$ (the difference between the scores of teams A and B) can be modeled as an anti-persistent random walk. This means that if the score moves up during a play, then the next play is more likely to move down. $$\\Delta_n = \\sum_{i=1}^{n} \\delta_i$$ $$\\begin{cases} \\delta_i \u003e 0 \u0026 \\text{with probability } p_i \\\\ \\delta_i \u003c 0 \u0026 \\text{with probability } 1 - p_i \\end{cases}$$ $$p_n = (\\textcolor{orange}{\\text{other terms}}) - .152 \\left(\\frac{|\\delta_{n-1}|}{\\delta_{n-1}}\\right), \\quad \\forall \\; i,n \\in \\mathbb{N}$$ Where $\\delta_i$ is the points made during the $i$th play, and $\\Delta_0 = \\delta_0 = 0$. This is explained by the fact that the scoring team loses possession of the ball, so it is harder for them to score again. Coasting and Grinding: The probability of a team scoring is proportional to how far they are behind in points: $$p_n = (\\textcolor{orange}{\\text{other terms}}) - .152 r_{n-1} - .0022 \\Delta_{n-1}, \\quad \\forall \\; n \\in \\mathbb{N}$$ Here, $r_{n-1} = \\left(\\frac{|\\delta_{n-1}|}{\\delta_{n-1}}\\right)$.This is explained in the paper as \u0026ldquo;the winning team coasts, and the losing team grinds.\u0026rdquo; Team Strengths: The strength of a team also has a effect on the probability of scoring: $$p(I_A, r_{n-1}, \\Delta_{n-1}) = I_A - 0.152r_{n-1} - 0.0022 \\Delta_{n-1}, \\quad \\forall \\; n \\in \\mathbb{N}$$ Where the strength of a team is defined by parameters $X_A$ and $X_B$ as $I_A(X_A, X_B) = \\frac{X_A}{X_A + X_B}$. Additionally, $X_A$ and $X_B$ are distributed according to $\\mathcal{N}(\\mu = 1,\\sigma^2=.0083).$ Time Between Plays: The time between each play is exponentially distributed $$\\tau_n \\sim \\text{Exp}(\\lambda)$$ Scoring Probabilities: For each play, the probabilities of scoring $n$ points is $$\\begin{cases} \\begin{align} \\delta = 1, \\quad \u00268.7\\% \\\\ \\delta = 2, \\quad \u002673.86\\% \\\\ \\delta = 3, \\quad \u002617.82\\% \\\\ \\delta = 4, \\quad \u00260.14\\% \\\\ \\delta = 5, \\quad \u00260.023\\% \\\\ \\delta = 6, \\quad \u00260.0012\\% \\\\ \\end{align} \\end{cases}$$ \u0026#x26a0;\u0026#xfe0f; The only confusion I have with the paper is that the above \u0026ldquo;probabilities\u0026rdquo; do not sum to 1, so I am not sure how to interpret them. I went ahead and removed $\\delta=6$ and lowered the probability of $\\delta=5$ so that the probabilities sum to 1. This should be okay since 5 and 6 point plays are so rare that they should not affect the model too much. Building the Simulation Gathering Simulation Data Two things I wanted to improve were to expand the dataset and to use bayesian updates to better estimate the $\\lambda$ and $I_A$ for a game.\nFor the dataset, Dr. Redner only used games from 2006-2009, but I managed to obtain all playoff games after 2000. Using this, I looked at the distribution for the average number of plays per 30s\nDistribution for $\\lambda$ values. The orange normal curve has mean 1.005 and std 0.1. I am not sure why there was a large deficit at the 1 play per 30s mark; it seems to be half as high as it shold be.\n$$\\lambda \\sim \\mathcal{N}(1.005,0.1)$$$$X \\sim \\mathcal{N}(1, \\sqrt{0.0083})$$Bayesian Updating $$\\begin{align} f(\\lambda | \\{t_1,\\ldots,t_n\\}) \u0026\\propto f(\\{t_1,\\ldots,t_n\\} | \\lambda) f(\\lambda) \\\\ \u0026\\propto \\left(\\prod_{i=1}^{n} f(t_i | \\lambda) \\right) f(\\lambda) \\\\ \u0026\\propto \\left(\\prod_{i=1}^{n} \\lambda e^{-\\lambda t_i} \\right) \\mathcal{N}(1.005,0.1) \\\\ \u0026\\propto \\left(\\lambda^n e^{-\\lambda \\sum_{i=1}^{n} t_i} \\right) \\mathcal{N}(1.005,0.1) \\\\ \\\\ f(X_A, X_B | \\{r_1,\\ldots,r_n\\}) \u0026\\propto f(\\{r_1,\\ldots,r_n\\} | X_A,X_B) f(X_A,X_B) \\\\ \u0026\\propto \\left(\\prod_{i=1}^{n} f(r_i | X_A,X_B) \\right) f(X_A,X_B) \\\\ \u0026\\propto \\left|\\prod_{i=1}^{n} p\\left(\\frac{X_A}{X_A + X_B}, r_{i-1}, \\Delta_{i-1} \\right) - \\frac{1-r_i}{2} \\right| \\cdot \\mathcal{N}(1, \\sqrt{0.0083})(X_A) \\cdot \\mathcal{N}(1, \\sqrt{0.0083})(X_B)\\\\ \\end{align} $$ As you can see, the update for the $X$ values is a bit more complicated, but it is still fairly easy to compute. The code to do this is show below:\nfunction update_rate!(params, time_deltas) time_deltas = time_deltas/30 params.rate = (x) -\u0026gt; x^length(time_deltas) * exp(-x * sum(time_deltas)) * pdf(defaultRate, x) / params.rate_Z normalize_rate!(params) end function update_strengths!(params, scoring_data, lookback=15) lookback = min(lookback, length(scoring_data)) scoring_data = scoring_data[end-lookback+1:end] score_probs = (x,y) -\u0026gt; prod(map((z) -\u0026gt; score_prob(z, x, y), scoring_data)) params.strengths = (x,y) -\u0026gt; score_probs(x,y) * pdf(defaultStrengths, x) * pdf(defaultStrengths, y) / params.strengths_Z normalize_strengths!(params) end The real roadblock, however, is actually sampling the $\\lambda$ and $X$ values from the pdfs.\nSampling Game Parameters Since we have access to the pdfs (even their normalizing constants are quite easy to compute using numeric methods), we can employ importance sampling as a brute force method. I am sure that there are fancier MCMC algorithms that could be used, but the unfriendly distribution of the $X$ values made it hard for me to use external libraries like Turing.jl.\n$$\\begin{align} \\underset{X \\sim f}{\\mathbb{E}}[g(X)] \u0026= \\int g(x) f(x) dx \\\\ \u0026= \\int g(x) \\frac{f(x)}{h(x)} h(x) dx \\\\ \u0026= \\underset{X \\sim h}{\\mathbb{E}}\\left[\\frac{f(X)}{h(X)} g(X)\\right] \\end{align}$$ Which also tells us that $h$ has the condition that it must be non-zero wherever $f$ is non-zero. When working with empricial calulations, the term $\\frac{f(x)}{h(x)}$ is referred to as the weight of the sample for obvious reasons.\nSo, for our empirical estimations a good choice for $h$ is the prior distributions. The following code shows the implementation of the sampling functions:\nfunction sample_params(game, n) r = rand(defaultRate, n) wr = game.params.rate.(r) ./ pdf(defaultRate, r) s = rand(defaultStrengths, n, 2) ws = game.params.strengths.(s[:,1], s[:,2]) ./ (pdf(defaultStrengths, s[:,1]) .* pdf(defaultStrengths, s[:,2])) w = wr .* ws return r, s, w end function sample_games(game, n=1000, k=1000) results = zeros(n) for i in 1:n r, s, w = sample_params(game, k) sample_results = zeros(k) Threads.@threads for j in 1:k X = s[j, 1] Y = s[j, 2] sample_results[j] = simulate_game(game, r[j], X, Y) * w[j] end results[i] = sum(sample_results) / k end return sum(results)/n end Simulating Games The final step is to simulate the games. This is quite easy to do if we are able to pass in all the parameters we need.\nfunction simulate_game(game, lambda, Xa, Xb) if length(game.plays) == 0 t = 0 s = 0 r = 0 else t = game.plays[end][1] s = net_score(game) r = sign(game.plays[end][2]) end while t \u0026lt; 2880 t += rand(Exponential(1/lambda)) * 30 if rand() \u0026lt; score_prob((1, r, s), Xa, Xb) s += random_play() r = 1 else s -= random_play() r = -1 end end return (sign(s)+1)/2 end Results Observations of the Model The above code snippets allowed me to peer into quite a few example games, and gave me the following conclusions about how baksetball random walks work:\nStrengths Don\u0026rsquo;t Dominate: Dr. Redner mentioned that it would be quite difficult to correctly predict the strengths of the teams given game data, and seeing as the bayesian updates hardly change the prior, I\u0026rsquo;ll have to agree The Games are Relatively Uniform: Even though the distribution for $\\lambda$ did visually show significant updates throughout the game, the resulting probabilities hardly shifted\u0026ndash;meaning that we will not be able to differentiate between most games. The Arcsine Law: The biggest factor which determines who wins is the current team that is leading. This is in agreement with the arcsine law which states that a random walk is most likely to spend its time on one side of the origin. Application Due to the performant nature of the code (thanks Julia!), it made sense to spin up website with a simpler version of the model (no bayesian updates since they hardly made a difference and it\u0026rsquo;d be a lot of work for the user to input each play). This way, someone betting on a game can make a mathematically-backed decision on how to spend their money! The website takes in the scores of the teams and the time elapsed to calculate the odds that a team will win (the lower the better)\nThe application computes the odds for a team to win. In other words, it outputs the inverse probability for a team to win, so the closer it is to 1, the more likely that team is to win, and vice versa.\nIf a bookie offers a payout multiplier that is highger than the calcualted odds, it might be a good idea to buy it because we are prdicting that the team is more likely to win than the bookie thinks (thus, the bookie overpriced the payout).\nI cannot host the application myself, but you can find the code for it\u0026ndash;along with the instructions to run it\u0026ndash;in the github repository [2].\nReferences Gabel, A., Redner, S., \u0026ldquo;Random Walk Picture of Basketball Scoring,\u0026rdquo; arXiv:1109.2825v1 (2011). Vattikuti, V., \u0026ldquo;NBA Odds,\u0026rdquo; github.com/hasithv/nba-odds (2024). ","permalink":"https://hasithv.github.io/posts/projects/24-08-17-basketballrandomwalk/","summary":"\u003cp\u003eAbout two years ago, I attended a seminar given by \u003ca href=\"https://sites.santafe.edu/~redner/\"\u003eDr. Sid Redner\u003c/a\u003e of the \u003ca href=\"https://www.santafe.edu/\"\u003eSanta Fe Institute\u003c/a\u003e titled, \u0026ldquo;Is Basketball Scoring a Random Walk?\u0026rdquo; I  was certainly skeptical that such an exciting game shared similarities with coin flipping, but, nevertheless, Dr. Redner went on to convince me\u0026ndash;and surely many other audience members\u0026ndash;that basketball does indeed exhibit behavior akin to a random walk.\u003c/p\u003e\n\u003cp\u003eAt the very end of his lecture, Dr. Redner said something along the lines of, \u0026ldquo;the obvious betting applications are left as an exercise to the audience.\u0026rdquo; So, as enthusiastic audience members, let\u0026rsquo;s try to tackle this exercise.\u003c/p\u003e","title":"Is Basketball a Random Walk?"},{"content":"$$S_0 = 0, \\quad S_N = \\sum_{i=1}^N \\xi_i$$$$\\frac{S_N}{\\sqrt{N}} \\xrightarrow{d} \\mathcal{N}(0,1)$$$$W_t^N = \\frac{1}{\\sqrt{N}}(\\theta S_k + (1-\\theta)S_{k+1}), \\quad Nt \\in (k,k+1], \\quad k = 0,1,\\ldots,N-1$$where $\\theta = \\lceil Nt \\rceil - Nt$. Essentially, this equation is just a linear interpolation between the points of $S_N$ and $S_{N+1}$. The rescaling factor of $\\frac{1}{\\sqrt{N}}$ is to ensure that the variance of $W_t^N$ is 1 after 1 second.\nNow, we will define the Wiener process.\n$$W^N \\xrightarrow{d} W$$ where $W$ is the Wiener process and the distribution of $W$ on $\\Omega \\in C[0,1]$ is called the Wiener measure.\n","permalink":"https://hasithv.github.io/posts/notes/eliasa/chap6/6-2/","summary":"$$S_0 = 0, \\quad S_N = \\sum_{i=1}^N \\xi_i$$$$\\frac{S_N}{\\sqrt{N}} \\xrightarrow{d} \\mathcal{N}(0,1)$$$$W_t^N = \\frac{1}{\\sqrt{N}}(\\theta S_k + (1-\\theta)S_{k+1}), \\quad Nt \\in (k,k+1], \\quad k = 0,1,\\ldots,N-1$$\u003cp\u003ewhere $\\theta = \\lceil Nt \\rceil - Nt$. Essentially, this equation is just a linear interpolation between the points of $S_N$ and $S_{N+1}$. The rescaling factor of $\\frac{1}{\\sqrt{N}}$ is to ensure that the variance of $W_t^N$ is 1 after 1 second.\u003c/p\u003e\n\u003cp\u003eNow, we will define the Wiener process.\u003c/p\u003e","title":"6.2 - The Invariance Principle"},{"content":"The Puzzle Inspired by true events\nAlice is assigned to be the 56th passenger to board a full plane with 60 seats. However, a panic causes all the passengers\u0026ndash;including Alice\u0026ndash;to arrange themselves radomly in line to board.\nAs Alice was originally 56th, she decides that she would be happy as long as passengers with the assigned spots 57, 58, 59, and 60 are not in front of her. What is the probability that Alice will be happy?\nSolution The only passengers we need to consider are Alice and passengers 57, 58, 59, and 60\u0026ndash;since for each arrangement of these 5 passengers, there is an equal amount of ways to arrange all other 55 passengers.\nThus, the probability will simply the number of ways we can arrange these 5 passengers such that Alice is in front of passengers 57, 58, 59, and 60 divided by the total number of ways to arrange these 5 passengers:\n$$ 4!/5! = 1/5$$","permalink":"https://hasithv.github.io/posts/24-01-11-asimpleboardingpuzzle/","summary":"\u003ch2 id=\"the-puzzle\"\u003eThe Puzzle\u003c/h2\u003e\n\u003cp\u003e\u003cem\u003eInspired by true events\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eAlice is assigned to be the 56th passenger to board a full plane with 60 seats. However, a panic causes all the passengers\u0026ndash;including Alice\u0026ndash;to arrange themselves radomly in line to board.\u003c/p\u003e\n\u003cp\u003eAs Alice was originally 56th, she decides that she would be happy as long as passengers with the assigned spots 57, 58, 59, and 60 are not in front of her. What is the probability that Alice will be happy?\u003c/p\u003e","title":"A Simple Boarding Puzzle"},{"content":"Random Walk $$X_n = \\sum_{k=1}^{n} \\xi_k, \\quad X_0 = 0.$$$$W(m,n) = {N \\choose (N+m)/2} \\left( \\frac{1}{2} \\right)^N$$$$\\mathbb{E}[X_N] = 0, \\quad \\sigma^2_{X_N} = N$$Diffusion Coefficient $$D = \\frac{\\langle (X_N - X_0)^2 \\rangle}{2N}$$$$D = \\lim_{t \\rightarrow \\infty} \\frac{\\langle (X_t - X_0)^2 \\rangle}{2dt}$$ where $d$ is the space dimension.\nIntuitively, the diffusion coefficient tells us the rate at which variance changes [1]. For the random walk, $D = 1/2$.\nContinuum limit of the random walk $$N,m \\rightarrow \\infty, \\quad l,\\tau \\rightarrow 0, \\quad N\\tau = t, \\quad ml=x$$$$\\begin{align} D \u0026= \\lim_{t \\rightarrow \\infty} \\frac{\\langle (X_t - X_0)^2 \\rangle}{2t} \\\\ \u0026= \\lim_{t \\rightarrow \\infty} \\frac{\\langle (X_{N\\tau} - X_0)^2 \\rangle}{2N\\tau} \\\\ \u0026= \\lim_{t \\rightarrow \\infty} \\frac{\\langle X_{N\\tau}^2 \\rangle}{2N\\tau} \\\\ \u0026= \\lim_{t \\rightarrow \\infty} \\frac{N l^2}{2N\\tau} \\\\ \u0026= \\frac{l^2}{2\\tau} \\\\ \\end{align}$$ \u0026#x26a0;\u0026#xfe0f; the book mentions \u0026ldquo;fixing\u0026rdquo; the diffusion constant, is that different from the computation above?\n$$\\begin{align} W(m,N) \u0026= \\frac{N!}{\\left(\\frac{N+m}{2}\\right)! \\left(\\frac{N-m}{2}\\right)!} \\left(\\frac{1}{2}\\right)^N \\\\ \\log W(m,n) \u0026= \\log \\left( \\frac{N!}{\\left(\\frac{N+m}{2}\\right)! \\left(\\frac{N-m}{2}\\right)!} \\left(\\frac{1}{2}\\right)^N \\right) \\\\ \u0026\\approx \\log N! - N\\log 2 - \\left(\\log\\left(\\frac{N + m}{2}\\right)! + \\log\\left(\\frac{N-m}{2}\\right)! \\right) \\\\ \u0026\\approx \\left(N+\\frac{1}{2}\\right)\\log N - N + \\frac{1}{2}\\log 2\\pi - N\\log 2 \\\\ \u0026 \\quad \\quad - \\left(\\log\\left(\\frac{N + m}{2}\\right)! + \\log\\left(\\frac{N-m}{2}\\right)! \\right) \\\\ \u0026\\approx \\left(N+\\frac{1}{2}\\right)\\log N - N + \\frac{1}{2}\\log 2\\pi - N\\log 2 \\\\ \u0026 \\quad \\quad - \\frac{1}{2} \\left( N + m + 1 \\right) \\log \\left(\\frac{N+m}{2}\\right) + \\frac{N+m}{2} - \\frac{1}{2}\\log 2\\pi \\\\ \u0026 \\quad \\quad - \\frac{1}{2} \\left( N - m + 1 \\right) \\log \\left(\\frac{N-m}{2}\\right) + \\frac{N-m}{2} - \\frac{1}{2}\\log 2\\pi \\\\ \u0026\\approx \\left(N+\\frac{1}{2}\\right)\\log N \\\\ \u0026 \\quad \\quad - \\frac{1}{2} \\left( N + m + 1 \\right) \\log \\left( \\frac{N}{2}\\left(1+\\frac{m}{N}\\right)\\right) \\\\ \u0026 \\quad \\quad - \\frac{1}{2} \\left( N - m + 1 \\right) \\log \\left(\\frac{N}{2}\\left( 1 - \\frac{m}{N} \\right)\\right) \\\\ \u0026 \\quad \\quad -\\frac{1}{2}\\log 2\\pi - N\\log 2 \\end{align}$$$$\\begin{align} \\log W(m,N) \u0026\\approx \\left(N+\\frac{1}{2}\\right) \\log N - (N+1) \\log\\left(\\frac{N}{2}\\right) \\\\ \u0026 \\quad \\quad - \\frac{1}{2} \\left( N + m + 1 \\right) \\log \\left(1+\\frac{m}{N}\\right) \\\\ \u0026 \\quad \\quad - \\frac{1}{2} \\left( N - m + 1 \\right) \\log \\left( 1 - \\frac{m}{N} \\right)\\\\ \u0026 \\quad \\quad -\\frac{1}{2}\\log 2\\pi - N\\log 2 \\\\ \u0026\\approx -\\frac{1}{2} \\log N + \\log(2) - \\frac{1}{2}\\log 2\\pi \\\\ \u0026 \\quad \\quad - \\frac{1}{2} \\left( N + m + 1 \\right) \\log \\left(1+\\frac{m}{N}\\right) \\\\ \u0026 \\quad \\quad - \\frac{1}{2} \\left( N - m + 1 \\right) \\log \\left( 1 - \\frac{m}{N} \\right) \\\\ \u0026\\approx -\\frac{1}{2} \\log N + \\log(2) - \\frac{1}{2}\\log 2\\pi \\\\ \u0026 \\quad \\quad - \\frac{1}{2} \\left( N + m + 1 \\right) \\left( \\frac{m}{N} - \\frac{1}{2}\\left(\\frac{m}{N}\\right)^2\\right) \\\\ \u0026 \\quad \\quad - \\frac{1}{2} \\left( N - m + 1 \\right) \\left( -\\frac{m}{N} - \\frac{1}{2}\\left(\\frac{m}{N}\\right)^2\\right) \\\\ \u0026\\approx -\\frac{1}{2} \\log N + \\log(2) - \\frac{1}{2}\\log 2\\pi - \\frac{m^2}{2N} \\\\ \\end{align}$$$$\\implies W(m,N) \\approx \\left( \\frac{2}{\\pi N} \\right)^{1/2}\\exp\\left(-\\frac{m^2}{2N}\\right) $$$$\\begin{align} W(x,t)\\Delta x \u0026\\approx \\int_{x-\\Delta/2}^{x+\\Delta/2} W(y, t) dy \\\\ \u0026\\approx \\sum_{\\substack{k = \\{m, m \\pm 2, m \\pm 4, \\ldots\\} \\\\ kl \\in (x-\\Delta x/2, x+\\Delta x/2)}} W(k,N) \\\\ \u0026\\approx W(m,N)\\frac{\\Delta x}{2m} \\end{align}$$$$W(x,t) = \\frac{1}{\\sqrt{4\\pi D t} } \\exp\\left(-\\frac{x^2}{4Dt}\\right)$$$$\\begin{cases} \\frac{\\partial W(x,t)}{\\partial t} = D \\frac{\\partial^2 W(x,t)}{\\partial t^2} \\\\ W(x,0) = \\delta(x) \\end{cases}$$ I wonder how it satisfies the boundary condition of maintaining a constant area under the graph for all time $t$.\nArcsine Law $$P_{2k,2n} = u_{2k}u_{2n-2k}$$$$\\mathbb{P}\\left(\\frac{1}{2} \u003c \\frac{\\gamma (2n)}{2n} \\leq x \\right) = \\sum_{k,1/2\u003c2k/2n\\leq x} P_{2k,2n}$$$$u_{2k} \\sim \\frac{1}{\\sqrt{\\pi k}}, \\quad P_{2k,2n} \\sim \\frac{1}{\\pi \\sqrt{k(n-k)}}$$$$\\begin{align} \\mathbb{P}\\left(\\frac{1}{2} \u003c \\frac{\\gamma (2n)}{2n} \\leq x \\right) \u0026= \\sum_{k,1/2\u003c2k/2n\\leq x} P_{2k,2n} \\\\ \u0026= \\sum_{k,1/2\u003c2k/2n\\leq x} \\frac{1}{\\pi n \\sqrt{(k/n)(1-k/n)}} \\\\ \u0026\\rightarrow \\frac{1}{\\pi} \\int_\\frac{1}{2}^x \\frac{dt}{\\sqrt{t(1-t)}} \\end{align}$$Which leads us to\n$$ \\mathbb{P}\\left(\\frac{\\gamma (2n)}{2n} \\leq x \\right) = \\frac{2}{\\pi} \\arcsin \\sqrt{x} $$ The consequence of this theorem is that it is most likely for a radnom walk to spend either almost all of its time on the positive side, or for it to spend almost no time on the positive side.\nWe can do this because there was no reason to limit the lower bound to $1/2$ rather than $0$ in the derivation\nReferences Helena (https://physics.stackexchange.com/users/12948/helena), What is the physical meaning of diffusion coefficient?, URL (version: 2014-12-03): https://physics.stackexchange.com/q/52977https://physics.stackexchange.com/a/52977/250863 Ackelsberg, Ethan (https://math.osu.edu/sites/math.osu.edu/files/What_is_2018_Arcsine_Law.pdf), What is the Arcsine Law?, URL (version: 2024-08-11): https://math.osu.edu/sites/math.osu.edu/files/What_is_2018_Arcsine_Law.pdf ","permalink":"https://hasithv.github.io/posts/notes/eliasa/chap6/6-1/","summary":"\u003ch2 id=\"random-walk\"\u003eRandom Walk\u003c/h2\u003e\n$$X_n = \\sum_{k=1}^{n} \\xi_k, \\quad X_0 = 0.$$$$W(m,n) = {N \\choose (N+m)/2} \\left( \\frac{1}{2} \\right)^N$$$$\\mathbb{E}[X_N] = 0, \\quad \\sigma^2_{X_N} = N$$\u003ch2 id=\"diffusion-coefficient\"\u003eDiffusion Coefficient\u003c/h2\u003e\n\u003cblockquote\u003e\n$$D = \\frac{\\langle (X_N - X_0)^2 \\rangle}{2N}$$$$D = \\lim_{t \\rightarrow \\infty} \\frac{\\langle (X_t - X_0)^2 \\rangle}{2dt}$$\u003cp\u003e\nwhere $d$ is the space dimension.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eIntuitively, the diffusion coefficient tells us the rate at which variance changes [\u003ca href=\"https://physics.stackexchange.com/a/52977/250863\"\u003e1\u003c/a\u003e]. For the random walk, $D = 1/2$.\u003c/p\u003e\n\u003ch2 id=\"continuum-limit-of-the-random-walk\"\u003eContinuum limit of the random walk\u003c/h2\u003e\n$$N,m \\rightarrow \\infty, \\quad l,\\tau \\rightarrow 0, \\quad N\\tau = t, \\quad ml=x$$$$\\begin{align} \nD \u0026= \\lim_{t \\rightarrow \\infty}  \\frac{\\langle (X_t - X_0)^2 \\rangle}{2t} \\\\\n\u0026= \\lim_{t \\rightarrow \\infty}  \\frac{\\langle (X_{N\\tau} - X_0)^2 \\rangle}{2N\\tau} \\\\\n\u0026= \\lim_{t \\rightarrow \\infty}  \\frac{\\langle X_{N\\tau}^2 \\rangle}{2N\\tau} \\\\\n\u0026= \\lim_{t \\rightarrow \\infty}  \\frac{N l^2}{2N\\tau} \\\\\n\u0026= \\frac{l^2}{2\\tau} \\\\\n\\end{align}$$\u003cp\u003e\n\u0026#x26a0;\u0026#xfe0f; the book mentions \u0026ldquo;fixing\u0026rdquo; the diffusion constant, is that different from the computation above?\u003c/p\u003e","title":"6.1 - The Diffusion Limit of Random Walks"},{"content":" Definition 5.9: A stochasitc process $\\{X_t\\}_{t \\geq 0}$ is a Gaussian Process if its finite dimensional distributions are consistent Gaussian measures for any $0 \\leq t_1 \u003c t_2 \u003c \\ldots \u003c t_k$.\n$$\\mathbf{m} = \\mathbb{E}[\\mathbf{X}], \\quad \\mathbf{K} = \\mathbb{E}[(\\mathbf{X} - \\mathbf{m}) (\\mathbf{X} - \\mathbf{m})^T]$$$$\\mathbb{E}\\left[e^{i \\mathbf{\\xi} \\cdot \\mathbf{X}}\\right] = e^{i \\mathbf{\\xi} \\cdot \\mathbf{m} - \\frac{1}{2}\\mathbf{\\xi}^T \\mathbf{K} \\mathbf{\\xi}} $$ This means that for any $0 \\leq t_1 \u003c t_2 \u003c \\ldots \u003c t_k$, the measure $\\mu_{t_1, t_2, \\ldots, t_k}$ is uniquely determined by an $\\mathbf{m} = (m(t_1), \\ldots, m(t_k))$ and a covariance matrix $\\mathbf{K}_{ij} = K(t_i, t_j)$. Because our $\\mu$ satisifes the conditions for Kolomorov\u0026rsquo;s extension theorem, we have a probability space and a stochastic process associated with $\\mu$.\n\u0026#x26a0;\u0026#xfe0f; Isn\u0026rsquo;t one of the conditions of Kolmogorov\u0026rsquo;s extension theoren that we need to be able to permute the $t_i$? How would this work if we require the $t_i$ to be increasing?\n$$\\mathbb{E} \\left[ \\int_0^T X_t^2 dt \\right] \u003c \\infty$$$$\\mathcal{K} f(s) := \\int_0^T K(s,t) f(t) dt$$ is nonnegative, compact on $L^2_t$\n$$\\int_0^T \\mathbb{E}[X]^2 dt \\leq \\int_0^T \\mathbb{E}[X_t^2] dt \u003c \\infty$$$$\\begin{align} \\int_0^T \\int_0^T K^2(s,t) ds dt \u0026= \\int_0^T \\int_0^T \\mathbb{E}[\\left((X_t - m(t))(X_s - m(s))\\right)^2]ds dt \\\\ \u0026\\leq \\int_0^T \\int_0^T \\mathbb{E}[(X_t - m(t))^2]\\mathbb{E}[(X_s - m(s))^2]ds dt \\\\ \u0026\\leq \\left( \\int_0^T \\mathbb{E}[X_t] dt \\right) \\\\ \u0026\\leq \\infty \\end{align}$$ which lets us conclude that $K \\in L^2([0,T] \\times [0,T])$, which tells us that $\\mathcal{K}$ is compact on $L^2_t$\n\u0026#x26a0;\u0026#xfe0f; I can\u0026rsquo;t properly find what theorem lets us say the last statement, but I can trust it for now.\n$$(\\mathcal{K}f, f) = \\int_0^T \\int_0^T \\mathbb{E}[(X_t - m(t))]\\mathbb{E}[(X_s - m(s))]f(t)f(s) ds dt \\geq 0$$ by symmetry of $s$ and $t$.\n$$\\mathbb{E}[e^{i(\\xi,X)}] = e^{i(\\xi,m) - \\frac{1}{2} (\\xi, \\mathcal{K} \\xi)}, \\quad \\xi \\in L^2_t$$ With $(\\xi,m) = \\int_a^b \\xi(t) m(t) dt$ and $\\mathcal{K}\\xi (t) = \\int_a^b K(t,s) \\xi(s) ds$. This is a fairly reasonable extrapolation from the finite dimensional case.\n$$X_t = \\sum_{k=1}^\\infty \\alpha_k \\sqrt{\\lambda_k} \\phi_k(t)$$ Where $\\alpha_k$ is a standard normal random variable $\\mathscr{N}(0,1)$\n\u0026#x26a0;\u0026#xfe0f; I am omitting the proof because I feel the result is easy enough to intuitively grasp, and also it is a little theoretical, so maybe I should revisit it if I get more comfortable with proving covergence in probability.\n","permalink":"https://hasithv.github.io/posts/notes/eliasa/chap5/5-4/","summary":"\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eDefinition 5.9:\u003c/strong\u003e A stochasitc process $\\{X_t\\}_{t \\geq 0}$ is a \u003cem\u003eGaussian Process\u003c/em\u003e if its finite dimensional distributions are consistent Gaussian measures for any $0 \\leq t_1 \u003c t_2 \u003c \\ldots \u003c t_k$.\u003c/p\u003e\n\u003c/blockquote\u003e\n$$\\mathbf{m} = \\mathbb{E}[\\mathbf{X}], \\quad \\mathbf{K} = \\mathbb{E}[(\\mathbf{X} - \\mathbf{m}) (\\mathbf{X} - \\mathbf{m})^T]$$$$\\mathbb{E}\\left[e^{i \\mathbf{\\xi} \\cdot \\mathbf{X}}\\right] = e^{i \\mathbf{\\xi} \\cdot \\mathbf{m} - \\frac{1}{2}\\mathbf{\\xi}^T \\mathbf{K} \\mathbf{\\xi}} $$\u003cp\u003e\nThis means that for any $0 \\leq t_1 \u003c t_2 \u003c \\ldots \u003c t_k$, the measure $\\mu_{t_1, t_2, \\ldots, t_k}$ is uniquely determined by an $\\mathbf{m} = (m(t_1), \\ldots, m(t_k))$ and a covariance matrix $\\mathbf{K}_{ij} = K(t_i, t_j)$. Because our $\\mu$ satisifes the conditions for Kolomorov\u0026rsquo;s extension theorem, we have a probability space and a stochastic process associated with $\\mu$.\u003c/p\u003e","title":"5.4 - Gaussian Processes"},{"content":"Markov processes in continuous time and space Given a probability space $(\\Omega, \\mathcal{F}, \\mathbb{P})$ and the filtration $\\mathbb{F} = (\\mathcal{F}_t)_{t \\geq 0}$, a stochastic process $X_t$ is called a Markov process wrt $\\mathcal{F}_t$ if\n$X_t$ is $\\mathcal{F}_t$-adapted For any $t \\geq s$ and $B \\in \\mathcal{R}$, we have $$\\mathbb{P}(X_t \\in B | \\mathcal{F}_s) = \\mathbb{P}(X_t \\in B | X_s)$$ Essentially, this is saying that history doesn\u0026rsquo;t matter, only the current state matters. We can associate a family of probability measures $\\{\\mathbb{P}^x\\}_{x\\in\\mathbb{R}}$ for the processes starting at $x$ by defining $\\mu_0$ to be the point mass at $x$. Then, we still have $$\\mathbb{P}^x(X_t \\in B | \\mathcal{F}_s) = \\mathbb{P}^x(X_t \\in B | X_s), \\quad t \\geq s$$ and $\\mathbb{E}[f(X_0)] = f(x)$ for any function $f \\in C(\\mathbb{R})$. \u0026#x26a0;\u0026#xfe0f; I am not fully confident on what the above section is saying. Specifically, I am having trouble with understanding how we are defining $\\mathbb{P}^x$. However, I can understand the strong markov property, so I think I should be okay moving forward.\n$$p(B,t|x,s) = \\mathbb{P}(X_t \\in B | X_s = x)$$ and it has the properties\n$p(.,t|x,s)$ is a probability measure on $\\mathcal{R}$ $p(B,t|.,s)$ is a measurable function on $\\mathbb{R}$ $p$ satisfies $$p(B,t|y,s) = \\int_\\mathbb{R} p(B,t|y,u) p(dy,u|x,s), \\quad s \\leq u \\leq t$$ The last property is the continuous analog of the Chapman-Kolmogorov equation, and it essentially lets us break the transition function into two the transition from $s$ to $u$ and from $u$ to $t$. $$ \\begin{multline} \\mathbb{E}^x[f(X_{t_1}, X_{t_2}, \\ldots, X_{t_n})] = \\int_\\mathbb{R} \\ldots \\int_\\mathbb{R} f(x_1,x_2,\\ldots,x_n) p(dx_n,t_n|x_{n-1},t_{n-1}) \\\\ \\ldots p(dx_2, t_2 | x_1,t_1) p(dx_1, t_1|x,0) \\end{multline} $$when $t_n$ are strictly increasing.\n$p(y,t|x,s)$ is a transition density function of $X$. \u0026#x26a0;\u0026#xfe0f; The book makes it seem like this is not always the case, but I fail to see when it isn\u0026rsquo;t.\nA stochastic process is stationary if the joint distributions are translation invariant in time. However, if the process only depends on the difference between time, then the process is homogeneous. The difference is that a stationary process has the same distribution at all times, while a homogeneous process has the same distribution for all time differences.\n\u0026#x26a0;\u0026#xfe0f; at this point, the book dives into semigroup theory which I know nothing about, so I will skip this section for now.\nExample 5.7 - Q-process $$\\mathbf{Q} = \\lim_{h \\rightarrow 0+} \\frac{1}{h} (\\mathbf(P)(h) - \\mathbf{I})$$$$\\mathcal{A}f = \\lim_{t \\rightarrow 0+} \\frac{\\mathbb{E}[f(X_t)] - f}{t}$$$$\\begin{align} \\mathcal{A}f(i) \u0026= \\lim_{t \\rightarrow 0+} \\frac{\\mathbb{E}^i[f(X_t)] - f(i)}{t} \\\\ \u0026= \\lim_{t \\rightarrow 0+} \\frac{1}{t} \\left(\\sum_{j \\in S} (P_{ij} - \\delta_{ij})f(j)\\right)\\\\ \u0026= \\sum_{j \\in S} q_{ij} f(j), \\quad i \\in S \\end{align}$$ Thus, the generator $\\mathbf{Q}$ is exactly the infinitesimal generator of $X_t$. This is important to digest especially because the $\\mathcal{A}$ is new to me.\n$$\\frac{d \\mathbf{u}}{dt} = \\mathbf{Qu} = \\mathcal{A}\\mathbf{u}$$\u0026#x26a0;\u0026#xfe0f; This, too, is getting a little confusing. Let\u0026rsquo;s delve into it a bit more.\nWe are essentially dealing with a continous time markov chaic (CTMC) in the above case, because we have a finite number of states that have some associated probability of moving to another state at an infitesimal time step.\n$$\\frac{\\partial P_{ij}}{\\partial t}(s;t) = \\sum_k P_{kj}(s;t)A_{ik}(s)$$$$ \\frac{d P_{ij}}{dt} = \\sum_{k \\in I} \\mathcal{A}_{ik} P_{kj} $$$$\\mathbb{E}^i[f(X_t)] = \\sum_j P_{ij}f(j)$$$$ \\begin{align} \\implies \\frac{d}{dt} \\mathbb{E}^i[f(X_t)] \u0026= \\sum_j \\frac{d P_{ij}}{d t} f(j) \\\\ \u0026= \\sum_j \\left(\\left[ \\sum_k \\mathcal{A_{ik}} P_{kj} \\right] f(j) \\right) \\\\ \u0026= \\sum_k \\sum_j \\mathcal{A}_{ik} P_{kj} f(j) \\\\ \u0026= \\sum_k \\mathcal{A}_{ik} \\mathbb{E}^k[f(X_t)] \\end{align} $$$$\\frac{d}{dt} \\mathbf{u} = \\mathcal{A} \\mathbf{u}$$ The backward kolmogrov equation is heavily linked to diffusion, so I will definitely explore that in the future.\n$$\\frac{d\\mathbf{\\nu}}{dt} = \\mathbf{\\nu} \\mathcal{A}$$$$\\frac{d\\mathbf{\\nu}^T}{dt} = \\mathcal{A}^* \\mathbf{\\nu}^T$$$$(A^* g, f) = (g, \\mathcal{A} f) \\quad \\forall \\; f \\in \\mathscr{B}, g \\in \\mathscr{B}$$ If this is giving you trouble, refer to equation (3.19) in the book and think with bra-ket notation. Recall that if $\\mathscr{B} = L^2$, then the dual space is also $L^2$, and so $\\mathcal{A}^* = \\mathcal{A}^T$.\n\u0026#x26a0;\u0026#xfe0f; Is this last statement rigorous? Specifically, I am asking about stating that $\\mathcal{A} = \\mathbf{Q}$. The book seems to avoid saying both are directly equal, but it really looks like they are.\nExample 5.8 - Poisson process $$\\begin{align} (\\mathcal{A}f)(n) \u0026= \\lim_{t \\rightarrow 0+} \\frac{\\mathbb{E}^n[f(X_t)] - f(n)}{t} \\\\ \u0026= \\lim_{t \\rightarrow 0+} \\frac{1}{t} \\left( \\sum_{k=n}^\\infty \\frac{(\\lambda t)^{k-n}}{(k-n)!} e^{-\\lambda t} f(k) - f(n)\\right) \\\\ \u0026= \\lim_{t \\rightarrow 0+} \\frac{1}{t} \\left( f(n)e^{-\\lambda t} + f(n+1)\\lambda t + \\sum_{k=n+2}^\\infty \\frac{(\\lambda t)^{k-n}}{(k-n)!} e^{-\\lambda t} f(k) - f(n)\\right) \\\\ \u0026= \\lim_{t \\rightarrow 0+} \\frac{1}{t} \\left( f(n)(e^{-\\lambda t}-1) + f(n+1)\\lambda t e^{-\\lambda t} + \\sum_{k=n+2}^\\infty \\frac{(\\lambda t)^{k-n}}{(k-n)!} e^{-\\lambda t} f(k) \\right) \\\\ \u0026= \\lambda(f(n+1) - f(n)) \\end{align}$$ The last step is justified with L\u0026rsquo;Hopital\u0026rsquo;s rule.\n$$\\mathcal{A}^*f(n) = \\lambda(f(n-1) - f(n))$$ $$(g, \\mathcal{A}f) = (\\mathcal{A}^* g, f), \\quad \\forall \\; f\\in \\mathscr{B}, g \\in \\mathscr{B}^*$$$$(\\mathcal{A}^*g, f) = \\lambda(g,f^+) - \\lambda(g,f)$$ Then if we note that $(g,f^+) = (g^-,f)$ (this is the part I cannot justify), then it follows that $\\mathcal{A}^*f(n) = \\lambda(f(n-1) - f(n))$\n$$\\begin{align} \\frac{d u}{dt} \u0026= \\frac{d}{dt}\\left(\\sum_{k \\geq n} f(k) \\frac{(\\lambda t)^{k-n}}{(k-n)!}e^{-\\lambda t}\\right) \\\\ \u0026= \\frac{d}{dt}\\left( e^{-\\lambda t} f(n) + \\sum_{k \u003e n} f(k) \\frac{(\\lambda t)^{k-n}}{(k-n)!}e^{-\\lambda t} \\right) \\\\ \u0026= -\\lambda e^{-\\lambda t} f(n) + \\sum_{k \u003e n} f(k) \\left[ \\left(-\\lambda\\frac{(\\lambda t)^{k-n}}{(k-n)!}e^{-\\lambda t}\\right) + \\left(\\lambda\\frac{(\\lambda t)^{k-n-1}}{(k-n-1)!}e^{-\\lambda t}\\right) \\right] \\\\ \u0026= \\lambda(u(t,n+1)-u(t,n)) \\\\ \u0026= \\mathcal{A}u(t,n) \\end{align}$$$$\\begin{align} \\frac{d \\nu_n(t)}{dt} \u0026= \\frac{d}{dt}\\left(\\frac{(\\lambda t)^{k-n}}{(k-n)!}e^{-\\lambda t}\\right) \\\\ \u0026= -\\lambda\\frac{(\\lambda t)^{k-n}}{(k-n)!}e^{-\\lambda t} + \\lambda \\frac{(\\lambda t)^{k-n-1}}{(k-n-1)!}e^{-\\lambda t} \\\\ \u0026= \\lambda(\\nu_{n-1} - \\nu_n) \\\\ \u0026= (\\mathcal{A}^* \\mathbf{\\nu})_n \\end{align}$$\u0026#x26a0;\u0026#xfe0f; I keep getting $\\lambda(\\nu_{n+1} - \\nu_n)$, which disagrees with the book. Where did I go wrong?\nNotice how both Markov processes satisfied the forward Kolmogrov equation for the distribution, and the backwards for the expected values. This is a general property of Markov processes (wow!) that will be revisited.\n","permalink":"https://hasithv.github.io/posts/notes/eliasa/chap5/5-3/","summary":"\u003ch2 id=\"markov-processes-in-continuous-time-and-space\"\u003eMarkov processes in continuous time and space\u003c/h2\u003e\n\u003cp\u003eGiven a probability space $(\\Omega, \\mathcal{F}, \\mathbb{P})$ and the filtration $\\mathbb{F} = (\\mathcal{F}_t)_{t \\geq 0}$, a stochastic process $X_t$ is called a Markov process wrt $\\mathcal{F}_t$ if\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e$X_t$ is $\\mathcal{F}_t$-adapted\u003c/li\u003e\n\u003cli\u003eFor any $t \\geq s$ and $B \\in \\mathcal{R}$, we have\n$$\\mathbb{P}(X_t \\in B | \\mathcal{F}_s) = \\mathbb{P}(X_t \\in B | X_s)$$\nEssentially, this is saying that history doesn\u0026rsquo;t matter, only the current state matters. We can associate a family of probability measures $\\{\\mathbb{P}^x\\}_{x\\in\\mathbb{R}}$ for the processes starting at $x$ by defining $\\mu_0$ to be the point mass at $x$. Then, we still have\n$$\\mathbb{P}^x(X_t \\in B | \\mathcal{F}_s) = \\mathbb{P}^x(X_t \\in B | X_s), \\quad t \\geq s$$\nand $\\mathbb{E}[f(X_0)] = f(x)$ for any function $f \\in C(\\mathbb{R})$.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u0026#x26a0;\u0026#xfe0f; I am not fully confident on what the above section is saying. Specifically, I am having trouble with understanding how we are defining $\\mathbb{P}^x$. However, I can understand the strong markov property, so I think I should be okay moving forward.\u003c/p\u003e","title":"5.3 - Markov Processes"},{"content":"Filtration Definition 5.3: (Filtration). Given a probability space, the filtration is a nondecreaseing family of $\\sigma$-algebras $\\{\\mathcal{F}_t\\}_{t \\leq 0}$ such that $\\mathcal{F}_s \\subset \\mathcal{F}_t \\subset \\mathcal{F}$ for all $0 \\leq s \u003c t$.\nIntuitively, the filtration is a sigma algebra of events that can be determined before time $t$ (we can\u0026rsquo;t lose information by foing forward in time). A stochastic process is called $\\mathcal{F}_t$-adapted if it is measurable with respect to $\\mathcal{F}_t$; that is, for all $B \\in \\mathcal{R}$, $X_t^{-1}(B) \\in \\mathcal{F}_t$. We can always assume that the $\\mathcal{F}_t$ contains $F_t^{X}$ and all sets of measure zero, where $F_t^{X} = \\sigma(X_s, s \\leq t)$ is the sigma algebra generated by the process $X$ up to time $t$.\n$$\\mathcal{F}_0^X = \\{\\emptyset, \\Omega\\}$$$$\\mathcal{F}_1^X = \\{\\emptyset, \\Omega, \\{H\\}, \\{T\\}\\}$$$$\\mathcal{F}_2^X = \\sigma(\\{\\emptyset, \\{HH\\}, \\{TT\\}, \\{HT\\}, \\{TH\\} \\})$$ (I believe this last statement is equivalent to what the book has)\nStopping Time $$\\{T \\leq n\\} \\in \\mathcal{F}_n$$ For the discrete case, it doesn\u0026rsquo;t matter if we say $\\{T \\leq n\\}$ or $\\{T = n\\}$ simply becase it has to be satisfied for all $n$.\nProposition 5.5: (Properties of stopping times). For the Markov process $\\{X_n\\}_{n \\in \\mathbb{N}}$, we have\n(1) if $T_1, T_2$ are stopping times, then $T_1 \\wedge T_2, T_1 \\vee T_2, T_1 + T_2$ are stopping times (2) if $\\{T_k\\}_{k \\geq 1}$ are stopping times then $\\sup_k T_k, \\inf_k T_k, \\limsup_k T_k, \\liminf_k T_k$ are stopping times $$\\{T \\leq t\\} \\in \\mathcal{F}_t$$ Note that we cannot swap the inequality for an equals sign in the definition of a stopping time for continuous time processes. Furthermore, porposition 5.5 holds for conitnious time processes if the filtration is right continuous: $\\mathcal{F}_t = \\mathcal{F}_{t^+}= \\bigcap_{s\u003et} \\mathcal{F}_s$.\n","permalink":"https://hasithv.github.io/posts/notes/eliasa/chap5/5-2/","summary":"\u003ch2 id=\"filtration\"\u003eFiltration\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eDefinition 5.3:\u003c/strong\u003e (Filtration). Given a probability space, the filtration is a nondecreaseing family of $\\sigma$-algebras $\\{\\mathcal{F}_t\\}_{t \\leq 0}$ such that $\\mathcal{F}_s \\subset \\mathcal{F}_t \\subset \\mathcal{F}$ for all $0 \\leq s \u003c t$.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eIntuitively, the filtration is a sigma algebra of events that can be determined before time $t$ (we can\u0026rsquo;t lose information by foing forward in time). A stochastic process is called \u003cem\u003e$\\mathcal{F}_t$-adapted\u003c/em\u003e if it is measurable with respect to $\\mathcal{F}_t$; that is, for all $B \\in \\mathcal{R}$, $X_t^{-1}(B) \\in \\mathcal{F}_t$. We can always assume that the $\\mathcal{F}_t$ contains $F_t^{X}$ and all sets of measure zero, where $F_t^{X} = \\sigma(X_s, s \\leq t)$ is the sigma algebra generated by the process $X$ up to time $t$.\u003c/p\u003e","title":"5.2 - Filtration and Stopping Time"},{"content":"Definition of a stochastic process A stochastic process is a parameterized random variable $\\{X_t\\}_{t\\in\\mathbf{T}}$ defined on a probability space $(\\Omega, \\mathcal{F}, \\mathbb{P})$ taking on values in $\\mathbb{R}$. $\\mathbf{T}$ can seemingly be any subset of $\\mathbb{R}$. For any fixed $t \\in \\mathbf{T}$, we can define the random variable\n$$X_t: \\Omega \\rightarrow \\mathbb{R}, \\quad \\omega \\rightarrowtail X_t(\\omega)$$Thinking of a simple random walk, this means that $X_t$ is a random variable that takes in some subset of $\\Omega = \\{H,T\\}^\\mathbb{N}$ and outputs a real valued number (the sum of the first $t$ values in $\\omega$): $\\{\\omega_1, \\omega_2, \\ldots \\} \\rightarrow \\sum_{n \\leq t} X(\\omega_n)$\nOn the other side of the coin, for a fixed $\\omega \\in \\Omega$, we can define a real-valued measureable function on $\\mathbf{T}$ called the trajectory of $X$\n$$X_.(\\omega): \\mathbf{T} \\rightarrow \\mathbb{R}, \\quad t \\rightarrowtail X_t(\\omega)$$Again, back to the random walk, this means that we can get a real valued output for any given $t$. To be even more compact, we can say taht a stochastic process is a measureable function from $\\Omega \\times \\mathbf{T}$ to $\\mathbb{R}$\n$$(\\omega, t) \\rightarrowtail X(\\omega, t) := X_t(\\omega)$$The largest probability space that one can take is the infinite product space $\\Omega = \\mathbb{R}^\\mathbf{T}$. Essentially, this is a space which can takeon any real value at any moment in time (\u0026#x26a0;\u0026#xfe0f; why are we restricting ourselves to $\\mathbb{R}$? Why can\u0026rsquo;t it be a vector valued function?)\n$$\\mu_{1,\\ldots,t_k}(F_1 \\times \\ldots \\times F_k) = \\mathbb{P[X_{t_1}\\in F_1, \\ldots X_{t_k} \\in F_k]}$$ $$\\mu_{1,\\ldots,t_k}(F_1 \\times \\ldots \\times F_k) = \\mathbb{P[X_{t_1}\\in F_1, \\ldots X_{t_k} \\in F_k]}$$ Kolmogorov\u0026rsquo;s extension theorem is very general. In fact, so general that it does not give us a very good idea of what the process actually looks like. Usually, we start with this extremely general definition and then impose stricter conditions to prove that the measure can be defined on a smaller probability space rather than $\\Omega$\n","permalink":"https://hasithv.github.io/posts/notes/eliasa/chap5/5-1/","summary":"\u003ch2 id=\"definition-of-a-stochastic-process\"\u003eDefinition of a stochastic process\u003c/h2\u003e\n\u003cp\u003eA stochastic process is a parameterized random variable $\\{X_t\\}_{t\\in\\mathbf{T}}$ defined on a probability space $(\\Omega, \\mathcal{F}, \\mathbb{P})$ taking on values in $\\mathbb{R}$. $\\mathbf{T}$ can seemingly be any subset of $\\mathbb{R}$. For any fixed $t \\in \\mathbf{T}$, we can define the random variable\u003c/p\u003e\n$$X_t: \\Omega \\rightarrow \\mathbb{R}, \\quad \\omega \\rightarrowtail X_t(\\omega)$$\u003cp\u003eThinking of a simple random walk, this means that $X_t$ is a random variable that takes in some subset of $\\Omega = \\{H,T\\}^\\mathbb{N}$ and outputs a real valued number (the sum of the first $t$ values in $\\omega$): $\\{\\omega_1, \\omega_2, \\ldots \\} \\rightarrow \\sum_{n \\leq t} X(\\omega_n)$\u003c/p\u003e","title":"5.1 - Axiomatic Construction of Stochastic Process"},{"content":"Here are my notes for E, Li, and Vanden-Eijnden\u0026rsquo;s Applied Stochastic Analysis\nChapter 5 - Stochastic Processes 5.1 - Axiomatic Construction of Stochastic Process 5.2 - Filtration and Stopping Time 5.3 - Markov Processes 5.4 - Gaussian Processes Chapter 6 - Wiener Process 6.1 - The Diffusion Limit of Random Walks 6.2 - The Invariance Principle ","permalink":"https://hasithv.github.io/posts/notes/eliasa/eliasa/","summary":"\u003cp\u003eHere are my notes for E, Li, and Vanden-Eijnden\u0026rsquo;s \u003ca href=\"https://bookstore.ams.org/gsm-199/\"\u003e\u003cem\u003eApplied Stochastic Analysis\u003c/em\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eChapter 5 - Stochastic Processes\n\u003cul\u003e\n\u003cli\u003e5.1 - \u003ca href=\"/posts/notes/eliasa/chap5/5-1/\"\u003eAxiomatic Construction of Stochastic Process\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e5.2 - \u003ca href=\"/posts/notes/eliasa/chap5/5-2/\"\u003eFiltration and Stopping Time\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e5.3 - \u003ca href=\"/posts/notes/eliasa/chap5/5-3/\"\u003eMarkov Processes\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e5.4 - \u003ca href=\"/posts/notes/eliasa/chap5/5-4/\"\u003eGaussian Processes\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eChapter 6 - Wiener Process\n\u003cul\u003e\n\u003cli\u003e6.1 - \u003ca href=\"/posts/notes/eliasa/chap6/6-1/\"\u003eThe Diffusion Limit of Random Walks\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e6.2 - \u003ca href=\"/posts/notes/eliasa/chap6/6-2/\"\u003eThe Invariance Principle\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e","title":"Applied Stochastic Analysis"},{"content":"2.2.1 - Definition and first examples Definition 2.2: A symmetric monoidal structure on a preoirder $(X, \\leq)$ consists of\n(i) a monoidal unit, $I \\in X$ (ii) a monoidal product $\\otimes: X \\times X \\rightarrow X$ And the monoidal product $\\otimes(x_1,x_2) = x_1 \\otimes x_2$ must also satisfy the following properties (assume all elements are in $X$)\n(a) $x_1 \\leq y_1$ and $x_2 \\leq y_2 \\implies x_1 \\otimes x_2 \\leq y_1 \\otimes y_2$ (b) $I \\otimes x = x \\otimes I = x$ (c) associativity (d) commutivity/symmetry (a) is called monotnoicity and (b) is unitality\nRemark 2.3: replacing $=$ with $\\cong$ in definition 2.2 will give us a weak monoidal structure.\nExercise 2.5: The preorder structure $(\\mathbb{R}, \\leq)$ and the multiplication operation $\\times$ will not give us a symmetric monoidal order because of the simple counter example of $-2 \\times -2 \\nleq 1 \\times 1$.\nExample 2.6: A monid is similar to a symmetric monoidal preorder in that it consists of a set $M$, a function $*: M\\times M \\rightarrow M$, and an elment $e \\in M$ called the monid unit, such that for every $m,n,p \\in M$,\n$m * e = m$ $e * m = m$ associativity holds Further, if commutivity holds (which isn\u0026rsquo;t not generally true), then it is also called commutative\n2.2.2 - Introducing wiring diagrams \u0026#x26a0;\u0026#xfe0f; I am seeing the wiring diagrams, but I fail to understand why they are any different from the Hasse diagrams we\u0026rsquo;ve seen previously.\nEssentially, wiring diagrams seem to be a way to encode information about symmetric monoidal structures. The basic rules built up so far are as follows:\nA wire without a label, or with the label of the monoidal unit, is equivalent to nothing Otherwise, a wire labeled with an element represents that element (\u0026#x26a0;\u0026#xfe0f; this could be wrong) Two parallel wires represent the monoidal product of those elements Placing a $\\leq$ block between two $x,y$ wires indicates that $x \\leq y$ Thinking back to the conditions for a symmetric monoidal structure, we find that\nTransitivity allows us to combine wiring diagrams left to right Monotonicity is represented as being able to combine wiring diagrams top to bottom A monoidal product with a monoidal unit and another element gives us the element again, because the monoidal unit is equivalent to nothing (reflexivity)\nAssociativity means we can \u0026ldquo;wiggle\u0026rdquo; around parallel wires Commutivity means we can cross wires It is intuitive to see how these wiring diagrams can be used to prove statements. In fact, the above images are trivial proofs. Take a look at the following exercise\nExercise 2.20: Prove\u0026ndash;given $t \\leq v+w$, $w+u \\leq x+z$, and $v+x \\leq y$\u0026ndash;that $t+u \\leq y+z$.\n$$\\begin{align} t + u \u0026\\leq (v+w) + u \\\\ \u0026\\leq v + (w+u) \\\\ \u0026\\leq v + (x+z) \\\\ \u0026\\leq (v + x) + z \\\\ \u0026\\leq y + z \\\\ \\end{align}$$ and the wiring diagram would look like The squares are the $\\leq$ blocks\n2.2.3 - Applied examples While this section did solidify some concepts. It wasn\u0026rsquo;t too important. Although, it did carry two useful examples: discarding and splitting.\nWith discarding, if a symmetric monoidal structure also satisfies $x \\leq I$ for every $x \\in X$, then it is possible to terminate any wire: And if instead have a property like $x \\leq x + x$, then we can split any wire: 2.2.4 - Abstract examples Again, after a skim through, this section did not seem critical.\n2.2.5 - Monoidal montone maps We begin with recalling that for any preorder $(X,\\leq)$ we have an induced equivalence relation $\\cong$ on $X$ where two elements $x \\cong x' \\iff x \\leq x$ and $x' \\leq x$\nDefinition 2.41: $\\mathcal{P} = (P, \\leq_P, I_P, \\otimes_P)$ and $\\mathcal{Q} = (Q, \\leq_Q, I_Q, \\otimes_Q)$ be monoidal preorders. A monoidal monotone from $\\mathcal{P}$ to $\\mathcal{Q}$ is a monotone map $f: (P, \\leq_P) \\rightarrow (Q, \\leq_Q)$ which satisfies\n(a) $I_Q \\leq_Q f(I_P)$ (b) $f(p_1) \\otimes_Q f(p_1 \\otimes_P p_2)$ for all $p_1,p_2 \\in P$. Additionally, $f$ is a strong monoidal monotone if it satisfies\n(a\u0026rsquo;) $I_Q \\cong f(I_P)$ (b\u0026rsquo;) $f(p_1) \\otimes_Q f(p_1 \\otimes_P p_2) \\cong f(p_1) \\otimes_Q f(p_2)$ And it is called a strict monoidal monotone if it satisfies (a\u0026rsquo;\u0026rsquo;) $I_Q = f(I_P)$ (b\u0026rsquo;\u0026rsquo;) $f(p_1) \\otimes_Q f(p_1 \\otimes_P p_2) = f(p_1) \\otimes_Q f(p_2)$ Monoidal monotones are said to be examples of monoidal functors in category theory.\nThe exercises for this section seem a little easy, so I will be skipping them for now, returning to them if I get confused on the defnitions of monoidal monotones.\n","permalink":"https://hasithv.github.io/posts/notes/fongspivakact/chap2/2-2/","summary":"\u003ch2 id=\"221---definition-and-first-examples\"\u003e2.2.1 - Definition and first examples\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eDefinition 2.2:\u003c/strong\u003e A \u003cem\u003esymmetric monoidal structure\u003c/em\u003e on a preoirder $(X, \\leq)$ consists of\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e(i) a \u003cem\u003emonoidal unit\u003c/em\u003e, $I \\in X$\u003c/li\u003e\n\u003cli\u003e(ii) a \u003cem\u003emonoidal product\u003c/em\u003e $\\otimes: X \\times X \\rightarrow X$\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAnd the monoidal product $\\otimes(x_1,x_2) = x_1 \\otimes x_2$ must also satisfy the following properties (assume all elements are in $X$)\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e(a) $x_1 \\leq y_1$ and $x_2 \\leq y_2 \\implies x_1 \\otimes x_2 \\leq y_1 \\otimes y_2$\u003c/li\u003e\n\u003cli\u003e(b) $I \\otimes x = x \\otimes I = x$\u003c/li\u003e\n\u003cli\u003e(c) associativity\u003c/li\u003e\n\u003cli\u003e(d) commutivity/symmetry\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e(a) is called \u003cem\u003emonotnoicity\u003c/em\u003e and (b) is \u003cem\u003eunitality\u003c/em\u003e\u003c/p\u003e","title":"2.2 - Symmetric monoidal preorders"},{"content":"This is a collection of my notes for Brendan Fong and David Spivak\u0026rsquo;s An Invitation to Appied Category Theory. The first chapter was done through LaTeX, but the rest should be markdown with mathjax.\nChapter 1 - Generative effects: Orders and adjunctions Chapter 2 - Resource theories: Monoidal preorders and enrichment Section 2.2 - Symmetric monoidal preorders ","permalink":"https://hasithv.github.io/posts/notes/fongspivakact/fongspivakact/","summary":"\u003cp\u003eThis is a collection of my notes for Brendan Fong and David Spivak\u0026rsquo;s \u003ca href=\"https://arxiv.org/pdf/1803.05316\"\u003e\u003cem\u003eAn Invitation to Appied Category Theory\u003c/em\u003e\u003c/a\u003e. The first chapter was done through LaTeX, but the rest should be markdown with mathjax.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/1.pdf\"\u003eChapter 1 - Generative effects: Orders and adjunctions\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eChapter 2 - Resource theories: Monoidal preorders and enrichment\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/posts/notes/fongspivakact/chap2/2-2/\"\u003eSection 2.2 - Symmetric monoidal preorders\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e","title":"An Invitation to Appied Category Theory"},{"content":"Coming soon! ","permalink":"https://hasithv.github.io/about/","summary":"\u003ch1 id=\"coming-soon\"\u003eComing soon!\u003c/h1\u003e","title":"About"},{"content":"$$\\mathbb{P}(X) = n p^n \\; \\quad \\forall \\: n\\in\\mathbb{N}, p \\in [0,1].$$But this is obviously wrong! Plug in $p=.9, n=2$, and you get that $\\mathbb{P}(X) = 1.62$. Thaat\u0026rsquo;s not how probability works! However, for $p=0.5$, $\\mathbb{P}(X)$ will remain $\\leq 1$ for all $n \\in \\mathbb{N}$. So, somewhere in the interval $(0.5,0.9)$, we reach a critical value where any $p$ greater than that will result in a probability greater than one, and any value less than it will be a bit more reasonable.\nSo, what is this critical value that will help me save face?\nWell, the question we are trying to answer, phrased a bit more formally, is:\nfind the largest $p \\in [0,1]$ such that $np^n \\leq 1$ for all $n \\in \\mathbb{N}.$\n$$np^n \\leq 1 \\iff p^n \\leq \\frac{1}{n}.$$Visually, this means that the exponential graph of $f_p(n) = p^n$ can never go above $g_p(n) = \\frac{1}{n}$ for some fixed $p$. From this, we can deduce that the critical value of $p$, which we will denote as $p_0$, will satisfy the following relation:\n$$F_{p}(t) = \\begin{bmatrix} t \\\\ f_p(t) \\end{bmatrix}, \\; G_p(t) = \\begin{bmatrix} t \\\\ g_p(t) \\end{bmatrix},$$$$F_{p_0}(t_0) = G_{p_0}(t_0),\\text{ and } \\dot{F}_{p_0}(t_0) = \\lambda \\dot{G}_{p_0}(t_0)$$ for some $\\lambda \\in \\mathbb{R}, t_0 \\in \\mathbb{R}^+$. In other words, their velocities will point in the same direction (and, perhaps more intuitively, the outward normals of each curve will be parallel, so the graphs \u0026lsquo;kiss\u0026rsquo; at some $t_0$ with the choice of $p_0$).\n$$ \\begin{aligned} f_{p_0}(t_0) \u0026= g_{p_0}(t_0) \\\\ \\implies p_0^n \u0026= \\frac{1}{t_0} \\end{aligned} $$$$ \\begin{aligned} \\dot{f}_{p_0}(t_0) \u0026= \\dot{g}_{p_0}(t_0) \\\\ \\implies -\\ln(p_0)p_0^n \u0026= \\frac{1}{t_0^2} \\end{aligned} $$$$ \\begin{cases} p_0^n = \\frac{1}{t_0}, \\\\ \\ln(p_0)p_0^n = -\\left(\\frac{1}{t_0}\\right)^2 \\end{cases} $$ which I cannot solve, but Desmos tells me that $p_0 \\approx 0.6922$ and $t_0 \\approx 2.7181$.\nThus, my answer would have been reasonable in some convoluted scenario in which $p \u003c 0.6922$.\n(This answer, too, is not totally right! This is because there may be a larger $p$ value that satisfies $np^n \\leq 1$ for $n \\in \\mathbb{N}$ but not for $n\\in \\mathbb{R}^+$. We solved for the $n \\in \\mathbb{R}^+$ case, which would technically give us a lower bound for $p_0$. Taking this into consideration, our $p_0$ value would really be $p_0 \\approx 0.6934$)\n","permalink":"https://hasithv.github.io/posts/24-07-29-nothowprobabilityworks/","summary":"$$\\mathbb{P}(X) = n p^n \\; \\quad \\forall \\: n\\in\\mathbb{N}, p \\in [0,1].$$\u003cp\u003eBut this is obviously wrong! Plug in $p=.9, n=2$, and you get that $\\mathbb{P}(X) = 1.62$. Thaat\u0026rsquo;s not how probability works! However, for $p=0.5$, $\\mathbb{P}(X)$ will remain $\\leq 1$ for all $n \\in \\mathbb{N}$. So, somewhere in the interval $(0.5,0.9)$, we reach a critical value where any $p$ greater than that will result in a probability greater than one, and any value less than it will be a bit more reasonable.\u003c/p\u003e","title":"That's not how Probability Works!"},{"content":"I have no idea what I am doing. Anyways, here\u0026rsquo;s a cool equation:\n$$\\frac{d}{dt}\\left(\\frac{\\partial L}{\\partial \\dot{q}}\\right) - \\frac{\\partial L}{\\partial q} = 0$$","permalink":"https://hasithv.github.io/posts/introduction/","summary":"\u003cp\u003eI have no idea what I am doing. Anyways, here\u0026rsquo;s a cool equation:\u003c/p\u003e\n$$\\frac{d}{dt}\\left(\\frac{\\partial L}{\\partial \\dot{q}}\\right) - \\frac{\\partial L}{\\partial q} = 0$$","title":"Introduction"}]