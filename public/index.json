[{"content":" Definition 5.9: A stochasitc process $\\{X_t\\}_{t \\geq 0}$ is a Gaussian Process if its finite dimensional distributions are consistent Gaussian measures for any $0 \\leq t_1 \u003c t_2 \u003c \\ldots \u003c t_k$.\nRecall that a Gaussian random vector $\\mathbf{X} = (X_1, X_2,\\ldots,X_n)^T$ is completely characterized by its first and second moments $$\\mathbf{m} = \\mathbb{E}[\\mathbf{X}], \\quad \\mathbf{K} = \\mathbb{E}[(\\mathbf{X} - \\mathbf{m}) (\\mathbf{X} - \\mathbf{m})^T]$$ Meaning that the characteristic function is expressed only in terms of $\\mathbf{m}$ and $\\mathbf{K}$ $$\\mathbb{E}\\left[e^{i \\mathbf{\\xi} \\cdot \\mathbf{X}}\\right] = e^{i \\mathbf{\\xi} \\cdot \\mathbf{m} - \\frac{1}{2}\\mathbf{\\xi}^T \\mathbf{K} \\mathbf{\\xi}} $$ This means that for any $0 \\leq t_1 \u003c t_2 \u003c \\ldots \u003c t_k$, the measure $\\mu_{t_1, t_2, \\ldots, t_k}$ is uniquely determined by an $\\mathbf{m} = (m(t_1), \\ldots, m(t_k))$ and a covariance matrix $\\mathbf{K}_{ij} = K(t_i, t_j)$. Because our $\\mu$ satisifes the conditions for Kolomorov\u0026rsquo;s extension theorem, we have a probability space and a stochastic process associated with $\\mu$.\n\u0026#x26a0;\u0026#xfe0f; Isn\u0026rsquo;t one of the conditions of Kolmogorov\u0026rsquo;s extension theoren that we need to be able to permute the $t_i$? How would this work if we require the $t_i$ to be increasing?\nTheorem 5.10: Assuming that the stochastic process $\\{X_t\\}_{t\\in[0,T]}$ satisfies $$\\mathbb{E} \\left[ \\int_0^T X_t^2 dt \\right] \u003c \\infty$$ then $m \\in L^2_t$. Also, the operator $$\\mathcal{K} f(s) := \\int_0^T K(s,t) f(t) dt$$ is nonnegative, compact on $L^2_t$\nProof: For the first statement, $$\\int_0^T \\mathbb{E}[X]^2 dt \\leq \\int_0^T \\mathbb{E}[X_t^2] dt \u003c \\infty$$ For the second, $$\\begin{align} \\int_0^T \\int_0^T K^2(s,t) ds dt \u0026= \\int_0^T \\int_0^T \\mathbb{E}[\\left((X_t - m(t))(X_s - m(s))\\right)^2]ds dt \\\\ \u0026\\leq \\int_0^T \\int_0^T \\mathbb{E}[(X_t - m(t))^2]\\mathbb{E}[(X_s - m(s))^2]ds dt \\\\ \u0026\\leq \\left( \\int_0^T \\mathbb{E}[X_t] dt \\right) \\\\ \u0026\\leq \\infty \\end{align}$$ which lets us conclude that $K \\in L^2([0,T] \\times [0,T])$, which tells us that $\\mathcal{K}$ is compact on $L^2_t$\n\u0026#x26a0;\u0026#xfe0f; I can\u0026rsquo;t properly find what theorem lets us say the last statement, but I can trust it for now.\nFurthermore, noting that $K$ is symmetric which means that $\\mathcal{K}$ is self adjoint, and we can say $$(\\mathcal{K}f, f) = \\int_0^T \\int_0^T \\mathbb{E}[(X_t - m(t))]\\mathbb{E}[(X_s - m(s))]f(t)f(s) ds dt \\geq 0$$ by symmetry of $s$ and $t$.\nIf we want to extend the characteristic to $L_t$ rather than the finite dimensional version, we can write $$\\mathbb{E}[e^{i(\\xi,X)}] = e^{i(\\xi,m) - \\frac{1}{2} (\\xi, \\mathcal{K} \\xi)}, \\quad \\xi \\in L^2_t$$ With $(\\xi,m) = \\int_a^b \\xi(t) m(t) dt$ and $\\mathcal{K}\\xi (t) = \\int_a^b K(t,s) \\xi(s) ds$. This is a fairly reasonable extrapolation from the finite dimensional case.\nTheorem 5.13: Karhunen-Loeve expansion. Let $(X_t)_{t \\in [0,1]}$ be a Gaussian process with mean 0 and covariance function $K(s,t)$, Assume that $K$ continuous and $\\{\\lambda_k\\}$ be the set of eigenvalues for orthonormal eigenfunctions of $K$, $\\{\\phi_k\\}$. Then, $X_t$ has the representation of $$X_t = \\sum_{k=1}^\\infty \\alpha_k \\sqrt{\\lambda_k} \\phi_k(t)$$ Where $\\alpha_k$ is a standard normal random variable $\\mathscr{N}(0,1)$\n\u0026#x26a0;\u0026#xfe0f; I am omitting the proof because I feel the result is easy enough to intuitively grasp, and also it is a little theoretical, so maybe I should revisit it if I get more comfortable with proving covergence in probability.\n","permalink":"http://localhost:1313/posts/notes/eliasa/chap5/5-4/","summary":"Definition 5.9: A stochasitc process $\\{X_t\\}_{t \\geq 0}$ is a Gaussian Process if its finite dimensional distributions are consistent Gaussian measures for any $0 \\leq t_1 \u003c t_2 \u003c \\ldots \u003c t_k$.\nRecall that a Gaussian random vector $\\mathbf{X} = (X_1, X_2,\\ldots,X_n)^T$ is completely characterized by its first and second moments $$\\mathbf{m} = \\mathbb{E}[\\mathbf{X}], \\quad \\mathbf{K} = \\mathbb{E}[(\\mathbf{X} - \\mathbf{m}) (\\mathbf{X} - \\mathbf{m})^T]$$ Meaning that the characteristic function is expressed only in terms of $\\mathbf{m}$ and $\\mathbf{K}$ $$\\mathbb{E}\\left[e^{i \\mathbf{\\xi} \\cdot \\mathbf{X}}\\right] = e^{i \\mathbf{\\xi} \\cdot \\mathbf{m} - \\frac{1}{2}\\mathbf{\\xi}^T \\mathbf{K} \\mathbf{\\xi}} $$ This means that for any $0 \\leq t_1 \u003c t_2 \u003c \\ldots \u003c t_k$, the measure $\\mu_{t_1, t_2, \\ldots, t_k}$ is uniquely determined by an $\\mathbf{m} = (m(t_1), \\ldots, m(t_k))$ and a covariance matrix $\\mathbf{K}_{ij} = K(t_i, t_j)$.","title":"5.4 - Gaussian Processes"},{"content":"Markov processes in continuous time and space Given a probability space $(\\Omega, \\mathcal{F}, \\mathbb{P})$ and the filtration $\\mathbb{F} = (\\mathcal{F}_t)_{t \\geq 0}$, a stochastic process $X_t$ is called a Markov process wrt $\\mathcal{F}_t$ if\n$X_t$ is $\\mathcal{F}_t$-adapted For any $t \\geq s$ and $B \\in \\mathcal{R}$, we have $$\\mathbb{P}(X_t \\in B | \\mathcal{F}_s) = \\mathbb{P}(X_t \\in B | X_s)$$ Essentially, this is saying that history doesn\u0026rsquo;t matter, only the current state matters. We can associate a family of probability measures $\\{\\mathbb{P}^x\\}_{x\\in\\mathbb{R}}$ for the processes starting at $x$ by defining $\\mu_0$ to be the point mass at $x$. Then, we still have $$\\mathbb{P}^x(X_t \\in B | \\mathcal{F}_s) = \\mathbb{P}^x(X_t \\in B | X_s), \\quad t \\geq s$$ and $\\mathbb{E}[f(X_0)] = f(x)$ for any function $f \\in C(\\mathbb{R})$. \u0026#x26a0;\u0026#xfe0f; I am not fully confident on what the above section is saying. Specifically, I am having trouble with understanding how we are defining $\\mathbb{P}^x$. However, I can understand the strong markov property, so I think I should be okay moving forward.\nThe transition function of a Markov process is defined as $$p(B,t|x,s) = \\mathbb{P}(X_t \\in B | X_s = x)$$ and it has the properties\n$p(.,t|x,s)$ is a probability measure on $\\mathcal{R}$ $p(B,t|.,s)$ is a measurable function on $\\mathbb{R}$ $p$ satisfies $$p(B,t|y,s) = \\int_\\mathbb{R} p(B,t|y,u) p(dy,u|x,s), \\quad s \\leq u \\leq t$$ The last property is the continuous analog of the Chapman-Kolmogorov equation, and it essentially lets us break the transition function into two the transition from $s$ to $u$ and from $u$ to $t$. Now, we can write the expenctation from $x$ as $$ \\begin{multline} \\mathbb{E}^x[f(X_{t_1}, X_{t_2}, \\ldots, X_{t_n})] = \\int_\\mathbb{R} \\ldots \\int_\\mathbb{R} f(x_1,x_2,\\ldots,x_n) p(dx_n,t_n|x_{n-1},t_{n-1}) \\\\ \\ldots p(dx_2, t_2 | x_1,t_1) p(dx_1, t_1|x,0) \\end{multline} $$ when $t_n$ are strictly increasing.\n$p(y,t|x,s)$ is a transition density function of $X$. \u0026#x26a0;\u0026#xfe0f; The book makes it seem like this is not always the case, but I fail to see when it isn\u0026rsquo;t.\nA stochastic process is stationary if the joint distributions are translation invariant in time. However, if the process only depends on the difference between time, then the process is homogeneous. The difference is that a stationary process has the same distribution at all times, while a homogeneous process has the same distribution for all time differences.\n\u0026#x26a0;\u0026#xfe0f; at this point, the book dives into semigroup theory which I know nothing about, so I will skip this section for now.\nExample 5.7 - Q-process Recall the definition of a generator $\\mathbf{Q}$ to be $$\\mathbf{Q} = \\lim_{h \\rightarrow 0+} \\frac{1}{h} (\\mathbf(P)(h) - \\mathbf{I})$$ Now, we will define an infinitesimal generator $\\mathcal{A}$ on a sample space $S = \\{1,2,\\ldots,I\\}$: $$\\mathcal{A}f = \\lim_{t \\rightarrow 0+} \\frac{\\mathbb{E}[f(X_t)] - f}{t}$$ and $$\\begin{align} \\mathcal{A}f(i) \u0026= \\lim_{t \\rightarrow 0+} \\frac{\\mathbb{E}^i[f(X_t)] - f(i)}{t} \\\\ \u0026= \\lim_{t \\rightarrow 0+} \\frac{1}{t} \\left(\\sum_{j \\in S} (P_{ij} - \\delta_{ij})f(j)\\right)\\\\ \u0026= \\sum_{j \\in S} q_{ij} f(j), \\quad i \\in S \\end{align}$$ Thus, the generator $\\mathbf{Q}$ is exactly the infinitesimal generator of $X_t$. This is important to digest especially because the $\\mathcal{A}$ is new to me.\nExtending the idea above, we get the backward Kolmogorov equation for $\\mathbf{u} = (u_1, u_2, \\ldots, u_I)^T$ and $u_i(t) = \\mathbb{E}^i[f(X_t)]$: $$\\frac{d \\mathbf{u}}{dt} = \\mathbf{Qu} = \\mathcal{A}\\mathbf{u}$$ \u0026#x26a0;\u0026#xfe0f; This, too, is getting a little confusing. Let\u0026rsquo;s delve into it a bit more.\nWe are essentially dealing with a continous time markov chaic (CTMC) in the above case, because we have a finite number of states that have some associated probability of moving to another state at an infitesimal time step.\nWikipedia says that for CTMC\u0026rsquo;s, the Komogorov backward equations are, rather intuitively, that the time derviative of the probaility of transitioning from state $i$ at time $s$ to state $j$ at time $t$. $$\\frac{\\partial P_{ij}}{\\partial t}(s;t) = \\sum_k P_{kj}(s;t)A_{ik}(s)$$ Where $A$ is the transition-rate matrix in which an element $q_{ij}$ denotes the rate departing from $i$ and arriving in state $j$. Knowing this, I can understand why $\\mathcal{A}$ is the generator $\\mathbf{Q}$. Converting things back into our notation, we have $$ \\frac{d P_{ij}}{dt} = \\sum_{k \\in I} \\mathcal{A}_{ik} P_{kj} $$ In that case, we look back at the expression for $\\mathbb{E}^i[f(X_t)]$ $$\\mathbb{E}^i[f(X_t)] = \\sum_j P_{ij}f(j)$$ So, $$ \\begin{align} \\implies \\frac{d}{dt} \\mathbb{E}^i[f(X_t)] \u0026= \\sum_j \\frac{d P_{ij}}{d t} f(j) \\\\ \u0026= \\sum_j \\left(\\left[ \\sum_k \\mathcal{A_{ik}} P_{kj} \\right] f(j) \\right) \\\\ \u0026= \\sum_k \\sum_j \\mathcal{A}_{ik} P_{kj} f(j) \\\\ \u0026= \\sum_k \\mathcal{A}_{ik} \\mathbb{E}^k[f(X_t)] \\end{align} $$ Now, it\u0026rsquo;s clear that $$\\frac{d}{dt} \\mathbf{u} = \\mathcal{A} \\mathbf{u}$$ The backward kolmogrov equation is heavily linked to diffusion, so I will definitely explore that in the future.\nOn the other hand, if we have a distribution $\\mathbf{\\nu} = (\\nu_1, \\nu_2, \\ldots, \\nu_I)$ (which, by convention, is a row vector), then it satisfies the forward Kolomogrov equation $$\\frac{d\\mathbf{\\nu}}{dt} = \\mathbf{\\nu} \\mathcal{A}$$ Or using the adjoint $$\\frac{d\\mathbf{\\nu}^T}{dt} = \\mathcal{A}^* \\mathbf{\\nu}^T$$ where $\\mathcal{A}^*$ is defined as $$(A^* g, f) = (g, \\mathcal{A} f) \\quad \\forall \\; f \\in \\mathscr{B}, g \\in \\mathscr{B}$$ If this is giving you trouble, refer to equation (3.19) in the book and think with bra-ket notation. Recall that if $\\mathscr{B} = L^2$, then the dual space is also $L^2$, and so $\\mathcal{A}^* = \\mathcal{A}^T$.\n\u0026#x26a0;\u0026#xfe0f; Is this last statement rigorous? Specifically, I am asking about stating that $\\mathcal{A} = \\mathbf{Q}$. The book seems to avoid saying both are directly equal, but it really looks like they are.\nExample 5.8 - Poisson process Consider the Poisson process $X_t$ on $\\mathbb{N}$ with rate $\\lambda$. Then, $$\\begin{align} (\\mathcal{A}f)(n) \u0026= \\lim_{t \\rightarrow 0+} \\frac{\\mathbb{E}^n[f(X_t)] - f(n)}{t} \\\\ \u0026= \\lim_{t \\rightarrow 0+} \\frac{1}{t} \\left( \\sum_{k=n}^\\infty \\frac{(\\lambda t)^{k-n}}{(k-n)!} e^{-\\lambda t} f(k) - f(n)\\right) \\\\ \u0026= \\lim_{t \\rightarrow 0+} \\frac{1}{t} \\left( f(n)e^{-\\lambda t} + f(n+1)\\lambda t + \\sum_{k=n+2}^\\infty \\frac{(\\lambda t)^{k-n}}{(k-n)!} e^{-\\lambda t} f(k) - f(n)\\right) \\\\ \u0026= \\lim_{t \\rightarrow 0+} \\frac{1}{t} \\left( f(n)(e^{-\\lambda t}-1) + f(n+1)\\lambda t e^{-\\lambda t} + \\sum_{k=n+2}^\\infty \\frac{(\\lambda t)^{k-n}}{(k-n)!} e^{-\\lambda t} f(k) \\right) \\\\ \u0026= \\lambda(f(n+1) - f(n)) \\end{align}$$ The last step is justified with L\u0026rsquo;Hopital\u0026rsquo;s rule.\nThen, the book says $$\\mathcal{A}^*f(n) = \\lambda(f(n-1) - f(n))$$ \u0026#x26a0;\u0026#xfe0f; Here is the best reasoning I can come up with: $$(g, \\mathcal{A}f) = (\\mathcal{A}^* g, f), \\quad \\forall \\; f\\in \\mathscr{B}, g \\in \\mathscr{B}^*$$ And defining $f^\\pm(n) := f(n \\pm 1)$, then we require $$(\\mathcal{A}^*g, f) = \\lambda(g,f^+) - \\lambda(g,f)$$ Then if we note that $(g,f^+) = (g^-,f)$ (this is the part I cannot justify), then it follows that $\\mathcal{A}^*f(n) = \\lambda(f(n-1) - f(n))$\nAgain, lets compute the time derivative of $u(t,n) = \\mathbb{E}^n[f(X_t)]$ $$\\begin{align} \\frac{d u}{dt} \u0026= \\frac{d}{dt}\\left(\\sum_{k \\geq n} f(k) \\frac{(\\lambda t)^{k-n}}{(k-n)!}e^{-\\lambda t}\\right) \\\\ \u0026= \\frac{d}{dt}\\left( e^{-\\lambda t} f(n) + \\sum_{k \u003e n} f(k) \\frac{(\\lambda t)^{k-n}}{(k-n)!}e^{-\\lambda t} \\right) \\\\ \u0026= -\\lambda e^{-\\lambda t} f(n) + \\sum_{k \u003e n} f(k) \\left[ \\left(-\\lambda\\frac{(\\lambda t)^{k-n}}{(k-n)!}e^{-\\lambda t}\\right) + \\left(\\lambda\\frac{(\\lambda t)^{k-n-1}}{(k-n-1)!}e^{-\\lambda t}\\right) \\right] \\\\ \u0026= \\lambda(u(t,n+1)-u(t,n)) \\\\ \u0026= \\mathcal{A}u(t,n) \\end{align}$$ And the time derivative of the distribution $\\mathbf{\\nu} = (\\nu_0, \\nu_1, \\ldots)$ will be $$\\begin{align} \\frac{d \\nu_n(t)}{dt} \u0026= \\frac{d}{dt}\\left(\\frac{(\\lambda t)^{k-n}}{(k-n)!}e^{-\\lambda t}\\right) \\\\ \u0026= -\\lambda\\frac{(\\lambda t)^{k-n}}{(k-n)!}e^{-\\lambda t} + \\lambda \\frac{(\\lambda t)^{k-n-1}}{(k-n-1)!}e^{-\\lambda t} \\\\ \u0026= \\lambda(\\nu_{n-1} - \\nu_n) \\\\ \u0026= (\\mathcal{A}^* \\mathbf{\\nu})_n \\end{align}$$ \u0026#x26a0;\u0026#xfe0f; I keep getting $\\lambda(\\nu_{n+1} - \\nu_n)$, which disagrees with the book. Where did I go wrong?\nNotice how both Markov processes satisfied the forward Kolmogrov equation for the distribution, and the backwards for the expected values. This is a general property of Markov processes (wow!) that will be revisited.\n","permalink":"http://localhost:1313/posts/notes/eliasa/chap5/5-3/","summary":"Markov processes in continuous time and space Given a probability space $(\\Omega, \\mathcal{F}, \\mathbb{P})$ and the filtration $\\mathbb{F} = (\\mathcal{F}_t)_{t \\geq 0}$, a stochastic process $X_t$ is called a Markov process wrt $\\mathcal{F}_t$ if\n$X_t$ is $\\mathcal{F}_t$-adapted For any $t \\geq s$ and $B \\in \\mathcal{R}$, we have $$\\mathbb{P}(X_t \\in B | \\mathcal{F}_s) = \\mathbb{P}(X_t \\in B | X_s)$$ Essentially, this is saying that history doesn\u0026rsquo;t matter, only the current state matters.","title":"5.3 - Markov Processes"},{"content":"Filtration Definition 5.3: (Filtration). Given a probability space, the filtration is a nondecreaseing family of $\\sigma$-algebras $\\{\\mathcal{F}_t\\}_{t \\leq 0}$ such that $\\mathcal{F}_s \\subset \\mathcal{F}_t \\subset \\mathcal{F}$ for all $0 \\leq s \u003c t$.\nIntuitively, the filtration is a sigma algebra of events that can be determined before time $t$ (we can\u0026rsquo;t lose information by foing forward in time). A stochastic process is called $\\mathcal{F}_t$-adapted if it is measurable with respect to $\\mathcal{F}_t$; that is, for all $B \\in \\mathcal{R}$, $X_t^{-1}(B) \\in \\mathcal{F}_t$. We can always assume that the $\\mathcal{F}_t$ contains $F_t^{X}$ and all sets of measure zero, where $F_t^{X} = \\sigma(X_s, s \\leq t)$ is the sigma algebra generated by the process $X$ up to time $t$.\nAs an example, in a series of coin flips, when $n=0$ $$\\mathcal{F}_0^X = \\{\\emptyset, \\Omega\\}$$ and when $n=1$, $$\\mathcal{F}_1^X = \\{\\emptyset, \\Omega, \\{H\\}, \\{T\\}\\}$$ when $n=2$, $$\\mathcal{F}_2^X = \\sigma(\\{\\emptyset, \\{HH\\}, \\{TT\\}, \\{HT\\}, \\{TH\\} \\})$$ (I believe this last statement is equivalent to what the book has)\nStopping Time Definition 5.4: (Stopping time for discrete time stochastic processes). A stopping time is a random variable $T$ taking values in $\\{1,2,\\ldots\\}\\cup \\{\\infty\\}$ such that for any $n \u003c \\infty$, $$\\{T \\leq n\\} \\in \\mathcal{F}_n$$ For the discrete case, it doesn\u0026rsquo;t matter if we say $\\{T \\leq n\\}$ or $\\{T = n\\}$ simply becase it has to be satisfied for all $n$.\nProposition 5.5: (Properties of stopping times). For the Markov process $\\{X_n\\}_{n \\in \\mathbb{N}}$, we have\n(1) if $T_1, T_2$ are stopping times, then $T_1 \\wedge T_2, T_1 \\vee T_2, T_1 + T_2$ are stopping times (2) if $\\{T_k\\}_{k \\geq 1}$ are stopping times then $\\sup_k T_k, \\inf_k T_k, \\limsup_k T_k, \\liminf_k T_k$ are stopping times Definition 5.6: (Stopping time for continuous time stochastic processes). A stopping time is a random variable $T$ taking values in $[0,\\infty]$ such that for any $t \\in \\mathbb{\\bar{R}}^+$, $$\\{T \\leq t\\} \\in \\mathcal{F}_t$$ Note that we cannot swap the inequality for an equals sign in the definition of a stopping time for continuous time processes. Furthermore, porposition 5.5 holds for conitnious time processes if the filtration is right continuous: $\\mathcal{F}_t = \\mathcal{F}_{t^+}= \\bigcap_{s\u003et} \\mathcal{F}_s$.\n","permalink":"http://localhost:1313/posts/notes/eliasa/chap5/5-2/","summary":"Filtration Definition 5.3: (Filtration). Given a probability space, the filtration is a nondecreaseing family of $\\sigma$-algebras $\\{\\mathcal{F}_t\\}_{t \\leq 0}$ such that $\\mathcal{F}_s \\subset \\mathcal{F}_t \\subset \\mathcal{F}$ for all $0 \\leq s \u003c t$.\nIntuitively, the filtration is a sigma algebra of events that can be determined before time $t$ (we can\u0026rsquo;t lose information by foing forward in time). A stochastic process is called $\\mathcal{F}_t$-adapted if it is measurable with respect to $\\mathcal{F}_t$; that is, for all $B \\in \\mathcal{R}$, $X_t^{-1}(B) \\in \\mathcal{F}_t$.","title":"5.2 - Filtration and Stopping Time"},{"content":"Definition of a stochastic process A stochastic process is a parameterized random variable $\\{X_t\\}_{t\\in\\mathbf{T}}$ defined on a probability space $(\\Omega, \\mathcal{F}, \\mathbb{P})$ taking on values in $\\mathbb{R}$. $\\mathbf{T}$ can seemingly be any subset of $\\mathbb{R}$. For any fixed $t \\in \\mathbf{T}$, we can define the random variable\n$$X_t: \\Omega \\rightarrow \\mathbb{R}, \\quad \\omega \\rightarrowtail X_t(\\omega)$$ Thinking of a simple random walk, this means that $X_t$ is a random variable that takes in some subset of $\\Omega = \\{H,T\\}^\\mathbb{N}$ and outputs a real valued number (the sum of the first $t$ values in $\\omega$): $\\{\\omega_1, \\omega_2, \\ldots \\} \\rightarrow \\sum_{n \\leq t} X(\\omega_n)$\nOn the other side of the coin, for a fixed $\\omega \\in \\Omega$, we can define a real-valued measureable function on $\\mathbf{T}$ called the trajectory of $X$\n$$X_.(\\omega): \\mathbf{T} \\rightarrow \\mathbb{R}, \\quad t \\rightarrowtail X_t(\\omega)$$ Again, back to the random walk, this means that we can get a real valued output for any given $t$. To be even more compact, we can say taht a stochastic process is a measureable function from $\\Omega \\times \\mathbf{T}$ to $\\mathbb{R}$\n$$(\\omega, t) \\rightarrowtail X(\\omega, t) := X_t(\\omega)$$ The largest probability space that one can take is the infinite product space $\\Omega = \\mathbb{R}^\\mathbf{T}$. Essentially, this is a space which can takeon any real value at any moment in time (\u0026#x26a0;\u0026#xfe0f; why are we restricting ourselves to $\\mathbb{R}$? Why can\u0026rsquo;t it be a vector valued function?)\nFor finite dimension distributions, we are interested in $$\\mu_{1,\\ldots,t_k}(F_1 \\times \\ldots \\times F_k) = \\mathbb{P[X_{t_1}\\in F_1, \\ldots X_{t_k} \\in F_k]}$$ Theorem 5.2: (Kolmogorov\u0026rsquo;s extension theorem). Kolmogorov\u0026rsquo;s extension theorem allows us to say, for any $\\mu$ invariant under permuting the order of $t_k$ and $F_k$ and also adding additional time points with their associated $F$ being $\\mathbb{R}$, that there exists a probability space and a stochastic prcess such that $$\\mu_{1,\\ldots,t_k}(F_1 \\times \\ldots \\times F_k) = \\mathbb{P[X_{t_1}\\in F_1, \\ldots X_{t_k} \\in F_k]}$$ Kolmogorov\u0026rsquo;s extension theorem is very general. In fact, so general that it does not give us a very good idea of what the process actually looks like. Usually, we start with this extremely general definition and then impose stricter conditions to prove that the measure can be defined on a smaller probability space rather than $\\Omega$\n","permalink":"http://localhost:1313/posts/notes/eliasa/chap5/5-1/","summary":"Definition of a stochastic process A stochastic process is a parameterized random variable $\\{X_t\\}_{t\\in\\mathbf{T}}$ defined on a probability space $(\\Omega, \\mathcal{F}, \\mathbb{P})$ taking on values in $\\mathbb{R}$. $\\mathbf{T}$ can seemingly be any subset of $\\mathbb{R}$. For any fixed $t \\in \\mathbf{T}$, we can define the random variable\n$$X_t: \\Omega \\rightarrow \\mathbb{R}, \\quad \\omega \\rightarrowtail X_t(\\omega)$$ Thinking of a simple random walk, this means that $X_t$ is a random variable that takes in some subset of $\\Omega = \\{H,T\\}^\\mathbb{N}$ and outputs a real valued number (the sum of the first $t$ values in $\\omega$): $\\{\\omega_1, \\omega_2, \\ldots \\} \\rightarrow \\sum_{n \\leq t} X(\\omega_n)$","title":"5.1 - Axiomatic Construction of Stochastic Process"},{"content":"Here are my notes for E, Li, and Vanden-Eijnden\u0026rsquo;s Applied Stochastic Analysis\nChapter 5 - Stochastic Processes 5.1 - Axiomatic Construction of Stochastic Process 5.2 - Filtration and Stopping Time 5.3 - Markov Processes 5.4 - Gaussian Processes ","permalink":"http://localhost:1313/posts/notes/eliasa/eliasa/","summary":"Here are my notes for E, Li, and Vanden-Eijnden\u0026rsquo;s Applied Stochastic Analysis\nChapter 5 - Stochastic Processes 5.1 - Axiomatic Construction of Stochastic Process 5.2 - Filtration and Stopping Time 5.3 - Markov Processes 5.4 - Gaussian Processes ","title":"Applied Stochastic Analysis"},{"content":"2.2.1 - Definition and first examples Definition 2.2: A symmetric monoidal structure on a preoirder $(X, \\leq)$ consists of\n(i) a monoidal unit, $I \\in X$ (ii) a monoidal product $\\otimes: X \\times X \\rightarrow X$ And the monoidal product $\\otimes(x_1,x_2) = x_1 \\otimes x_2$ must also satisfy the following properties (assume all elements are in $X$)\n(a) $x_1 \\leq y_1$ and $x_2 \\leq y_2 \\implies x_1 \\otimes x_2 \\leq y_1 \\otimes y_2$ (b) $I \\otimes x = x \\otimes I = x$ (c) associativity (d) commutivity/symmetry (a) is called monotnoicity and (b) is unitality\nRemark 2.3: replacing $=$ with $\\cong$ in definition 2.2 will give us a weak monoidal structure.\nExercise 2.5: The preorder structure $(\\mathbb{R}, \\leq)$ and the multiplication operation $\\times$ will not give us a symmetric monoidal order because of the simple counter example of $-2 \\times -2 \\nleq 1 \\times 1$.\nExample 2.6: A monid is similar to a symmetric monoidal preorder in that it consists of a set $M$, a function $*: M\\times M \\rightarrow M$, and an elment $e \\in M$ called the monid unit, such that for every $m,n,p \\in M$,\n$m * e = m$ $e * m = m$ associativity holds Further, if commutivity holds (which isn\u0026rsquo;t not generally true), then it is also called commutative\n2.2.2 - Introducing wiring diagrams \u0026#x26a0;\u0026#xfe0f; I am seeing the wiring diagrams, but I fail to understand why they are any different from the Hasse diagrams we\u0026rsquo;ve seen previously.\nEssentially, wiring diagrams seem to be a way to encode information about symmetric monoidal structures. The basic rules built up so far are as follows:\nA wire without a label, or with the label of the monoidal unit, is equivalent to nothing Otherwise, a wire labeled with an element represents that element (\u0026#x26a0;\u0026#xfe0f; this could be wrong) Two parallel wires represent the monoidal product of those elements Placing a $\\leq$ block between two $x,y$ wires indicates that $x \\leq y$ Thinking back to the conditions for a symmetric monoidal structure, we find that\nTransitivity allows us to combine wiring diagrams left to right Monotonicity is represented as being able to combine wiring diagrams top to bottom A monoidal product with a monoidal unit and another element gives us the element again, because the monoidal unit is equivalent to nothing (reflexivity)\nAssociativity means we can \u0026ldquo;wiggle\u0026rdquo; around parallel wires Commutivity means we can cross wires It is intuitive to see how these wiring diagrams can be used to prove statements. In fact, the above images are trivial proofs. Take a look at the following exercise\nExercise 2.20: Prove\u0026ndash;given $t \\leq v+w$, $w+u \\leq x+z$, and $v+x \\leq y$\u0026ndash;that $t+u \\leq y+z$.\nALgebraically, we proceed like so: $$\\begin{align} t + u \u0026\\leq (v+w) + u \\\\ \u0026\\leq v + (w+u) \\\\ \u0026\\leq v + (x+z) \\\\ \u0026\\leq (v + x) + z \\\\ \u0026\\leq y + z \\\\ \\end{align}$$ and the wiring diagram would look like The squares are the $\\leq$ blocks\n2.2.3 - Applied examples While this section did solidify some concepts. It wasn\u0026rsquo;t too important. Although, it did carry two useful examples: discarding and splitting.\nWith discarding, if a symmetric monoidal structure also satisfies $x \\leq I$ for every $x \\in X$, then it is possible to terminate any wire: And if instead have a property like $x \\leq x + x$, then we can split any wire: 2.2.4 - Abstract examples Again, after a skim through, this section did not seem critical.\n2.2.5 - Monoidal montone maps We begin with recalling that for any preorder $(X,\\leq)$ we have an induced equivalence relation $\\cong$ on $X$ where two elements $x \\cong x' \\iff x \\leq x$ and $x' \\leq x$\nDefinition 2.41: $\\mathcal{P} = (P, \\leq_P, I_P, \\otimes_P)$ and $\\mathcal{Q} = (Q, \\leq_Q, I_Q, \\otimes_Q)$ be monoidal preorders. A monoidal monotone from $\\mathcal{P}$ to $\\mathcal{Q}$ is a monotone map $f: (P, \\leq_P) \\rightarrow (Q, \\leq_Q)$ which satisfies\n(a) $I_Q \\leq_Q f(I_P)$ (b) $f(p_1) \\otimes_Q f(p_1 \\otimes_P p_2)$ for all $p_1,p_2 \\in P$. Additionally, $f$ is a strong monoidal monotone if it satisfies\n(a\u0026rsquo;) $I_Q \\cong f(I_P)$ (b\u0026rsquo;) $f(p_1) \\otimes_Q f(p_1 \\otimes_P p_2) \\cong f(p_1) \\otimes_Q f(p_2)$ And it is called a strict monoidal monotone if it satisfies (a\u0026rsquo;\u0026rsquo;) $I_Q = f(I_P)$ (b\u0026rsquo;\u0026rsquo;) $f(p_1) \\otimes_Q f(p_1 \\otimes_P p_2) = f(p_1) \\otimes_Q f(p_2)$ Monoidal monotones are said to be examples of monoidal functors in category theory.\nThe exercises for this section seem a little easy, so I will be skipping them for now, returning to them if I get confused on the defnitions of monoidal monotones.\n","permalink":"http://localhost:1313/posts/notes/fongspivakact/chap2/2-2/","summary":"2.2.1 - Definition and first examples Definition 2.2: A symmetric monoidal structure on a preoirder $(X, \\leq)$ consists of\n(i) a monoidal unit, $I \\in X$ (ii) a monoidal product $\\otimes: X \\times X \\rightarrow X$ And the monoidal product $\\otimes(x_1,x_2) = x_1 \\otimes x_2$ must also satisfy the following properties (assume all elements are in $X$)\n(a) $x_1 \\leq y_1$ and $x_2 \\leq y_2 \\implies x_1 \\otimes x_2 \\leq y_1 \\otimes y_2$ (b) $I \\otimes x = x \\otimes I = x$ (c) associativity (d) commutivity/symmetry (a) is called monotnoicity and (b) is unitality","title":"2.2 - Symmetric monoidal preorders"},{"content":"This is a collection of my notes for Brendan Fong and David Spivak\u0026rsquo;s An Invitation to Appied Category Theory. The first chapter was done through LaTeX, but the rest should be markdown with mathjax.\nChapter 1 - Generative effects: Orders and adjunctions Chapter 2 - Resource theories: Monoidal preorders and enrichment Section 2.2 - Symmetric monoidal preorders ","permalink":"http://localhost:1313/posts/notes/fongspivakact/fongspivakact/","summary":"This is a collection of my notes for Brendan Fong and David Spivak\u0026rsquo;s An Invitation to Appied Category Theory. The first chapter was done through LaTeX, but the rest should be markdown with mathjax.\nChapter 1 - Generative effects: Orders and adjunctions Chapter 2 - Resource theories: Monoidal preorders and enrichment Section 2.2 - Symmetric monoidal preorders ","title":"An Invitation to Appied Category Theory"},{"content":"Coming soon! ","permalink":"http://localhost:1313/about/","summary":"Coming soon! ","title":"About"},{"content":"I was recently doing a probability puzzle that I can\u0026rsquo;t quite remember the context of, but I came across the answer that the probability would be $$\\mathbb{P}(X) = n p^n \\; \\quad \\forall \\: n\\in\\mathbb{N}, p \\in [0,1].$$ But this is obviously wrong! Plug in $p=.9, n=2$, and you get that $\\mathbb{P}(X) = 1.62$. Thaat\u0026rsquo;s not how probability works! However, for $p=0.5$, $\\mathbb{P}(X)$ will remain $\\leq 1$ for all $n \\in \\mathbb{N}$. So, somewhere in the interval $(0.5,0.9)$, we reach a critical value where any $p$ greater than that will result in a probability greater than one, and any value less than it will be a bit more reasonable.\nSo, what is this critical value that will help me save face?\nWell, the question we are trying to answer, phrased a bit more formally, is:\nfind the largest $p \\in [0,1]$ such that $np^n \\leq 1$ for all $n \\in \\mathbb{N}.$\nFirst, we rephrase the problem by stating $$np^n \\leq 1 \\iff p^n \\leq \\frac{1}{n}.$$ Visually, this means that the exponential graph of $f_p(n) = p^n$ can never go above $g_p(n) = \\frac{1}{n}$ for some fixed $p$. From this, we can deduce that the critical value of $p$, which we will denote as $p_0$, will satisfy the following relation:\nGiven the parametrized forms of $f_p$ and $g_p$ $$F_{p}(t) = \\begin{bmatrix} t \\\\ f_p(t) \\end{bmatrix}, \\; G_p(t) = \\begin{bmatrix} t \\\\ g_p(t) \\end{bmatrix},$$ the critical $p_0$ value will be such that $$F_{p_0}(t_0) = G_{p_0}(t_0),\\text{ and } \\dot{F}_{p_0}(t_0) = \\lambda \\dot{G}_{p_0}(t_0)$$ for some $\\lambda \\in \\mathbb{R}, t_0 \\in \\mathbb{R}^+$. In other words, their velocities will point in the same direction (and, perhaps more intuitively, the outward normals of each curve will be parallel, so the graphs \u0026lsquo;kiss\u0026rsquo; at some $t_0$ with the choice of $p_0$).\nNow, we have a fairly simple problem to solve. Because the $x$ component of $F$ and $G$ are always equal, we immediately find that $\\lambda = 1$ for their time derivatives to be equal to each other. Now, that leads us to solve for a $p_0$ and $n$ such that $$ \\begin{aligned} f_{p_0}(t_0) \u0026= g_{p_0}(t_0) \\\\ \\implies p_0^n \u0026= \\frac{1}{t_0} \\end{aligned} $$ and $$ \\begin{aligned} \\dot{f}_{p_0}(t_0) \u0026= \\dot{g}_{p_0}(t_0) \\\\ \\implies -\\ln(p_0)p_0^n \u0026= \\frac{1}{t_0^2} \\end{aligned} $$ So, rather unsatisfyingly, we boiled it down to a system of nonlinear equations $$ \\begin{cases} p_0^n = \\frac{1}{t_0}, \\\\ \\ln(p_0)p_0^n = -\\left(\\frac{1}{t_0}\\right)^2 \\end{cases} $$ which I cannot solve, but Desmos tells me that $p_0 \\approx 0.6922$ and $t_0 \\approx 2.7181$.\nThus, my answer would have been reasonable in some convoluted scenario in which $p \u003c 0.6922$.\n(This answer, too, is not totally right! This is because there may be a larger $p$ value that satisfies $np^n \\leq 1$ for $n \\in \\mathbb{N}$ but not for $n\\in \\mathbb{R}^+$. We solved for the $n \\in \\mathbb{R}^+$ case, which would technically give us a lower bound for $p_0$. Taking this into consideration, our $p_0$ value would really be $p_0 \\approx 0.6934$)\n","permalink":"http://localhost:1313/posts/07-29-24-nothowprobabilityworks/","summary":"I was recently doing a probability puzzle that I can\u0026rsquo;t quite remember the context of, but I came across the answer that the probability would be $$\\mathbb{P}(X) = n p^n \\; \\quad \\forall \\: n\\in\\mathbb{N}, p \\in [0,1].$$ But this is obviously wrong! Plug in $p=.9, n=2$, and you get that $\\mathbb{P}(X) = 1.62$. Thaat\u0026rsquo;s not how probability works! However, for $p=0.5$, $\\mathbb{P}(X)$ will remain $\\leq 1$ for all $n \\in \\mathbb{N}$.","title":"That's not how Probability Works!"},{"content":"I have no idea what I am doing. Anyways, here\u0026rsquo;s a cool equation:\n$$\\frac{d}{dt}\\left(\\frac{\\partial L}{\\partial \\dot{q}}\\right) - \\frac{\\partial L}{\\partial q} = 0$$ ","permalink":"http://localhost:1313/posts/introduction/","summary":"I have no idea what I am doing. Anyways, here\u0026rsquo;s a cool equation:\n$$\\frac{d}{dt}\\left(\\frac{\\partial L}{\\partial \\dot{q}}\\right) - \\frac{\\partial L}{\\partial q} = 0$$ ","title":"Introduction"}]