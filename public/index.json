[{"content":"Filtration Definition 5.3: (Filtration). Given a probability space, the filtration is a nondecreaseing family of $\\sigma$-algebras $\\{\\mathcal{F}_t\\}_{t \\leq 0}$ such that $\\mathcal{F}_s \\subset \\mathcal{F}_t \\subset \\mathcal{F}$ for all $0 \\leq s \u003c t$.\nIntuitively, the filtration is a sigma algebra of events that can be determined before time $t$ (we can\u0026rsquo;t lose information by foing forward in time). A stochastic process is called $\\mathcal{F}_t$-adapted if it is measurable with respect to $\\mathcal{F}_t$; that is, for all $B \\in \\mathcal{R}$, $X_t^{-1}(B) \\in \\mathcal{F}_t$. We can always assume that the $\\mathcal{F}_t$ contains $F_t^{X}$ and all sets of measure zero, where $F_t^{X} = \\sigma(X_s, s \\leq t)$ is the sigma algebra generated by the process $X$ up to time $t$.\nAs an example, in a series of coin flips, when $n=0$ $$\\mathcal{F}_0^X = \\{\\emptyset, \\Omega\\}$$ and when $n=1$, $$\\mathcal{F}_1^X = \\{\\emptyset, \\Omega, \\{H\\}, \\{T\\}\\}$$ when $n=2$, $$\\mathcal{F}_2^X = \\sigma(\\{\\emptyset, \\{HH\\}, \\{TT\\}, \\{HT\\}, \\{TH\\} \\})$$ (I believe this last statement is equivalent to what the book has)\nStopping Time Definition 5.4: (Stopping time for discrete time stochastic processes). A stopping time is a random variable $T$ taking values in $\\{1,2,\\ldots\\}\\cup \\{\\infty\\}$ such that for any $n \u003c \\infty$, $$\\{T \\leq n\\} \\in \\mathcal{F}_n$$ For the discrete case, it doesn\u0026rsquo;t matter if we say $\\{T \\leq n\\}$ or $\\{T = n\\}$ simply becase it has to be satisfied for all $n$.\nProposition 5.5: (Properties of stopping times). For the Markov process $\\{X_n\\}_{n \\in \\mathbb{N}}$, we have\n(1) if $T_1, T_2$ are stopping times, then $T_1 \\wedge T_2, T_1 \\vee T_2, T_1 + T_2$ are stopping times (2) if $\\{T_k\\}_{k \\geq 1}$ are stopping times then $\\sup_k T_k, \\inf_k T_k, \\limsup_k T_k, \\liminf_k T_k$ are stopping times Definition 5.6: (Stopping time for continuous time stochastic processes). A stopping time is a random variable $T$ taking values in $[0,\\infty]$ such that for any $t \\in \\mathbb{\\bar{R}}^+$, $$\\{T \\leq t\\} \\in \\mathcal{F}_t$$ Note that we cannot swap the inequality for an equals sign in the definition of a stopping time for continuous time processes. Furthermore, porposition 5.5 holds for conitnious time processes if the filtration is right continuous: $\\mathcal{F}_t = \\mathcal{F}_{t^+}= \\bigcap_{s\u003et} \\mathcal{F}_s$.\n","permalink":"https://hasithv.github.io/posts/notes/eliasa/chap5/5-2/","summary":"Filtration Definition 5.3: (Filtration). Given a probability space, the filtration is a nondecreaseing family of $\\sigma$-algebras $\\{\\mathcal{F}_t\\}_{t \\leq 0}$ such that $\\mathcal{F}_s \\subset \\mathcal{F}_t \\subset \\mathcal{F}$ for all $0 \\leq s \u003c t$.\nIntuitively, the filtration is a sigma algebra of events that can be determined before time $t$ (we can\u0026rsquo;t lose information by foing forward in time). A stochastic process is called $\\mathcal{F}_t$-adapted if it is measurable with respect to $\\mathcal{F}_t$; that is, for all $B \\in \\mathcal{R}$, $X_t^{-1}(B) \\in \\mathcal{F}_t$.","title":"5.2 - Filtration and Stopping Time"},{"content":"Definition of a stochastic process A stochastic process is a parameterized random variable $\\{X_t\\}_{t\\in\\mathbf{T}}$ defined on a probability space $(\\Omega, \\mathcal{F}, \\mathbb{P})$ taking on values in $\\mathbb{R}$. $\\mathbf{T}$ can seemingly be any subset of $\\mathbb{R}$. For any fixed $t \\in \\mathbf{T}$, we can define the random variable\n$$X_t: \\Omega \\rightarrow \\mathbb{R}, \\quad \\omega \\rightarrowtail X_t(\\omega)$$ Thinking of a simple random walk, this means that $X_t$ is a random variable that takes in some subset of $\\Omega = \\{H,T\\}^\\mathbb{N}$ and outputs a real valued number (the sum of the first $t$ values in $\\omega$): $\\{\\omega_1, \\omega_2, \\ldots \\} \\rightarrow \\sum_{n \\leq t} X(\\omega_n)$\nOn the other side of the coin, for a fixed $\\omega \\in \\Omega$, we can define a real-valued measureable function on $\\mathbf{T}$ called the trajectory of $X$\n$$X_.(\\omega): \\mathbf{T} \\rightarrow \\mathbb{R}, \\quad t \\rightarrowtail X_t(\\omega)$$ Again, back to the random walk, this means that we can get a real valued output for any given $t$. To be even more compact, we can say taht a stochastic process is a measureable function from $\\Omega \\times \\mathbf{T}$ to $\\mathbb{R}$\n$$(\\omega, t) \\rightarrowtail X(\\omega, t) := X_t(\\omega)$$ The largest probability space that one can take is the infinite product space $\\Omega = \\mathbb{R}^\\mathbf{T}$. Essentially, this is a space which can takeon any real value at any moment in time (\u0026#x26a0;\u0026#xfe0f; why are we restricting ourselves to $\\mathbb{R}$? Why can\u0026rsquo;t it be a vector valued function?)\nFor finite dimension distributions, we are interested in $$\\mu_{1,\\ldots,t_k}(F_1 \\times \\ldots \\times F_k) = \\mathbb{P[X_{t_1}\\in F_1, \\ldots X_{t_k} \\in F_k]}$$ Theorem 5.2: (Kolmogorov\u0026rsquo;s extension theorem). Kolmogorov\u0026rsquo;s extension theorem allows us to say, for any $\\mu$ invariant under permuting the order of $t_k$ and $F_k$ and also adding additional time points with their associated $F$ being $\\mathbb{R}$, that there exists a probability space and a stochastic prcess such that $$\\mu_{1,\\ldots,t_k}(F_1 \\times \\ldots \\times F_k) = \\mathbb{P[X_{t_1}\\in F_1, \\ldots X_{t_k} \\in F_k]}$$ Kolmogorov\u0026rsquo;s extension theorem is very general. In fact, so general that it does not give us a very good idea of what the process actually looks like. Usually, we start with this extremely general definition and then impose stricter conditions to prove that the measure can be defined on a smaller probability space rather than $\\Omega$\n","permalink":"https://hasithv.github.io/posts/notes/eliasa/chap5/5-1/","summary":"Definition of a stochastic process A stochastic process is a parameterized random variable $\\{X_t\\}_{t\\in\\mathbf{T}}$ defined on a probability space $(\\Omega, \\mathcal{F}, \\mathbb{P})$ taking on values in $\\mathbb{R}$. $\\mathbf{T}$ can seemingly be any subset of $\\mathbb{R}$. For any fixed $t \\in \\mathbf{T}$, we can define the random variable\n$$X_t: \\Omega \\rightarrow \\mathbb{R}, \\quad \\omega \\rightarrowtail X_t(\\omega)$$ Thinking of a simple random walk, this means that $X_t$ is a random variable that takes in some subset of $\\Omega = \\{H,T\\}^\\mathbb{N}$ and outputs a real valued number (the sum of the first $t$ values in $\\omega$): $\\{\\omega_1, \\omega_2, \\ldots \\} \\rightarrow \\sum_{n \\leq t} X(\\omega_n)$","title":"5.1 - Axiomatic Construction of Stochastic Process"},{"content":"Here are my notes for E, Li, and Vanden-Eijnden\u0026rsquo;s Applied Stochastic Analysis\nChapter 5 - Stochastic Processes 5.1 - Axiomatic Construction of Stochastic Process 5.2 - Filtration and Stopping Time ","permalink":"https://hasithv.github.io/posts/notes/eliasa/eliasa/","summary":"Here are my notes for E, Li, and Vanden-Eijnden\u0026rsquo;s Applied Stochastic Analysis\nChapter 5 - Stochastic Processes 5.1 - Axiomatic Construction of Stochastic Process 5.2 - Filtration and Stopping Time ","title":"Applied Stochastic Analysis"},{"content":"2.2.1 - Definition and first examples Definition 2.2: A symmetric monoidal structure on a preoirder $(X, \\leq)$ consists of\n(i) a monoidal unit, $I \\in X$ (ii) a monoidal product $\\otimes: X \\times X \\rightarrow X$ And the monoidal product $\\otimes(x_1,x_2) = x_1 \\otimes x_2$ must also satisfy the following properties (assume all elements are in $X$)\n(a) $x_1 \\leq y_1$ and $x_2 \\leq y_2 \\implies x_1 \\otimes x_2 \\leq y_1 \\otimes y_2$ (b) $I \\otimes x = x \\otimes I = x$ (c) associativity (d) commutivity/symmetry (a) is called monotnoicity and (b) is unitality\nRemark 2.3: replacing $=$ with $\\cong$ in definition 2.2 will give us a weak monoidal structure.\nExercise 2.5: The preorder structure $(\\mathbb{R}, \\leq)$ and the multiplication operation $\\times$ will not give us a symmetric monoidal order because of the simple counter example of $-2 \\times -2 \\nleq 1 \\times 1$.\nExample 2.6: A monid is similar to a symmetric monoidal preorder in that it consists of a set $M$, a function $*: M\\times M \\rightarrow M$, and an elment $e \\in M$ called the monid unit, such that for every $m,n,p \\in M$,\n$m * e = m$ $e * m = m$ associativity holds Further, if commutivity holds (which isn\u0026rsquo;t not generally true), then it is also called commutative\n2.2.2 - Introducing wiring diagrams \u0026#x26a0;\u0026#xfe0f; I am seeing the wiring diagrams, but I fail to understand why they are any different from the Hasse diagrams we\u0026rsquo;ve seen previously.\nEssentially, wiring diagrams seem to be a way to encode information about symmetric monoidal structures. The basic rules built up so far are as follows:\nA wire without a label, or with the label of the monoidal unit, is equivalent to nothing Otherwise, a wire labeled with an element represents that element (\u0026#x26a0;\u0026#xfe0f; this could be wrong) Two parallel wires represent the monoidal product of those elements Placing a $\\leq$ block between two $x,y$ wires indicates that $x \\leq y$ Thinking back to the conditions for a symmetric monoidal structure, we find that\nTransitivity allows us to combine wiring diagrams left to right Monotonicity is represented as being able to combine wiring diagrams top to bottom A monoidal product with a monoidal unit and another element gives us the element again, because the monoidal unit is equivalent to nothing (reflexivity)\nAssociativity means we can \u0026ldquo;wiggle\u0026rdquo; around parallel wires Commutivity means we can cross wires It is intuitive to see how these wiring diagrams can be used to prove statements. In fact, the above images are trivial proofs. Take a look at the following exercise\nExercise 2.20: Prove\u0026ndash;given $t \\leq v+w$, $w+u \\leq x+z$, and $v+x \\leq y$\u0026ndash;that $t+u \\leq y+z$.\nALgebraically, we proceed like so: $$\\begin{align} t + u \u0026\\leq (v+w) + u \\\\ \u0026\\leq v + (w+u) \\\\ \u0026\\leq v + (x+z) \\\\ \u0026\\leq (v + x) + z \\\\ \u0026\\leq y + z \\\\ \\end{align}$$ and the wiring diagram would look like The squares are the $\\leq$ blocks\n2.2.3 - Applied examples While this section did solidify some concepts. It wasn\u0026rsquo;t too important. Although, it did carry two useful examples: discarding and splitting.\nWith discarding, if a symmetric monoidal structure also satisfies $x \\leq I$ for every $x \\in X$, then it is possible to terminate any wire: And if instead have a property like $x \\leq x + x$, then we can split any wire: 2.2.4 - Abstract examples Again, after a skim through, this section did not seem critical.\n2.2.5 - Monoidal montone maps We begin with recalling that for any preorder $(X,\\leq)$ we have an induced equivalence relation $\\cong$ on $X$ where two elements $x \\cong x' \\iff x \\leq x$ and $x' \\leq x$\nDefinition 2.41: $\\mathcal{P} = (P, \\leq_P, I_P, \\otimes_P)$ and $\\mathcal{Q} = (Q, \\leq_Q, I_Q, \\otimes_Q)$ be monoidal preorders. A monoidal monotone from $\\mathcal{P}$ to $\\mathcal{Q}$ is a monotone map $f: (P, \\leq_P) \\rightarrow (Q, \\leq_Q)$ which satisfies\n(a) $I_Q \\leq_Q f(I_P)$ (b) $f(p_1) \\otimes_Q f(p_1 \\otimes_P p_2)$ for all $p_1,p_2 \\in P$. Additionally, $f$ is a strong monoidal monotone if it satisfies\n(a\u0026rsquo;) $I_Q \\cong f(I_P)$ (b\u0026rsquo;) $f(p_1) \\otimes_Q f(p_1 \\otimes_P p_2) \\cong f(p_1) \\otimes_Q f(p_2)$ And it is called a strict monoidal monotone if it satisfies (a\u0026rsquo;\u0026rsquo;) $I_Q = f(I_P)$ (b\u0026rsquo;\u0026rsquo;) $f(p_1) \\otimes_Q f(p_1 \\otimes_P p_2) = f(p_1) \\otimes_Q f(p_2)$ Monoidal monotones are said to be examples of monoidal functors in category theory.\nThe exercises for this section seem a little easy, so I will be skipping them for now, returning to them if I get confused on the defnitions of monoidal monotones.\n","permalink":"https://hasithv.github.io/posts/notes/fongspivakact/chap2/2-2/","summary":"2.2.1 - Definition and first examples Definition 2.2: A symmetric monoidal structure on a preoirder $(X, \\leq)$ consists of\n(i) a monoidal unit, $I \\in X$ (ii) a monoidal product $\\otimes: X \\times X \\rightarrow X$ And the monoidal product $\\otimes(x_1,x_2) = x_1 \\otimes x_2$ must also satisfy the following properties (assume all elements are in $X$)\n(a) $x_1 \\leq y_1$ and $x_2 \\leq y_2 \\implies x_1 \\otimes x_2 \\leq y_1 \\otimes y_2$ (b) $I \\otimes x = x \\otimes I = x$ (c) associativity (d) commutivity/symmetry (a) is called monotnoicity and (b) is unitality","title":"2.2 - Symmetric monoidal preorders"},{"content":"This is a collection of my notes for Brendan Fong and David Spivak\u0026rsquo;s An Invitation to Appied Category Theory. The first chapter was done through LaTeX, but the rest should be markdown with mathjax.\nChapter 1 - Generative effects: Orders and adjunctions Chapter 2 - Resource theories: Monoidal preorders and enrichment Section 2.2 - Symmetric monoidal preorders ","permalink":"https://hasithv.github.io/posts/notes/fongspivakact/fongspivakact/","summary":"This is a collection of my notes for Brendan Fong and David Spivak\u0026rsquo;s An Invitation to Appied Category Theory. The first chapter was done through LaTeX, but the rest should be markdown with mathjax.\nChapter 1 - Generative effects: Orders and adjunctions Chapter 2 - Resource theories: Monoidal preorders and enrichment Section 2.2 - Symmetric monoidal preorders ","title":"An Invitation to Appied Category Theory"},{"content":"Coming soon! ","permalink":"https://hasithv.github.io/about/","summary":"Coming soon! ","title":"About"},{"content":"I was recently doing a probability puzzle that I can\u0026rsquo;t quite remember the context of, but I came across the answer that the probability would be $$\\mathbb{P}(X) = n p^n \\; \\quad \\forall \\: n\\in\\mathbb{N}, p \\in [0,1].$$ But this is obviously wrong! Plug in $p=.9, n=2$, and you get that $\\mathbb{P}(X) = 1.62$. Thaat\u0026rsquo;s not how probability works! However, for $p=0.5$, $\\mathbb{P}(X)$ will remain $\\leq 1$ for all $n \\in \\mathbb{N}$. So, somewhere in the interval $(0.5,0.9)$, we reach a critical value where any $p$ greater than that will result in a probability greater than one, and any value less than it will be a bit more reasonable.\nSo, what is this critical value that will help me save face?\nWell, the question we are trying to answer, phrased a bit more formally, is:\nfind the largest $p \\in [0,1]$ such that $np^n \\leq 1$ for all $n \\in \\mathbb{N}.$\nFirst, we rephrase the problem by stating $$np^n \\leq 1 \\iff p^n \\leq \\frac{1}{n}.$$ Visually, this means that the exponential graph of $f_p(n) = p^n$ can never go above $g_p(n) = \\frac{1}{n}$ for some fixed $p$. From this, we can deduce that the critical value of $p$, which we will denote as $p_0$, will satisfy the following relation:\nGiven the parametrized forms of $f_p$ and $g_p$ $$F_{p}(t) = \\begin{bmatrix} t \\\\ f_p(t) \\end{bmatrix}, \\; G_p(t) = \\begin{bmatrix} t \\\\ g_p(t) \\end{bmatrix},$$ the critical $p_0$ value will be such that $$F_{p_0}(t_0) = G_{p_0}(t_0),\\text{ and } \\dot{F}_{p_0}(t_0) = \\lambda \\dot{G}_{p_0}(t_0)$$ for some $\\lambda \\in \\mathbb{R}, t_0 \\in \\mathbb{R}^+$. In other words, their velocities will point in the same direction (and, perhaps more intuitively, the outward normals of each curve will be parallel, so the graphs \u0026lsquo;kiss\u0026rsquo; at some $t_0$ with the choice of $p_0$).\nNow, we have a fairly simple problem to solve. Because the $x$ component of $F$ and $G$ are always equal, we immediately find that $\\lambda = 1$ for their time derivatives to be equal to each other. Now, that leads us to solve for a $p_0$ and $n$ such that $$ \\begin{aligned} f_{p_0}(t_0) \u0026= g_{p_0}(t_0) \\\\ \\implies p_0^n \u0026= \\frac{1}{t_0} \\end{aligned} $$ and $$ \\begin{aligned} \\dot{f}_{p_0}(t_0) \u0026= \\dot{g}_{p_0}(t_0) \\\\ \\implies -\\ln(p_0)p_0^n \u0026= \\frac{1}{t_0^2} \\end{aligned} $$ So, rather unsatisfyingly, we boiled it down to a system of nonlinear equations $$ \\begin{cases} p_0^n = \\frac{1}{t_0}, \\\\ \\ln(p_0)p_0^n = -\\left(\\frac{1}{t_0}\\right)^2 \\end{cases} $$ which I cannot solve, but Desmos tells me that $p_0 \\approx 0.6922$ and $t_0 \\approx 2.7181$.\nThus, my answer would have been reasonable in some convoluted scenario in which $p \u003c 0.6922$.\n(This answer, too, is not totally right! This is because there may be a larger $p$ value that satisfies $np^n \\leq 1$ for $n \\in \\mathbb{N}$ but not for $n\\in \\mathbb{R}^+$. We solved for the $n \\in \\mathbb{R}^+$ case, which would technically give us a lower bound for $p_0$. Taking this into consideration, our $p_0$ value would really be $p_0 \\approx 0.6934$)\n","permalink":"https://hasithv.github.io/posts/07-29-24-nothowprobabilityworks/","summary":"I was recently doing a probability puzzle that I can\u0026rsquo;t quite remember the context of, but I came across the answer that the probability would be $$\\mathbb{P}(X) = n p^n \\; \\quad \\forall \\: n\\in\\mathbb{N}, p \\in [0,1].$$ But this is obviously wrong! Plug in $p=.9, n=2$, and you get that $\\mathbb{P}(X) = 1.62$. Thaat\u0026rsquo;s not how probability works! However, for $p=0.5$, $\\mathbb{P}(X)$ will remain $\\leq 1$ for all $n \\in \\mathbb{N}$.","title":"That's not how Probability Works!"},{"content":"I have no idea what I am doing. Anyways, here\u0026rsquo;s a cool equation:\n$$\\frac{d}{dt}\\left(\\frac{\\partial L}{\\partial \\dot{q}}\\right) - \\frac{\\partial L}{\\partial q} = 0$$ ","permalink":"https://hasithv.github.io/posts/introduction/","summary":"I have no idea what I am doing. Anyways, here\u0026rsquo;s a cool equation:\n$$\\frac{d}{dt}\\left(\\frac{\\partial L}{\\partial \\dot{q}}\\right) - \\frac{\\partial L}{\\partial q} = 0$$ ","title":"Introduction"}]